job_title,company,job_location,post_date,salary,job_url,job_description
Senior Data Engineer,Etainement,"Flower Mound, TX",EmployerActive Today,"$140,000 - $200,000 a year",https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Axsm1kqmNbOoV7ZWG73X0b__pIExLy604NOm_-6xaU1Tg7-VL6nvjJAfK0kMeERTDHLrFFOCH2IzcZs3j8HHjs2u-NS5NpW_D_aLCnMTk_dhLWPO65T0iUrrgoVirZ8erBiiDqaHI74_GIKTmdJ3SNqlx6yYL061A7ujgpafBIAe55mdfRkWaf89vF-yg8tBLJHS_iSdzBeLgQPrJr-B0uqk8JUbVMWWVpWMkkPBpHJNV6J1HUWVyfhZMczib_FJui9dXl5ZOS2TEjTSKl84fY1CkHGxWsZyFE_1ZugS6JUBfEnELTzjnxkNj9UffQUd5EwenR0PixMXBeJWgHRvUnzHgfxmda9-6s8Yo7xQ-Z7Acfma-LqHnyNiAgz981shbbHpuHoaWgToEik2wuPiRkPZBACSF_EToJ0wgfD7VFQNL850VDVzecV_V_Y07KUnA1KSuY6AIUAUSnlg0w3vNzBixGyPqpl8ckeMkqYr6dwwpiQ7udEzJ6Le6baoEjwgVfQVbPz23IjJHaFeaDrojd&xkcb=SoDm-_M3ML2tGex8Zx0JbzkdCdPP&p=0&fvj=1&vjs=3,"Etainement is a leader in live event ticket pricing, data and distribution. We trade live event tickets, including concerts, sports and theatre across multiple online marketplaces using data and proprietary technology.
https://etainement.co/
We are seeking a Senior Data Engineer to develop and maintain our technology and analytics stack. As a Senior Data Engineer, you must have a proven history of designing and implementing scalable data infrastructure that enables the efficient and effective analysis of large and complex data sets. You should be skilled in database querying, data modeling and ETL processes with diverse data sources. You should be enthusiastic about data and proficient in various tools to deliver precise results. You should have experience supporting data analytics for multiple departments, possessing a comprehensive understanding of the business to provide a data infrastructure that drives success. You are a team player, who excels in fast-paced environments and collaborating with cross-functional teams.
RESPONSIBILITIES & JOB DUTIES (INCLUDE, BUT ARE NOT LIMITED TO):

Develop and maintain cloud data pipelines for use in analytics and machine learning.
Build and manage databases and infrastructure to ensure data accuracy, scalability and integrity.
Access, acquire, organize, transform and clean data from internal and external sources as needed.
Create and maintain data models for analysis and insights.
Triage and troubleshoot technical issues that arise while helping to see them through to resolution.
Work closely with the development team and stakeholders to flesh out requirements and solution design.

REQUIRED QUALIFICATIONS:

Experience with cloud data services, including Big Query, Cloud Run, Cloud Functions or cloud equivalents.
4+ years of experience as a Data Engineer, or related role.
Excellent oral and written communication skills and keen attention to detail.
Proven ability to work independently or as part of a team.
Have advanced knowledge of commonly used concepts, practices and procedures with data analytics and administration of cloud data platforms.
Some experience in small or medium-sized projects as a lead or SME (subject matter expert).
Strong understanding of data modeling, querying and transformation processes.
Excellent problem-solving skills.

PREFERRED QUALIFICATIONS:

Experience with different programming languages related to building data pipelines and analyzing data, such as SQL, Python or R.
Proven experience building and managing cloud data pipelines and infrastructure in GCP or similar cloud environments.
Experience with ETL tools, such as SSIS, Azure Data Factory or AWS Glue.
Experience working on teams solving Revenue Management (RM), forecasting and inventory optimization problems.
Familiarity with the event ticketing industry.

Job Type: Full-time
Pay: $140,000.00 - $200,000.00 per year
Benefits:

Flexible schedule
Paid time off

Compensation package:

Bonus pay

Experience level:

4 years

Schedule:

Monday to Friday

Work Location: In person"
Data Engineer,Malin USA,"Hybrid remote in Addison, TX 75001",PostedToday,Full-time,https://www.indeed.com/company/Malin-A-Raymond-Company/jobs/Data-Engineer-4250894ed590c591?fccid=758977ef84686043&vjs=3,"***NO SPONSORSHIP AT THIS TIME***
Data Engineer Duties and Responsibilities
· Assemble large, complex sets of data that meet non-functional and functional business requirements
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
· Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies
· Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
· Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues
· Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Skills and Qualifications
· Ability to build and optimize data sets, ‘big data’ data pipelines and architectures
· Ability to understand and build complex data models for Power BI reporting.
· Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
· Excellent analytic skills associated with working on unstructured datasets
· Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Must Have experience in (in order of importance)
· 10 + years experience as a Data Engineer or similar role.
· SQL Server
o T-SQL code
o Administrator functions
· Data Factory
o Python
· Power BI
o DAX code
o M code
· Azure
Malin is an Equal Opportunity Employer -- M/F/Veteran/Disability/Sexual Orientation/Gender Identity
Job Type: Full-time
Benefits:

401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Paid time off
Referral program
Vision insurance

Experience level:

10 years

Schedule:

Monday to Friday

Work Location: Hybrid remote in Addison, TX 75001"
Data Logging Engineer,GTA (Global Technology Associates),"Plano, TX",Posted 13 days ago,From $50 an hour,https://www.indeed.com/company/GTA-(Global-Technology-Associates)/jobs/Data-Engineer-1bfe315b01b0c031?fccid=5db15845914e1098&vjs=3,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:

8 hour shift

Work Location: In person"
Junior Data Engineer,HYR Global Source Inc,"Dallas, TX",Posted 27 days ago,Contract,https://www.indeed.com/rc/clk?jk=9d43667475e5d566&fccid=5d5dddb0dec55dfa&vjs=3,"Job Title - Junior Data Engineer

Location - Dallas TX Hybrid ( No Relocation )

Tax Term (W2, C2C) - W2 / C2C


Position Type(Contract/permanent) - Contract

Project Duration - Long term 12+ months




Job description -
 The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within.


Scripting – Python, KSH, Powershell 
Knowledge about Linux, and Windows Server Operating System 
AWS basic knowledge"
Data Engineer,Omnitech Consulting,"Plano, TX",EmployerActive 2 days ago,$50 - $60 an hour,https://www.indeed.com/company/Omnitech-Consulting/jobs/Data-Engineer-56a2e4470bb5667f?fccid=1587c0d5ad922a82&vjs=3,"About us
Omnitech Consulting LLC, a technology consulting firm dedicated to providing innovative solutions for businesses of all sizes. We work closely with our clients to understand their unique talent needs and develop customized approach and provide skilled talent that align with their business objectives.
Job Description
W2 Only, Hybrid Work at Plano TX.
The Data Engineer will be responsible for the following:

Data engineering and design, including data modeling, data cleansing, and data integration
Designing, implementing, and supporting data systems and tools to meet company’s business needs
Developing data models to support the collection, storage, analysis, and reporting of data
Implementing data integration and reporting tools to support business needs
Developing and maintaining data-driven reports and dashboards for management and internal use
Developing code that is efficient and maintainable for production use
Maintaining a strong understanding of the underlying database technologies, including SQL Server, MS SQL Server, Oracle, etc.
Working with the development team to ensure the application is built in a way that is scalable and maintainable.
Other duties as assigned.

Requirements:

Min 3 to 5 years of experience in Python
Min 3 to 5 years of experience in SQL
Min 3 to 5 years of experience in Cloud
Min 3 to 5 years of experience in ETL
Bachelor’s degree in Computer Science or related field; Master’s degree preferred. Master’s degree preferred.
5+ years of experience in a similar role.

Benefits:Omnitech Consulting offers competitive compensation based on experience.
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Benefits:

Paid time off

Compensation package:

Hourly pay

Experience level:

5 years

Schedule:

Monday to Friday

Ability to commute/relocate:

Plano, TX: Reliably commute or planning to relocate before starting work (Required)

Education:

Bachelor's (Required)

Experience:

Python: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Cloud computing: 3 years (Required)
Azure Data Lake: 3 years (Required)
Corporate finance: 3 years (Required)

Work Location: In person"
Data Engineer,"Stefanini, Inc","Dallas, TX",Posted 1 day ago,$60 - $65 an hour,https://www.indeed.com/rc/clk?jk=72eef199562b6fa5&fccid=7e646b8cb4a11b13&vjs=3,"Stefanini is looking for a Data Engineer in Dallas, TX (hybrid role). W2 only. US Citizens only


 For quick apply, please reach out to Vishal Sharma- 
Vishal.sharma@stefanini.com
 / 248.263.5616



 The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support. You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages. You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality. As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members.
 


 Responsibilities:


 Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns
 Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity
 Work under general guidance and clear framework of accountability with substantial autonomy
 Use best practices and knowledge of internal or external business issues to improve products or services
 Solve complex problems; takes a new perspective using existing solutions





Required Skills:


 An associate degree, a bachelor's degree in computer science or equivalent courses
 At least 4 years of experience in Data Engineering with SQL, Python
 Experience with relational SQL and NoSQL databases
 Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)
 At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)
 At least 2 years of experience with AWS cloud (with focus on Data services)
 Experience with Databricks a plus
 Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs
 Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix/Linux shell scripting
 Able to interpret user requirements and identify additional information needed in user requirements
 Able to see effects of current design with future requirements and see possible coding solutions to meet the requirements
 Detailed understanding of logical and physical data structures
 Highly skilled in tools, evaluates the need for various tools for continuous integration, testing, automation, deployment etc. and discuss with the team
 Highly skilled at designing tests for unfamiliar designs; Evaluates tests for weaknesses and continuously improves them
 Detailed understanding of how to effectively test against multiple tools/software
 Equivalent education and/or experience may be substituted for any of the above requirements



 ***Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives.

 Stefanini takes pride in hiring top talent and developing relationships with our future employees. Our talent acquisition teams will never make an offer of employment without having a phone conversation with you. Those face-to-face conversations will involve a description of the job for which you have applied. We also speak with you about the process including interviews and job offers."
Data Engineer,Kommforcesolutions,"Dallas, TX",EmployerActive 2 days ago,$65 - $70 an hour,https://www.indeed.com/company/Kommforcesolutions/jobs/Data-Engineer-e68f8198d648ddea?fccid=b9aa8c10996bb21c&vjs=3,"Job title: Data Engineer ( USC GC )
Location: Dallas, TX ( Hybrid )
Client: Southwest Airlines
Looking for strong experience with Abinitio and AWS.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Work Location: On the road"
"Engineer, Data - Clinical Analytics",Concentra,"Addison, TX 75001",EmployerActive 3 days ago,Full-time,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DDQFhFpEj8E6I-jJYIuuzBZBfxsun5AZUCGICqmkVSgTAldL_Puln30_Ys0aKNaC_COkl8j60lib1YQIRKND6PUZ4dE9dhMgCBkPIVnFC53DtbbQSJhImHsaIn2_OjICdXv87db2dbSJ5mCknb-b8LhZInLCjRMzVZoNykLnOHSLCMOI8tmJTXIS1ixoQd5RLRr3uLWCF_Av2icMBfqn-4vL14mpNp3isQZN7fDNgjZ-rf6Pgfwn91TT0bH0986l7KFLf2SYUCSgGyfztVU1EvZmeTEF3CN8rvUivJ8Cop_Y4LzE4XK2u_JDhp979lh143aoShik7tRObR09-7iy1QMbiW3okt43oEQ0Bmk484OH_3d7Qh_AkKLkf0GtRSR2vtYLtCFp0lY2bkcqYGZthNtH1KF1624LI66G_ewK-q4tTDO6HDv5ArWZ4wdYqjXoltRFOFwsvbFzgnzvMcoFBsyVDKVrSsBC1BNcHVHHxpECenKWkYOP7L_sENafuvSGMLQgykYFgVuIP6Tv9oR0ALV1KEsaeko9-ju-uUbzx2o8FMwx8Rexvq5Df8w7zXxsg8guurx9tX29q8kRf5Zhoh-6FH2jIk2XY_6-HZxN0ruDwv3QdMliHsN8pSbENxDRp6UO6eunELikerpy0BCqx2_cLYVXtkbGCWJdvF72oew-ORnUTfZ4Y5iWAZkuGfo8J6EDGcfmJKkZDzRYX5jIZBb2Ln-zCAAAyBhPltyfrPBQdJEZYIivAY8XTsTx-UKEiZHpVoGzd6CDerjtMfUtcZNz8kmYJLxdT0MSZ3agT7MugYwOaNBo75dVuPpypQcwkVJ6YiCIF_ocQ1W-NelaRUR7XNxmhsL6CclzbvBOiOiryxFUtEFOlX7UlvVBFM6bpHUe0ylWgxqHeMyHIBpk2lcvdDHPs0Q5G-xhcE-WXqzj35XGMBPl2ETl9HynkRV7Ri12EOiGytHRm_QdRbXOd0PfsOmnJIn8Ghl5hgHjd2mqRgM-PtWSz1q1T-gG-vdJhK_-LR0F6-wDRX2aldGcJhUTmvRcQYPfA=&xkcb=SoAc-_M3ML2tGex8Zx0CbzkdCdPP&p=7&fvj=0&vjs=3,"Overview: 
 
   The Data Engineer- Clinical Analytics is primarily focused on analytical processes with ability to implement database solutions and best practices in the realm of data science and machine learning projects. Essential software engineering skills with strong foundational knowledge on data movement and orchestration both on-premises and cloud environment. The Data Engineer supports and aligns with business decisions within Concentra by analyzing raw data, constructing, and maintaining data systems, and improving data quality and efficiency. Implements programming languages to develop and test architectures that enables data operations for predictive (i.e., machine learning/AI) or prescriptive modeling.
  Responsibilities: 
 
Analyze, develop, combine raw information, and maintain various data sources
 Improve data quality and efficiency to build data systems and pipelines
 Identify opportunities for data acquisition and collaborate with Application owners and Subject Matter Experts (SME) to document data domain knowledge
 Implement ETL methods to prepare both structured and unstructured data for predictive and prescriptive modeling
 Leverage data serialization techniques to meet project needs for use in various reporting platforms
 Collaborate with Business Intelligence (BI) ETL Developers/Data Architect, Data Scientists, Reporting Analysts, and Subject Matter Experts (SME) to understand business goals
 Understand enterprise project life cycle and prepare for integration and user acceptance testing methods.
 Produce technical documentation by following enterprise standards and guidelines
 Participate in relevant information-sharing activities
 Serve as escalation point for application support and troubleshooting
 Proactive identification of issues and opportunities that will have an impact on the business use of reports and ensure managerial awareness
 Daily review outstanding issues to assure that troubleshooting and resolutions are current
 Ensure all changes comply with change management policies and procedures
 This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
 Qualifications: 
  Education Level: Bachelor’s Degree
 Major: Computer Science or Computer Engineering
 Degree must be from an accredited college or university

 Job-Related Experience
 Customarily has at least the following experience:

 Customarily has at least three or more years in Software development / Data-Centric pipelines / Model-Centric pipelines
 Relational Database experience
 Documentation and publication


 Job-Related Skills/Competencies

 Concentra Core Competencies of Service Mentality, Attention to Detail, Sense of Urgency, Initiative and Flexibility
 Ability to make decisions or solve problems by using logic to identify key facts, explore alternatives, and propose quality solutions
 Outstanding customer service skills as well as the ability to deal with people in a manner which shows tact and professionalism
 The ability to properly handle sensitive and confidential information (including HIPAA and PHI) in accordance with federal and state laws and company policies
 Strong SQL development and performance tuning skills
 Competencies with Oracle, SQLServer, SSIS, Sybase, NoSQL, Python, Azure, Docker, Git, and Visual Studio
 Experience with Data Lakes, Lake Houses, and ELT is preferred
 Experience with Azure ML, Azure Data Factory, Azure Data Bricks, Azure Data Flow, and Azure Functions is preferred
 Concentra Core Competencies of Service Mentality, Attention to Detail, Sense of Urgency, Initiative and Flexibility
 Ability to make decisions or solve problems by using logic to identify key facts, explore alternatives, and propose quality solutions
 Outstanding customer service skills as well as the ability to deal with people in a manner which shows tact and professionalism
 The ability to properly handle sensitive and confidential information (including HIPAA and PHI) in accordance with federal and state laws and company policies
 Highly organized
 Communication skills to be able to effectively speak and write in a clear and professional manner
 Skilled at listening and providing feedback
 Additional Data: 
 

 Employee Benefits


 401(k) Retirement Plan with Employer Match
 Medical, Vision, Prescription, Telehealth, & Dental Plans
 Life & Disability Insurance
 Paid Time Off
 Colleague Referral Bonus Program
 Tuition Reimbursement
 Commuter Benefits
 Dependent Care Spending Account
 Employee Discounts





     We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation, if required.
   


 *This job requires access to confidential and sensitive information, requiring ongoing discretion and secure information management*
   


 Concentra is an Equal Opportunity Employer, including disability/veterans"
Data Engineer,OneSource Regulatory,"Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=ace7d2b8833db496&fccid=13c11dac3d323b9a&vjs=3,"Company Introduction
 OneSource Regulatory Technology hosts a number of innovative solutions to enhance job performance in the Pharmaceutical space. OSR Technology is looking for an experienced and dedicated data engineer to join our product solutions team!

 
Job Description
 OneSource Regulatory is trying to identify a full-time contractor with at least 4+ years of experience to assist us with ongoing R&D projects.
 We are looking for a data engineer to pull data from various sources and do all the necessary steps to clean, normalize, possibly annotate, and finally load the data into databases. The candidate should be able to develop and implement a strategy for testing the data integrity of the collected data. This role requires extreme attention to detail to ensure data quality is top priority.

 Responsibilities

Well versed in parsing and synthesizing of XML and/or JSON documents.
Curating of data that can involve some intermediate to advanced web scraping. (data may need to be fetched via SFTP, FTP, Wget, Curl, REST APIs, GraphQL queries from spots on the Internet)
Proficiency with Linux command line and various simple tools, such as grep, wc, sed, awk, find, ls, cat, piped commands and possibly some very light Bash shell scripting, setting up crontab schedules and programs
Must have basic knowledge of SQL with the following databases: PostGres, MySQL, Google BigQuery
Must have basic knowledge of No-SQL database knowledge such as MongoDB or similar
Familiarity with basic Cloud technology such as storage buckets, cloud serverless functions
Must have experience extracting text and images from PDF files
Knowledge of Puppeteer or other automatable web client technologies
Understanding JavaScript, HTML/CSS and HTTP methods (for understanding page structure for web scraping)


Skills

Solid experience with Python and Python Libraries such as Pandas, requests, etc
Skill set should match up with required responsibilities listed above
Strong English skills (e.g. grammatical analysis and rhetorical structure)
Team Player
Great communication skills



 Bonus Skills

Experience within the Pharmaceutical Space
Ability to expose data via C# NETCore and/or GraphQL
Google Cloud Platform (Cloud Buckets, Google Cloud Functions (.NET, Python, Node.JS))
Ability to parallelize data manipulation and scraping via Python multi-threading, etc.
Python BeautifulSoup
Scrapy
Docker (setting up Kubernetes style processing if warranted for data scraping/data ingestion/normalization)
Multithreading concepts"
Data Engineer,CVS Health,"Irving, TX",PostedToday,"$70,000 - $140,000 a year",https://www.indeed.com/rc/clk?jk=54cac3fbc10da31b&fccid=be3b11aa573faee7&vjs=3,"Assists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs 
 
 Applies understanding of key business drivers to accomplish own work 
 
 Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems 
 
 Leads portions of initiatives of limited scope, with guidance and direction 
 
 Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing 
 
 Collaborates with client team to transform data and integrate algorithms and models into automated processes 
 
 Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines 
 
 Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems 
 
 Builds data marts and data models to support clients and other internal customers 
 
 Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards
 

Pay Range
 The typical pay range for this role is:
  Minimum: 70,000
  Maximum: 140,000
 
 This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. 
 
 In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies. 
 
 For more detailed information on available benefits, please visit
  jobs.CVSHealth.com/benefits
 Required Qualifications
 1+ years of progressively complex related experience 
 
 Experience with bash shell scripts, UNIX utilities & UNIX Commands
 

Preferred Qualifications
 Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources 
 
 Ability to understand complex systems and solve challenging analytical problems 
 
 Strong problem-solving skills and critical thinking ability 
 
 Strong collaboration and communication skills within and across teams 
 
 Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar 
 
 Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment 
 
 Experience building data transformation and processing solutions 
 
 Has strong knowledge of large-scale search applications and building high volume data pipelines
 

Education
 Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline 
 
 Master’s degree or PhD preferred
 

Business Overview
 Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities."
Data Engineer,Garner Health,"Remote in Dallas, TX",Posted 30+ days ago,"$100,000 - $145,000 a year",https://www.indeed.com/rc/clk?jk=d872bc5b38737852&fccid=6e1d1accd5aeee95&vjs=3,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.

 We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
 Main Responsibilities:

Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production

Our Tools:
 Python, AWS, Snowflake, dbt, Terraform, Postgres
 The ideal candidate has:

2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly

Why You Should Join Our Team:

You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies

The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.


 If you are hired, we may require proof of full vaccination against COVID-19. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement, in accordance with applicable law."
Sr. Azure Data Engineer,Kairos Technologies,"Dallas, TX 75201 (Downtown area)",EmployerActive 24 days ago,$75 - $80 an hour,https://www.indeed.com/company/Kairos-Technologies/jobs/Data-Engineer-a379a589a4baee37?fccid=609eeb6b6a1a2e5b&vjs=3,"Hello,
Hope you are doing great!
If you are comfortable with the requirement, kindly respond with below information and your updated resume( word formatted ) ASAP
Full Name:
Current Location:
Contact No Primary:
Work Authorization:
S.S.N No( Last 4 digits ):
D.O.B( MM/DD ):
Availability( Any notice period required to join in project ):
Currently working(Yes/No):
LinkedIn:
Employer details( If on C2C ):
Please check the below requirement
Direct Client Requirement
Position: Sr. Data Engineer with Azure Cloud and Strong SQL( Hybrid/ Onsite for 2 days a week )
Location: Dallas TX or VA
Duration: 12+ Months Long Term Contract
Work Authorization: US Citizen/GC/GC-EAD/TN Visa( on C2C is fine )
Need Locals Candidates..
Job Summary:
We are seeking an experienced and highly skilled Senior Database Engineer with strong hands-on experience in modernizing and automating on-premises SQL databases, as well as a solid track record of successful migration projects from on-premises to Azure cloud. As a Senior Database Engineer, you will play a critical role in driving our database modernization efforts, ensuring scalability, performance, and security in the Azure cloud environment.
Responsibilities:

Lead the modernization and automation of on-premises SQL databases, implementing best practices and efficient processes to enhance performance, scalability, and reliability.
Design and execute successful migration strategies from on-premises databases to Azure cloud, ensuring seamless data transfer and minimal downtime.
Develop and implement automation solutions using tools such as Python, PowerShell, and Azure DevOps (ADO) to streamline database management tasks, including provisioning, backup and recovery, monitoring, and deployment processes.
Collaborate closely with cross-functional teams to gather requirements, understand business needs, and propose effective database solutions that align with the overall technical architecture in the Azure cloud environment.
Utilize tools like Liquibase or similar to manage database schema changes and version control.
Implement effective monitoring and logging solutions using tools like Splunk and Datadog to ensure database performance, availability, and security.
Perform thorough assessments and evaluations of existing on-premises databases, identifying areas for improvement, optimization, and consolidation prior to migration to Azure cloud.
Ensure data integrity, security, and compliance with industry standards and regulations throughout the database modernization and migration process in the Azure cloud environment.
Provide guidance and mentorship to junior database engineers, fostering their technical growth and promoting best practices in database management, automation, and migration in the Azure cloud.
Stay up to date with the latest trends, tools, and technologies in database management, Azure cloud services, and DevOps practices, evaluating their potential impact and relevance to our organization.
Collaborate with vendors and third-party providers to assess and implement new technologies, tools, and services that can enhance our Azure cloud database environment and support our modernization goals.

Qualifications:

Bachelor's degree in computer science, information technology, or a related field.
Minimum of 10 years of hands-on experience in database engineering, with a strong focus on SQL databases and Azure cloud.
Extensive experience in modernizing and automating on-premises SQL databases, including performance optimization, scalability improvements, and process automation.
Proven expertise in successfully migrating on-premises databases to Azure cloud, ensuring data integrity and minimal disruption to operations.
Strong proficiency in SQL scripting, database performance tuning, backup and recovery, and database security practices in the Azure cloud environment.
Experience with database migration tools and technologies, such as Azure Database Migration Service, Azure Data Factory, or similar tools.
Solid understanding of Azure cloud database services, including Azure SQL Database, Azure SQL Managed Instance, and Azure Cosmos DB.
Familiarity with automation and configuration management tools like Python, PowerShell, and Azure DevOps (ADO) for streamlining database management and deployment processes in Azure.
Experience in using tools like Liquibase or similar for managing database schema changes and version control.
Knowledge of monitoring and logging solutions like Splunk and Datadog for database performance and security monitoring in Azure cloud.
Strong analytical and problem-solving skills, with the ability to troubleshoot complex database issues and propose effective solutions in the Azure cloud environment.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.

Tech Stack:

Databases: On-premises SQL databases (e.g., Oracle, MySQL, PostgreSQL)
Cloud Platform: Microsoft Azure (Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB)
Scripting and Automation: Python, PowerShell
Version Control: Git
CI/CD: Azure DevOps (ADO)
Database Migration: Azure Database Migration Service, Azure Data Factory
Database Schema Management: Liquibase or similar tools
Monitoring and Logging: Splunk, Datadog

Thanks and Regards
Prasad Mamidela | Kairos Technologies Inc.
Direct Number: 972.777.9484 | Mobile: 201.613.3664
433 E Las Colinas Blvd, # 1240, Irving, TX 75039 USA
http://www.kairostech.com​
LinkedIn: https://www.linkedin.com/in/prasadmamidela
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

11+ years

Schedule:

Monday to Friday

Ability to commute/relocate:

Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)

Experience:

Azure: 4 years (Required)
SQL: 6 years (Required)
On-premises: 2 years (Required)
Total IT: 10 years (Required)
Azure DevOps: 2 years (Required)

Work Location: In person"
Data Engineer,KEA,"Rockwall, TX 75087",Posted 15 days ago,Full-time,https://www.indeed.com/rc/clk?jk=ab2b09cdc1120e91&fccid=7f54ef3ce9a55e38&vjs=3,"We have an immediate need for a Data Engineer whose primary role will be compiling comprehensive data and information on energy and commercial real estate data, and managing the back-end framework while helping the team utilize the data to create front-end visuals. This also involves architecting the back-end system to be scalable in use with web applications. Use multiple software platforms and develop and create reports and graphics to show trends and analysis for internal and external reports. The role is based in Rockwall, 10 miles east of Dallas, Texas. We are a team-first culture that involves leaning on the expertise around you, so any experience in this regard is helpful.


 Analyze, automate, and prepare data for filings in certain key markets
Track, maintain and disseminate detailed data for major CRE markets, including: Leasing activity and comparables, Building and land sales activity and comparables, Ownership analysis
Utilize shape files and platforms such as Mapbox and ArcGIS and tie back to big data
Prepare and develop a monthly market report detailing sales transactions at the submarket level in major markets
Analyze and integrate large amounts of oil & gas data into our system
Work with the business development team to prepare external market reports as thought leadership in the marketplace
Prepare and create multiple template reports out of multiple software systems




Bachelor's degree in economics, real estate, computer science, engineering, or finance
Usage and full understanding of shape files and a platform such as Mapbox or ArcGIS
Understanding of Python, C#, and R
Web application building a plus
3+ years of work experience
Previous usage of Tableau or Alteryx
Strong data and analysis skills including Excel and Microsoft Office
Strong communication skills, with the proven ability to coherently ideas and opinions using the data and communicate them




Full health benefits
1-2 days at home per week
Full dental benefits
401(k) 4% automatic contribution
Competitive compensation
9/80 opportunities"
Big Data Engineer with Active VOS,"Pyramid Consulting, Inc","Dallas, TX",Posted 14 days ago,"$115,000 a year",https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bjic9BpODao-m9BEup4myv2yv9o6hanv70kCRpjMjSDb0tRlE4H9I0NFhFwwd29vaMPFSsGY5bemtuTxVH8QBWyQRQULpv_acqngu7TkPNrov50Fb5U5vPKzqBgo5q0vWB_jrwMBoZp3IGvxaFbl5gt4b7_hrx-MmOzYP4zXi2F9GdW1Aj5X2sLC0fKbY3aQNAEARxdk4hiFIoLowsb2ZjIVaV4LtoTo8sK8ju2j2Msja74Q0COc8M1ItXJ8KmZjp3ALywx3flF_3I-838uOAwXTQK5YdX5A06KR6tEeNEAb-elv2nyhc-ywocEyL6vPNAsXPsbhe5SivhtbaFaou124lMtwdaCrBpcWO6BqxyVnn4mHLN3BnOG8Q6vPn0qRMxwaQkiG12ionMv-taJgbHA70jq1L7GFFMPFZxM0F-DjF98CQ1nIH5-OitHxw_Iq4a4gDtykfXR2WhMlV43_pk5U9ytiBNZzKLrYiQxjjyGY9JDbj3mkljsvxSRUOxHvjesuhw97xHyC6TIbR25nd_l6UzTWX_COsQi_t3D6VzkVfWuWN-mw3kom5O8v7iTSY2A1joV8XrLALBqta6u85aZxFrYOKMm9C40svYZkTZfKcD7dy3U5Hxk5qiaUvFTL23twSaHXqVP2I3c9gyMm7ngQf_fE90DbGLHK-tOVq8ZwwXmwtcUA-5aEwzoH0p04ULL0akEuT_QTN25kxP1vZEM_tDkJcp9V6QUEyan5-wp-sJ3h78jP5ziiFHWyVNCQlc8W7JDfoBWHbhtyh0-4206JbZCq6AHhi80B2drBZPl6ms4C3N1DB7vKBpFaMH3oGmDcFIeCI6qBh63d-YB0ZRrfqaxKPxi7OUsK2kM2sP1gxBTcLTolsNG_ItabZsYBzTYuVezaG5ULb6GBaFA7XAp5NJ_gKaWOYCr_ksqbZHkIqhJSfzekmn8FJjMPGR8fs6z9Lki9hxTDnKVE2duwdqqc3DTwBbbH8UM725mxcTJKqmx-BWJ8-uc7Sn2rs86hx5x8HIpvTdKLjm70H6sztGEBKB5UM7Akn44p46AQc5dvm-gIoWiRTk&xkcb=SoAm-_M3ML2tGex8Zx0EbzkdCdPP&p=13&fvj=0&vjs=3,"Immediate need for a talented
   Big Data Engineer with Active VOS. This is a 
  06+ Months contract opportunity with long-term potential and is located in 
  Dallas, TX, Atlanta, GA (Initial Remote for 1 month and then Onsite). Please review the job description below and contact me ASAP if you are interested.
  

Job ID: 23-29355


Pay Range: $115k/Annum. Employee benefits include, but are not limited to, health insurance (medical, dental, vision), 401(k) plan, and paid sick leave (depending on work location).

 

Key Requirements and Technology Experience:


Overall, 8+ years of experience in Applications development/deployments/implementation with AIOPs being focus area.
Must have development experience with Java, Cramer, ActiveVOS, BPEL, Flex.
Well versed with product/solution implementation life cycle.
Experience in implementing applications with AIOPS open-source technologies Big Data, Python, NodeJS/Angular with NoSCH.
As part of implementation, should have good knowledge of Integrating front-end and back-end application components developed.
Experience with NoSQL databases such as MongoDB, Elasticsearch and Big data stacks like ELK.
Exposure to cloud-based deployments (GCP/AWS/AZURE/Private).
Knowledge of Machine learning and relevant certification.
Experience with Client is a plus.


  Our client is a leading 
  IT Industry and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration.
  
 Pyramid Consulting, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
  
 #DEL"
GCP Data Engineer,High5,"Irving, TX",Posted 8 days ago,$60 an hour,https://www.indeed.com/rc/clk?jk=7997aafae91b7a53&fccid=9f0bb2dcfc3da429&vjs=3,"Job Title: GCP Data Engineer - Onsite
 Location: Irving, Texas ( 75039)
 Pay Rate: $60/hr / 65/hr

 Job Description
 We are seeking a highly skilled and motivated ETL/ELT GCP Pipeline Engineer to join our team and contribute to the development of a brand-new product/application.
 As an ETL/ELT Pipeline Engineer, you will be responsible for designing and building efficient data integration pipelines using GCP native tools or open-source technologies such as Python and PySpark.
 A candidate with expertise in building APIs would be advantageous.

 Responsibilities:

Designing and developing robust ETL/ELT pipelines to extract, transform, and load data from various sources into our new product/application.
Collaborating with cross-functional teams to understand data requirements and translate them into effective pipeline designs.
Implementing data quality checks and ensuring the accuracy, completeness, and consistency of data throughout the pipeline process.
Optimizing and tuning pipeline performance to ensure efficient data processing and delivery.
Working experience in Airflow DAG
Working with cloud-based technologies, particularly Google Cloud Platform (GCP), and leveraging its native tools for data integration.
Integrating data from diverse sources and formats, including structured, semi-structured, and unstructured data.
Identifying and resolving data integration and transformation issues, ensuring the smooth and reliable flow of data.
Staying up-to-date with industry trends and emerging technologies in the ETL/ELT space to continuously improve pipeline efficiency and effectiveness.


 Requirements:


 Strong experience in designing and building ETL/ELT pipelines using GCP native tools or open-source technologies such as Python and PySpark.
Familiarity with ETL tools like Ab Initio is a plus, but not a must.
Proven ability to work on a brand-new product/application, demonstrating adaptability and problem-solving skills in a dynamic environment.
Solid understanding of data integration concepts, data modeling, and database systems.
Proficiency in SQL and experience working with various databases and data formats.
Excellent analytical and problem-solving skills, with the ability to troubleshoot and resolve complex data-related issues.
Strong communication and collaboration skills to effectively work with cross-functional teams and stakeholders.
Detail-oriented with a focus on delivering high-quality results within project timelines.
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
If you are a motivated and talented ETL/ELT Pipeline Engineer looking to join a dynamic team working on an exciting new product, we would love to hear from you. Apply now and be part of our journey in revolutionizing data integration and analytics."
DATA ENGINEER,TRA'BIAN ENTERPRISES,"Dallas, TX",Posted 30+ days ago,Contract,https://www.indeed.com/rc/clk?jk=8c3f6b8eef7f128d&fccid=acfaa464fb995fc7&vjs=3,"Location: Dallas, TX 


Position Type: Contract 


Position Term: 1 Year 

 All submitted candidates must be on our W2. 

 Candidates can must be able to relocate after Covid. 


SKILLS:
 Data Engineer – 10 years experience 
Enterprise Content Management 
Documentum 
Captiva tool 
Javascript"
Data Engineer - ETL Developer,Verizon,"Remote in Irving, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=8fbbd3aed20318d7&fccid=f7029f63fe5c906e&vjs=3,"When you join Verizon
 Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

 What you’ll be doing...
 At Verizon, we are on a multi-year journey to industrialize our data practices and AI capabilities. Very simply, this means that AI and data will fuel all decisions and business processes across the company. At hundred and thirty six billion dollars in annual revenue, this is a pioneering opportunity to build the data products and talent at a top global telco organization.

 With our leadership in bringing 5G networks nationwide, the opportunity for AI and Data will only grow exponentially in going from enabling new revenue opportunities and exquisite customer experience through real-time data insights. Therefore, it will be critical that this candidate possess previous experience, passion and strong commitment to creating value for Verizon’s customers, employees, stakeholders and the society.

 You will develop and drive the execution of strategies to streamline the data acquisition i.e. collecting the data from transaction systems (GTS, GNT and 3rd party vendors), storing the data into data warehouses, data lakes and public/private cloud platforms, curating and transforming the data to build data products. You will report to the Sr. Manager of data services and interfaces team.

 Wrangling with raw data from large, diverse data sets such as structured data (csv, big Query ..), unstructured data (xml, yaml ..), binary data (avro, parquet ..) from our distribution partners.
 Working closely with the AI&D Data engineering teams to ingest data, transform and expose data for AI/ML model development, curation and data productization.
 Working with business product owners to understand business requirements and use cases
 Evaluating tools for use case fit, perform vendor/tool comparisons and present recommendations
 Monitoring the code and overall application to ensure seamless performance.
 Ensuring data quality and integrity by implementing data validations, cleaning and curation processes.
 Contributing to the design and development of application standards, common reference architecture.
 Working closely with global teams (onshore and offshore coordination) to deliver the data products as a team.


 What we’re looking for...
 You are excited to work in a cloud environment, supporting development and deployment in the Verizon Grid. You are self-directed and comfortable supporting the data needs of multiple teams, systems and products. You are excited by the prospect of optimizing or even re-designing the architecture to support our next generation of products and dataset creation for modeling purposes.

 You'll need to have:

 Bachelor’s degree or four or more years of work experience.
 Four or more years of relevant work experience.
 Knowledge of ETL & Big Data Tools in Cloud Environment (BigQuery, Dataflow, Dataproc, Pub/Sub, Cloud Composer….)
 Experience in programming languages: Unix shell scripts, Python or Scala.
 Experience working in a fast-paced environment.
 Ability to quickly adapt to changing priorities.


 Even better if you have one or more of the following:

 Bachelor's degree in Computer Science and six or more years of work experience.
 Five or more years of data engineering.
 Master’s degree.
 Google Cloud Certified Professional Cloud Architect Certification
 Experience with data pipeline and workflow management tools: NiFi.
 Experience in building CI/CD pipelines using Jenkins.
 Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working with a variety of databases.
 Expertise in Data Warehousing concepts.
 Experience with big data tools: Hadoop EcoSystem (Hive, Oozie, Spark, Kafka, Elastic search, Kibana…)
 Experience with stream-processing systems: Spark-Streaming, Storm etc.
 Experience transforming complex data into easily understandable and actionable information.


 If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

 This role is eligible to be considered for the Department of Defense SkillBridge Program.

 Why Verizon?

 Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.

 We are a ‘pay for performance’ company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.
 Your benefits are market competitive and delivered by some of the best providers.
 You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.
 We offer generous paid time off benefits to help you manage your work life balance and opportunities for flexible working arrangements*.
 Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.
 You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.


 Your benefits package will vary depending on the country in which you work.

subject to business approval









 Where you’ll be working






 In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
 Scheduled Weekly Hours 40
 
 Equal Employment Opportunity
 We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more."
Azure Data Engineer,Allruva Technology Services,"Irving, TX 75063",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=4bc9aaac90703b3b&fccid=a7c411829b326dc6&vjs=3,"Job Id: 2023010 Job Summary:The Data Engineer is responsible for all aspects of the design, development, security, capacity planning, delivery of data and database solutions and day-to-day operations. This position will involve working with PaaS SQL solutions, Azure data Factory, Modern Data stacks and Azure DevOps.





 


Responsibilities

Constantly reviews overall physical database structures for data integrity, performance, recoverability and space requirement considerations.
Effectively manage day-to-day tasks / activities in coordination with a team of developers to meet the deliverables and schedule of a data solution component within a larger application project
Closely monitors system performance, identifies problems, and implements solutions in a timely manner.
Researches and recommends optimal data storage solutions, publishes standards and policies, and works with development staff to standardize the environment and improve efficiency.
Ensures work plans across assigned teams are coordinated to meet dates and expectations. Meets application team deadlines by delivering database solutions as well as completing UAT and Production promotions on time. The ability to multi-task is essential.
Lend support to various business and technology teams as necessary during design, development and delivery to ensure solid, scalable, robust solutions.
Responsible for the integrity of Production, Development, and User Acceptance Testing environments.
Evaluates and recommends data storage solutions.
Where necessary, it provides a High Availability environment utilizing Always-On, Clustering, cloud solutions, or other advanced technical database solutions.

Requirements:

Minimum 5+ years’ experience in information systems or equivalent, Minimum 3+ years of experience with implementing data solutions in Azure, including Azure Data Factory, Azure Data Lake Store, and other relevant services.
Administer databases with regards to efficiency, data security, user security, and system integrity.
Complete knowledge of SQL. Ability to read, write and debug T-SQL. Experience in the use of SQL for query and transaction processing in a high-performance environment.
Experience with Modern Data Stack – Azure Data Lake, Gen2 Storage, Synapse, Databricks.
Good knowledge of ETL tools, SSIS and Azure Data Factory, experience with building pipelines in Azure Data Factory and Synapse pipelines.
Experience with building and managing Ci/Cd using Azure DevOps and GitHub.
Experience with build-tools like MSBuild or Maven.
Experience with unstructured (BIG) data is a plus.
Experience with implementing data encryption techniques.

Note: Interested candidates may reach out to us with their resumes on bobby.d@allruva.com"
Data Engineer,Galderma,"Dallas, TX 75201 (Downtown area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=ef16cad41ddc9efc&fccid=f747d980a7349998&vjs=3,"With a unique legacy in dermatology as well as decades of cutting-edge innovation, Galderma is the pure-play dermatology category leader, present in approximately 90 countries. We deliver an innovative, science-based portfolio of premium flagship brands and services that spans the full spectrum of the fast-growing dermatology market through Injectable Aesthetics, Dermo-cosmetics, and Therapeutic Dermatology. Since our foundation in 1981, we have dedicated our focus and passion to the human body’s largest organ – the skin – meeting individual consumer and patient needs with superior outcomes in partnership with healthcare professionals. Because we understand that the skin we’re in shapes our lives, we are advancing dermatology for every skin story.
 


   We look for people who focus on getting results, embrace learning and bring a positive energy. They must combine initiative with a sense of teamwork and collaboration. Above all, they must be passionate about doing something meaningful for consumers, patients, and the healthcare professionals we serve every day. We aim to empower each employee and promote their personal growth while ensuring business needs are met now and into the future. Across our company, we embrace diversity and respect the dignity, privacy, and personal rights of every employee.
 


   At Galderma, we actively give our teams reasons to believe in our bold ambition to become the leading dermatology company in the world. With us, you have the ultimate opportunity to gain new and challenging work experiences and create an unparalleled, direct impact.
 


   Job Title: Data Engineer
 


   Location: Dallas, TX (Hybrid)
 


   Job Description
 

   The Data Engineer leads initiatives to develop and implement business-driven BI and Analytics solutions to enable business to win in the markets and gain competitive advantage. This individual serves as the primary liaison between IT Analytics, Shared Services, and the Business Units.
 


   Key Responsibilities
 

 Understand customers’ overall data estate business, related success measures, and IT priorities in order to design data solutions that drive business value
 Educate the business and promote best practices on how to leverage BI and analytics solutions, as well as help facilitate discussions to anticipate future needs and opportunities
 Assist in the identification and integration of data sources
 Clearly communicate findings, recommendations, and opportunities to improve data systems and solutions
 Seek out information to learn about emerging methodologies and technologies
 Work closely with DevOps team to automate the deployment of resources to our various Azure subscription
 Performs technical design reviews and code reviews



   Skills & Qualifications
 

 Bachelor’s degree in Information Technology or a related field, required
 5 years of experience in implementing DWH / BI solutions
 3 years of experience leading projects in analysis, architecture, design, and development of traditional data warehouse, data pipeline and business intelligence solutions
 3 years of experience as a Data Engineer using Data Brick and Snowflake
 Good understanding of various Data Models such as Dimensional Data Modeling and DataVault Modeling
 Experience with Azure cloud services such as azure cloud security, monitoring, logging, scaling, caching, etc.
 Experience dealing with cloud workloads, application, and infrastructure architecture, data ingestion architecture, data storage, and transformations, data analytics, serverless function/application design, micro-services, DevOps
 Experience with Linux and cloud environment including commands and scripting
 Solid experience in writing complex SQL queries as well as PL/SQL programs on Oracle database
 Familiarity with DevOps best practices and automation of documentation, testing, build, deployment, configuration, and monitoring
 Experience in visualization tools/solution such as Tableau and Power BI
 Solid communication skills, particularly in writing technical solution proposals



   What we offer in return
 

   You will be working for an organization that embraces diversity & inclusion and believe we will deliver better outcomes by reflecting the perspectives of our diverse customer base. You will also have access to a range of company benefits, including a competitive wage with shift differential, annual bonus opportunities and career advancement and cross-training.
 


   Next Steps
 

 If your profile is a match, we will invite you for a first virtual conversation with the recruiter.
 The next step is a virtual conversation with the hiring manager
 The final step is a panel conversation with the extended team



   Our people make a difference:
 

   At Galderma, you’ll work with people who are like you. And people that are different. We value what every member of our team brings. Professionalism, collaboration, and a friendly, supportive ethos is the perfect environment for people to thrive and excel in what they do.
 


   Employer’s Rights:
 

   This job description does not list all the duties of the job. You may be asked by your supervisors or managers to perform other duties. You will be evaluated in part based on your performance of the tasks listed in this job description. The employer has the right to revise this job description at any time. This job description is not an employment contract, and either you or the employer may terminate employment at any time, for any reason. In addition, reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position."
"Data Engineer, Senior",Toyota,"Plano, TX 75024",Posted 28 days ago,Full-time,https://www.indeed.com/rc/clk?jk=dfaa6409532c05c8&fccid=511fb7be73754c38&vjs=3,"Overview

 Who we are


   Collaborative. Respectful. A place to dream and do. These are just a few words that describe what life is like at Toyota. As one of the world’s most admired brands, Toyota is growing and leading the future of mobility through innovative, high-quality solutions designed to enhance lives and delight those we serve. We’re looking for diverse, talented team members who want to Dream. Do. Grow. with us.
 

 An important part of the Toyota family is Toyota Financial Services (TFS), the finance and insurance brand for Toyota and Lexus in North America. While TFS is a separate business entity, it is an essential part of this world-changing company – delivering on Toyota’s vision to move people beyond what’s possible. At TFS, you will help create best-in-class customer experiences in an innovative, collaborative environment.

 Who we're looking for
 Toyota Financial Services (TFS) is looking for highly motivated and hands-on people to fill multiple openings as Data Engineer, Senior.
 The primary responsibilities of these roles are to prototype, build, operationalize, and support full-stack data solutions across the entire data processing pipeline. Aligning with the solution architecture team, these roles will develop and productionize reusable and repeatable data frameworks to leverage and extend existing data feeds to deliver new capabilities to support TFS’ digital modernization.
 Reporting to the Manager, Data Engineering, these roles will participate in functional requirements, define non-functional requirements, and build a pipeline for data to be stored, consumed, integrated, and managed by different data entities and systems, both operational and analytical systems using or processing that data.

 What you'll be doing

 Apply data & systems engineering principles to develop code spanning the data lifecycle including ingest, transform, consume end to end from source to consumption for operational and analytical workloads that minimize complexity and maximize business value.
 Work as part of an agile scrum team to deliver business value.
 Participate in design sessions to understand customers functional needs.
 Work with solution architect and development team to build quick prototypes leveraging existing or new architecture.
 Provide end to end flow for data process, map technical solutions to the process.
 Develop & deploy code in continuous development pipelines leveraging off-the-shelf and open-source components of Enterprise Data Warehouse, ETL and Data Management processes adhering to the solution architecture.
 Perform software analysis, code analysis, requirements analysis, release analysis and deployment.


 What you bring

 Bachelor’s degree
 The candidate must have hands-on development experience in distributed, analytical, cloud-based and open-source technologies
 Experience building software for Data Ingestion & Data Movement ETL pipelines for Operational and Analytical systems
 Expertise coding and implementing data pipelines in cloud-based data infrastructure, analytical and no-SQL databases (AWS, Snowflake, MongoDB, Postgres)
 Hands-on programming experience in Python and/or Java, SnowSQL
 Experience leveraging build and deploy tools (Github, Gradle, Maven, Jenkins)
 Implementing software leveraging flow-based pipelines such as NiFi or Airflow and Streaming services such as Kafka


 What we’ll bring

 During your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include:

 A work environment built on teamwork, flexibility, and respect.
 Professional growth and development programs to help advance your career, as well as tuition reimbursement.
 Vehicle purchase & lease programs.
 Comprehensive health care and wellness plans for your entire family.
 Flexible work options based on business needs.
 Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute.
 Paid holidays and paid time off.
 Referral services related to prenatal services, adoption, childcare, schools and more.
 Tax Advantaged Accounts (Health Savings Account, Health Care FSA, Dependent Care FSA).
 Relocation assistance (if applicable).


 Belonging at Toyota

 Our success begins and ends with our people. We embrace diverse perspectives and value unique human experiences. Respect for all is our North Star. Toyota is proud to have 10+ different Business Partnering Groups across 100 different North American chapter locations that support team members’ efforts to dream, do and grow without questioning that they belong. As a company that has been one of DiversityInc’s Top 50 Companies for Diversity and a member of The Billion Dollar Roundtable supporting minority and woman-owned suppliers for over 10 years, we are proud to be an equal opportunity employer that celebrates the diversity of the communities where we live and do business.

 Applicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law.

 Have a question or need assistance with your application? Please send an email to talent.acquisition@toyota.com."
Senior Data Engineer- AWS,JPMorgan Chase & Co,"Plano, TX 75024",Posted 13 days ago,Full-time,https://www.indeed.com/rc/clk?jk=5f7476a8481136e9&fccid=c46d0116f6e69eae&vjs=3,"JOB DESCRIPTION
 If you're interested in making significant contributions to your team and business, you're heading the right direction, but even more so- with our Data & Analytics team, the difference we make is to help provide tools and resources that give senior management what they need to make decisions that will keep JP Morgan Chase ahead of the competition through leveraging data across Chase to build advantages for the businesses while providing value and protection for customers.
 Job summary
 The Data Analytics and Reporting (DART) team’s mission is to provide high-quality dashboards & insights that executive leadership will use to monitor KPIs, steer the business, and make investment decisions. This team will constantly balance the need for speed with the importance of providing accurate results, to enable leadership to make informed decisions with agility & precision. The position will be part of a team that will take a loosely-defined idea or problem statement, find & acquire the relevant data, design & build the dashboards, develop the automated workflows, and document the details. You will also help to develop frameworks to ensure cross-platform data & logic consistency and optimization. They will write code to enrich and transform data sets to produce output that will help to advance the DART teams objectives and impact to the broader organization.
 Job responsibilities

Perform user acceptance testing and deliver demos to stakeholders by SQL queries or Python scripts.
Perform data analysis to define / support model development including metadata and data dictionary documentation that will enable data analysis and analytical exploration
Participate in strategic projects and provide ideas and inputs on ways to leverage quantitative analytics to generate actionable business insights and/or solutions to influence business strategies and identify opportunities to grow 
Partners closely with business partners to identify impactful projects, influence key decisions with data, and ensure client satisfaction

 Required qualifications, capabilities, and skills

Scripting skills e.g. Python, R and experience with tools such as Alteryx or similar tool
Strong understanding of data and database methodologies as well as hands on Oracle and/or AWS Cloud experience
Bachelor’s degree in a relevant quantitative field (e.g. Statistics, Economics, Finance, Business Analytics, Mathematics, Engineering, Computer Science, Information Technology) 
5+ years of industry experience in business analytics roles (e.g., marketing analytics, product analytics, business insights) 
5+ years of work experience across broad range of analytics platforms, languages, and tools (SAS, SQL, Spark and Python, Unix, Excel Pivot etc.); hands-on experience using big data platform required
Strong understanding of CI/CD Pipelines in a globally distributed environment using Git, Bit-Bucket, Jenkins, Spinnaker, etc.
Experience with the entire Software Development Life Cycle (SDLC) including planning, analysis, development and testing of new applications and enhancements to existing application.

 Preferred qualifications, capabilities, and skills

Master's degree or Bachelor degree with 5+ years experience in a relevant quantitative field (e.g. Statistics, Economics, Finance, Business Analytics, Mathematics, Engineering, Computer Science or any related fields involving significant quantitative research & data analytics)
Strong communicator who's able to convey complex information in an understandable, compelling, and persuasive manner to business partners
Results-oriented with a strong attention to details
Strong knowledge in quantitative methods for business analytics; proficiency in critical thinking and problem solving

ABOUT US
 Chase is a leading financial services firm, helping nearly half of America’s households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the “WELL Health-Safety Rating” for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm’s current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm’s vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
 We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.

Equal Opportunity Employer/Disability/Veterans



ABOUT THE TEAM

 Our Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We’re proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions – all while ranking first in customer satisfaction."
Vice President; Data Engineer II,Bank of America,"Plano, TX 75024",Posted 7 days ago,Full-time,https://www.indeed.com/rc/clk?jk=e4ce386e5d97824b&fccid=5bd99dfa21c8a490&vjs=3,"Job Description:

 At Bank of America, we are guided by a common purpose to help make financial lives better through the power of every connection. Responsible Growth is how we run our company and how we deliver for our clients, teammates, communities and shareholders every day.
 One of the keys to driving Responsible Growth is being a great place to work for our teammates around the world. We’re devoted to being a diverse and inclusive workplace for everyone. We hire individuals with a broad range of backgrounds and experiences and invest heavily in our teammates and their families by offering competitive benefits to support their physical, emotional, and financial well-being.
 Bank of America believes both in the importance of working together and offering flexibility to our employees. We use a multi-faceted approach for flexibility, depending on the various roles in our organization.
 Working at Bank of America will give you a great career with opportunities to learn, grow and make an impact, along with the power to make a difference. Join us!

 RESPONSIBILITIES:

 Develop and deliver data/ reporting solutions to accomplish technology and business goals.
 Code solutions to integrate, clean, transform, and control data in operational and/or analytics data systems per the defined acceptance criteria.
 Assemble large, complex data sets to meet functional reporting requirements.
 Build processes supporting data transformation, data structures, metadata, data quality controls, dependency, and workload management.
 Define and build reporting applications that enable better data-informed decision-making.
 Contribute to existing test suites (integration, regression, performance), analyze test reports, identify any test issues/errors, and triage the underlying cause.
 Document and communicate required information for deployment, maintenance, support, and business functionality.
 Work closely with business partners to help translate functional requirements into technical approach, design, and decisions- Banking & Markets business acumen.
 Create MicroStrategy schema objects, complex attributes / metrics, conditional and level metrics, and their use within a report.
 Develop MicroStrategy dossiers & Tableau dashboards and 3rd party application report integration.
 Use SQL, data warehouse concepts /architecture, dimensional modeling, and ETL solution design.
 Tune and optimize query performance for large datasets-cubes, caching, aggregate structures within MicroStrategy, Tableau and various RDBMS, Hadoop backend systems.


 REQUIREMENTS:

 Bachelor’s degree or equivalent in Computer Science, Computer Information Systems, Management Information Systems, Engineering (any) or related; and
 5 years of progressively responsible experience in the Job offered or a related IT occupation.
 Working closely with business partners to help translate functional requirements into technical approach, design, and decisions- Banking & Markets business acumen;
 Creating MicroStrategy schema objects, complex attributes / metrics, conditional and level metrics, and their use within a report;
 Developing MicroStrategy dossiers & Tableau dashboards and 3rd party application report integration;
 Using SQL, data warehouse concepts /architecture, dimensional modeling, and ETL solution design; and,
 Tuning and optimizing query performance for large datasets-cubes, caching, aggregate structures within MicroStrategy, Tableau and various RDBMS, Hadoop backend systems.


 If interested apply online at www.bankofamerica.com/careersor email your resume to bofajobs@bofa.comand reference the job title of the role and requisition number.

 EMPLOYER: Bank of America N.A.

 Job Band: H5
 
 Shift: 1st shift (United States of America)
 
 Hours Per Week: 40
 
 Weekly Schedule:

 Referral Bonus Amount: 0"
Data Engineer,Virtualware Innovations,"Dallas, TX 75204 (M Streets area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=2231661f3e8645ab&fccid=57077bd28d6e3f33&vjs=3,"Job Description
5+ years of experience in IT
Excellent knowledge in SQL and SSIS packages
Should have worked in GCP for more than 6 month
Good to have knowledge in Hadoop/spark ( any of this)
Should have basic knowledge in ETL
Good to have skill set: DB2 & Informix
Skillset Required – GCP, Spark , PySpark and Python, ETL tools , SQL, SSIS"
Data Engineer (Full Time; Multiple Openings),RingCentral,"Dallas-Fort Worth, TX",Posted 30+ days ago,"$153,000 a year",https://www.indeed.com/rc/clk?jk=3c5a820ca6623b13&fccid=1cbb498b08d4e46a&vjs=3,"JOB DESCRIPTION: Responsible for building and maintaining solutions around the various RingCentral distributed database technologies.
  




 JOB DUTIES: Develop and enhance RingCentral’s distributed databases and data infrastructure; Develop high-quality, high-performance distributed systems in Python, SQL and Java; Build robust, reliable, automated data pipelines using Kafka and Spark streaming; Develop upon and integrate with other services within the RingCentral application and development stacks; Work closely with other teams to understand and mitigate issues and improve performance; Work closely with RingCentral’s operations teams to help develop and optimize solutions; Work with large data volumes, including processing, transforming and transporting large-scale data using big data stacks; Design, build and launch new data models in production; Design, build and launch new data extraction, transformation and loading processes in production.
  




 REQUIRED SKILLS/TOOLS: Experience with Hadoop, HDFS, Hive, HBase, MongoDB, ElasticSearch, Vertica, Amazon Redshift, Oracle, Python, SQL, Java, Kafka, Spark, MapReduce, Hive SQL, Python, C/C++, Unix, and Linux is required.
  




 EDUCATION/EXPERIENCE /
  



    QUALIFICATIONS: U.S. Master’s degree or foreign equivalent in Computer Science, Information Systems or a related field plus two (2) years of related experience, or U.S. Bachelor’s degree or foreign equivalent in Computer Science, Information Systems or a related field plus five (5) years of related experience is required. Within the foregoing parameters, any suitable combination of education, training and experience is acceptable.
  




 *** Documentary verification of education/experience required ***
  




 JOB LOCATION: Dallas, TX
  




 SALARY: $153,000 per year"
Junior Big Data Platform Engineer,G-Research,"Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=afab2f6f4dc5fee6&fccid=4fab22f542766288&vjs=3,"Location : Dallas, TX 

This is a hybrid role, based in G-Research’s office in Dallas.






 G-Research is Europe’s leading quantitative finance research firm. We hire the brightest minds in the world to tackle some of the biggest questions in finance. We pair this expertise with machine learning, big data, and some of the most advanced technology available to predict movements in financial markets.

 Opened in 2022, the Dallas office is a key infrastructure hub where we work on the latest technologies in a cutting-edge environment.

 The role

 Our business focuses on forecasting financial markets and we use an ever-growing amount of data and processing to achieve this.

 The Big Data Platform team provides open source analytics and storage platforms that are key to enabling business-critical functions. It is responsible for cutting-edge clusters, which underpin diverse use-cases such as quantitative research, risk analysis and cyber security.

 As a Junior Big Data Platform Engineer, you will work closely with other teams across the business, such as Quantitative Research and various development teams; it is vital to maintain effective close relationships with these teams in order to understand their use-cases and challenges, and help users get the most out of Big Data Platforms.

 At such a scale, automation is key, and there is a focus on containerisation, container orchestration, configuration management, orchestration, Infrastructure as Code and CI/CD for this role.

 Key responsibilities of the role include:

 Researching and implementing new technologies in line with key business objectives
 Helping to shape and engineer the Big Data Platform, ensuring it is scalable, stable, and performant, as well as easy to use and maintain
 Providing metrics, documentation, and self-service infrastructure to help our users work at pace and get the most out of the platform
 Implementing and maintaining automation
 Using advanced troubleshooting skills to diagnose and fix problems


 Who are we looking for?

 We’re looking for an aspiring Platform Engineer who is enthusiastic about contributing to an automated, scalable, reliable and high performing Big Data Platform.

 The ideal candidate will have:

 A strong desire to continually learn about new technologies, approaches and systems, along with the agility to work across multiple disciplines
 A willingness to learn by working with users across different business areas, experiences and cultures to drive towards the best outcome for G-Research
 Experience with at least one programming language


 The following technology experience is beneficial but not essential:

 Big Data:
       
 Spark
 Trino/Hive
 Cloud technologies (e.g. AWS EMR, Dataproc)

 Automation/operational:
       
 Kubernetes
 Dev Ops principals
 Linux OS core principles
 Prometheus / ELK



 Why should you apply?

 Market-leading compensation plus annual discretionary bonus
 Informal dress code and excellent work/life balance
 Excellent paid time off allowance
 Sick days, military leave, and family and medical leave
 Generous 401(k) plan
 12-weeks’ fully paid parental leave
 Medical and Prescription, Dental, and Vision insurance
 Life and Accidental Death & Dismemberment (AD&D) insurance
 Employee Assistance and Wellness programs
 Generous relocation allowance and support
 Great selection of office snacks, and hot and cold drinks
 On-site gym and car park





G-Research is committed to cultivating and preserving an inclusive work environment. We are an ideas-driven business and we place great value on diversity of experience and opinions.
We want to ensure that applicants receive a recruitment experience that enables them to perform at their best. If you have a disability or special need that requires accommodation please let us know in the relevant section."
Senior Software Engineer (Senior Azure Data Engineer),"Vizient, Inc.","Irving, TX 75062 (Las Colinas Urban Center area)",Posted 19 days ago,"$102,400 - $152,200 a year",https://www.indeed.com/rc/clk?jk=d63742ebd05f18c7&fccid=ab22e7c357e67bd4&vjs=3,"When you’re the best, we’re the best. We instill an environment where employees feel engaged, satisfied and able to contribute their unique skills and talents while living and working as their authentic selves. We provide extensive opportunities for personal and professional development, building both employee competence and organizational capability to fuel exceptional performance through an inclusive environment both now and in the future.
 


   Summary:
 


   In this role, you will use best practices and knowledge of internal and external business operations to improve products and/or services. You will solve complex problems and bring a new perspective to the usage of existing solutions. You will also mentor employees in the immediate group.
 


   Responsibilities:
 

 Design, develop, enhance, code, test, deliver and debug software independently across multiple products.
 Implement larger, more complex, or new stories for multiple products.
 Play an active role in story breakup and refinement sessions.
 Drive and lead story level architecture/design sessions.
 Participate in feature level architecture/design sessions.
 Recommend actions to improve procedures and standards.
 Stay up to date on technical trends and emerging technology.
 Proactively improve standards and procedures.


 Qualifications:
 

 Relevant degree preferred.
 5 or more years of experience in a software development role required.
 Exceptional analytical and conceptual thinking along with logical and physical database design skills required.
 Strong aptitude and experience in writing and troubleshooting SQL and T-SQL required.
 Data analysis, data modeling, and data integration using Azure technologies like Azure Data Factory (ADF) required.
 Experience with Azure SQL and cloud data solutions within a data warehouse environment, using multiple data sources, data lakes, etc. preferred.
 Hands on experience with Azure Synapse, Azure Stream Analytics, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2, & Logic Apps strongly preferred.
 Experience with Python, Spark, Hive, Hbase and other bigdata technologies preferred.
 Extensive experience working with enterprise solution delivery in a large-scale distributed software design environment is preferred.
 Experience working in an Agile based development environment, using Agile concepts such as Continuous Integration, TDD (Test Driven Development), and Paired Programming preferred.
 You must be authorized to work in the United States without sponsorship.



   Estimated Hiring Range:
  $102,400.00 - $152,200.00
 

   This position is also incentive eligible.
 


   Vizient has a comprehensive benefits plan! Please view our benefits here:
 



    http://www.vizientinc.com/about-us/careers
  



   Equal Opportunity Employer: Females/Minorities/Veterans/Individuals with Disabilities
 


   The Company is committed to equal employment opportunity to all employees and applicants without regard to race, religion, color, gender identity, ethnicity, age, national origin, sexual orientation, disability status, veteran status or any other category protected by applicable law."
Support Engineer-Informatica Data Quality,Persistent Systems,"Dallas, TX",PostedToday,Rotating shift,https://www.indeed.com/rc/clk?jk=c2ee6ed859c2325a&fccid=b90433ed96d63887&vjs=3,"Job Description




""Production migration/deployment of ETL objects & associated files through CICD pipelines- Provide L1 (Monitoring), L1.5 (Basic Dev & incident management) support, L2 (Complex Incident Management), L3 (Complex Dev) support from the Persistent Offshore Delivery Center(ODC) set up.- Continuous Platform Optimizations to improve Resiliency, Up-time, and resolution of Vulnerabilities within SLA- Enforce platform governance & standards- Platform Support for Informatica EDC Major, Minor & Ad hoc releases (EBFs)- Development and refinement of dashboards for monitoring capacity planning and platform resiliency- Execute annual Disaster Recovery & Password rotation exercises- Assist with Audit requests- Platform Health Reporting, Weekly and Monthly Executive Status Reporting - Implement Continuous Improvement & Automation opportunities- Platform Upgrade Support- To manage and support the platforms 24x7 on rotational shift basis. - To learn new platform management technologies and tools as a part of the cross-skilling exercise in Project life cycle. - Customer / Stakeholder management"""
SiteOps Data Center Production Operations Engineer,Meta,"Garland, TX",Posted 28 days ago,"$72,488 - $122,990 a year",https://www.indeed.com/rc/clk?jk=a01a9a5792332e28&fccid=ba07516c418dda52&vjs=3,"Facebook is seeking a forward thinking experienced Engineer to join the Production Operations team within Data Center Operations. Our data centers, and the tens of thousands of servers installed in them, are the foundation upon which our rapidly scaling infrastructure efficiently operates and upon which our innovative services are delivered. Facebook is at the leading edge of the global data center industry both in terms of how data centers are designed and operated. This person should enjoy working in a fast paced environment where adaptability and flexibility will be key to their success. We seek an IT professional with advanced hands-on technical skills in Networks, Server Hardware and Linux (ideally in a Data Center environment). Having extensive knowledge of managing servers and performing complex projects in a large-scale distributed data center environment is a core competency of this individual. The candidate should also have deep knowledge and experience in at least one of the following core areas: Networking, Project Management, Tool and Automation, Hardware and OS repair.
 


SiteOps Data Center Production Operations Engineer Responsibilities:  

Perform deep dives and analyze complex technical issues within the data center, ranging from automated tooling to hardware failures and network issues.
 Work as a technical lead with cross functional teams on large scale data center projects and initiatives.
 Provide cross data center support and identify potentially larger issues, displaying effective communication when something is identified.
 Work with internal hardware teams and vendors to help resolve complex technical issues, maintain high hardware quality levels and influence future design to ensure ease of serviceability.
 Understand/analyze issues and be able to update and develop scripts and smaller sets of software.
 Use data to drive maximum server fleet up-time and utilization rates, by understanding hardware failure rates and SLAs to customers. Identify trends and systemic issues in the fleet and drive resolution.
 Mentor team members to evaluate and identify better ways to resolve issues and define updates to tools and processes.
 Provide guidance and mentor technical leads and the go-to technical resource for management.
 Build cross functional relationships and have the ability to influence policies and procedures to improve global data center operations.
 Participate in an on-call rotation.




Minimum Qualifications: 

 Knowledge of Linux and hardware systems support in an Internet operations environment.
 Knowledge of the interdependencies of data center functions and technologies.
 Knowledge of out-of-band/lights-out server communication methods, such as IPMI and serial console.
 Experience managing multiple projects within the same time schedule.
 Knowledge of enterprise level networking and storage equipment installations.
 BS, BA or BEng in technical field or commensurate experience.
 Proven communication skills.
 Time and project management experience.
 5+ years of infrastructure or related experience.
 Experience in modifying and developing in commonly used scripting or programming languages.




Preferred Qualifications: 

 Experience in a large-scale data center environment.
 Experience in providing technical guidance to external vendors.







  Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view Meta's Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the E-Verify program in certain locations, as required by law"
Connected Data Compliance Engineer,PACCAR,"Lewisville, TX 75067",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=b4683c40cfbe93a8&fccid=c2c6a7536e4d9df3&vjs=3,"Company Information


 PACCAR is a Fortune 500 company established in 1905. PACCAR Inc is recognized as a global leader in the commercial vehicle, financial, and customer service fields with internationally recognized brands such as Kenworth, Peterbilt, and DAF trucks. PACCAR is a global technology leader in the design, manufacture and customer support of premium light-, medium- and heavy-duty trucks under the Kenworth, Peterbilt and DAF nameplates and also provides customized financial services, information technology and truck parts related to its principal business.

 Whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services, you can develop the career you desire with PACCAR. Get started!




 Requisition Summary


 The PACCAR Powertrain Organization is seeking a talented engineer to join the newly formed Connected Compliance team to help with new upcoming On-Board Diagnostics (OBD) data reporting to the California Air Resources Board (CARB). This position will utilize data analytics to analyze complex datasets to help validate regulatory compliance of the product and continuously monitor performance to identify issues & improvement opportunities. This position is located in beautiful Lewisville, TX.




 Job Functions / Responsibilities



 Perform OBD data analysis for regulatory compliance reporting of heavy-duty PACCAR Engines operating in North America.
 Conduct statistical data analysis on both, internal test development and production vehicle fleets to inform and report on trends/patterns observed.
 Support testing, validating, and reporting for Production Vehicle Evaluation, Heavy-Duty Inspection & Maintenance, and other programs as required.
 Collaborate with outside suppliers to perform data verification/validation for different vehicle fleets.
 Assimilate, aggregate and query data from remote diagnostics to develop status dashboards.
 Present information using data visualization techniques for easy interpretation of complex datasets.
 Prepare reports, as appropriate, incorporating results, conclusions, and recommendations.
 Establish and maintain cooperative and productive work relations with other departments and divisions.
 Participate in compliance data reviews with other team leads.
 Support other regulatory projects with different teams in North America and Europe as needed.
 Support responding to regulatory agency questions on product compliance.
 Some travel (up to 10%) required





 Qualifications



 Preferred 2 to 5 years of relevant work experience in analyzing and presenting complex datasets from multiple sources.
 Working knowledge of Tableau and SQL preferred.
 Proficiency in programming languages such as MATLAB, VBA, R, or Python is a plus.
 Familiarity with OBD, emissions regulations, and certification.
 Knowledge of vehicle CAN network technologies and associated communication protocols (UDS, J1979, J1939, KWP2000, etc.).
 Strong verbal and written communication skills.
 Ability to thrive in a team environment.
 Ability to manage personal workload and have the willingness, flexibility, and initiative to respond to shifting time & project demands 





 Education


 BS degree in electrical, mechanical, or similar technical field required; Advanced degree preferred.




 PACCAR Benefits


 As a U.S. PACCAR employee, you have a full range of benefit options including:

 401k with up to a 5% company match
 Fully funded pension plan that provides monthly benefits after retirement
 Comprehensive paid time off – Minimum of 10 paid vacation days (additional days are provided with additional seniority/years of service), 12 paid holidays, and sick time
 Tuition reimbursement for continued education
 Medical, dental, and vision plans for you and your family
 Flexible spending accounts (FSA) and health savings account (HSA)
 Paid short-and long-term disability program
 Life and accidental death and dismemberment insurance
 EAP services including wellness plans, estate planning, financial counseling and more





 Additional Job Board Information



 PACCAR is an Equal Opportunity Employer/Protected Veteran/Disability and E-Verify Employer.
 Hiring Location: Lewisville, TX"
Data Engineer,Evaya Data Systems,"Allen, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=762dcef94f0ba06f&fccid=d3963513239ea6de&vjs=3,"The engineering team consists of talented, team-oriented individuals who are empowered to take advantage of the latest cloud and distributed technologies to deliver reliable, high-throughput applications.  As a Data Engineer, you’ll employ your skills on a daily basis to design and build data processing and storage applications to handle millions of transactions per day. You will analyze business requirements and consult with the broader team to ensure successful processing, storage and reporting of our Big Data. You’ll have a wide variety of languages and technologies at your disposal that you can use to solve problems. Your work will directly shape and create our data architecture to ultimately deliver systems that stand up to unpredictable environments at massive scale.
 Technical Skills Needed:

5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.
Growing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:
Strong Educational Background
Hands-on Engineering experience in

Problem solving and debugging skills
Writing and deploying code the Linux, Windows, or cloud environments
Familiarity with algorithms and performance analysis
Willingness to contribute to the operational responsibility of the team’s applications

Some experience with one or more of the following:

Relational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)
Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase"
Data Science/ Machine Learning Engineer,ICS Global Soft,"Irving, TX 75038 (Cottonwood area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=1b3a3771dab71412&fccid=0e79b4b653bf4742&vjs=3,"Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you.


Responsibilities:


Study and transform data science prototypes
Design machine learning systems
Research and implement appropriate ML algorithms and tools
Develop machine learning applications according to requirements
Select appropriate datasets and data representation methods
Run machine learning tests and experiments
Perform statistical analysis and fine-tuning using test results


Requirements:


Proven experience as a Machine Learning Engineer or similar role
Understanding of data structures, data modeling and software architecture
Deep knowledge of math, probability, statistics and algorithms
Ability to write robust code in Python, Java and R
Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)
Excellent communication skills
Ability to work in a team

Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038."
Sr Java Data Engineer,Intone Networks,"Plano, TX 75023",Posted 30+ days ago,Contract,https://www.indeed.com/rc/clk?jk=83d76faaf5fadfbb&fccid=3ed0572c448b2368&vjs=3,"QUALIFICATIONS · 5+ years of total IT experience · 3+ years of experience as a Data Engineer, having built large and complex data pipelines. · Experience designing, building and operationalizing large scale enterprise data solutions and applications using AWS data and analytics services in combination with other services/platforms o Spark, EMR, RedShift, Kinesis, Kinesis Firehose, Athena, Lambda & AWS Glue · Thorough knowledge of AWS concepts for use in day-to-day development regarding security, networking, and other concepts. o This will require understanding how IAM Roles, VPCs, Security Groups, Access Control Lists, Lifecycle Policies, etc., and will impact the products built in this role. o Must also know how to adjust those pieces. · Experience integrating with orchestration tools for data pipeline and workflow management such as: AWS Step Functions, Airflow, etc. · Experience implementing streaming big-data pipelines as well as batch job flows in AWS · Able to implement a change data capture (CDC). Needs to have this experience. · Able to support and contribute towards the building of end-to-end products on AWS in all aspects including Development, DevOps, designing, architecting, testing, and maintenance/operations. · Advanced working SQL knowledge and experience working with relational databases and NoSQL databases. · Experience designing and build production data pipelines from ingestion to consumption within a big data architecture, using Java, Scala. · Must have experience with infrastructure-as-code tools such as Terraform (preferably) or CloudFormation. · Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations. · Understanding of concepts regarding security, privacy, performance, etc. · Kubernetes experience for developing, deploying, and orchestrating microservices."
Data Engineer - APM Technology (Hybrid),Citi,"Irving, TX",Posted 30+ days ago,"$116,880 - $175,320 a year",https://www.indeed.com/rc/clk?jk=486f6b8e6c1ff8fd&fccid=5bcd1ef0a7f4fb99&vjs=3,"The Data Engineer/APM Lead Analyst is a senior-level position responsible for leading a variety of engineering activities including the design, acquisition and deployment of hardware, software and network infrastructure in coordination with the technology team. The overall objective of this role is to lead efforts to ensure quality standards are being met within existing and planned frameworks.  Responsibilities:

 Expert in Application Performance Management/APM technology such as AppDynamics
 Experience creating product strategy
 Define necessary system enhancements to deploy new products and process enhancements
 Experience in automating solutions at wide or global scale
 Performance optimization and root cause analysis
 Exhibit knowledge of how employee's own specialty area contributes to the business and apply knowledge of competitors, products and services
 Advise or mentor junior team members
 Impact engineering functions by influencing decisions through advice, counsel or facilitating services
 Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.  Qualifications:
 6-10 years of relevant experience in an engineering role
 Experience working in financial services or a large complex and/or global environment
 Project management experience
 Consistently demonstrates clear and concise written and verbal communication
 Comprehensive knowledge of design metrics, analytics tools, benchmarking activities and related reporting to identify best practices
 Demonstrated analytic/diagnostic skills
 Ability to work in a matrix environment and partner with virtual/globally-distributed teams
 Ability to work independently, multi-task, and take ownership of various parts of a project or initiative
 Ability to work under pressure and manage tight deadlines or unexpected changes in requirements
 Proven track record of operational process change and improvement  Education:
 Bachelor’s degree/University degree or equivalent experience
 Master’s degree preferred


 -
 Job Family Group: Technology
  -
 Job Family: Systems & Engineering
  -
 Time Type: Full time
  -
 Primary Location: Irving Texas United States
  -
 Primary Location Salary Range: $116,880.00 - $175,320.00
  -
 Citi is an equal opportunity and affirmative action employer.

 Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

 Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.

 View the ""EEO is the Law"" poster. View the EEO is the Law Supplement.
 View the EEO Policy Statement.
 View the Pay Transparency Posting"
Data Engineer,Greystar Real Estate Partners LLC,"Remote in Irving, TX 75038",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=aa4c76bbb279dc6d&fccid=2acd6fee0f6b700e&vjs=3,"At Greystar, we've launched a program aimed at bringing the real estate leasing experience for residents into the digital era.
 
 As a data engineer, you will join the global Enterprise Data Organization and Apex Organization, which builds a resident-centric ecosystem of products that enable a 360 view of the prospect/resident to improve operational efficiency and resident satisfaction.
 
 You will provide data capabilities and build out a common data model that supports 360 view of our prospect/resident powered by Azure SQL, Synapse data warehouse, and Microsoft Customer Insights. You will also work and support our industry-changing products and features designed to make shopping for an apartment more streamlined, e-commerce friendly, and efficient.
 
 With your help, we will improve our customer’s apartment shopping journey and enable business intelligence to help us personalize the apartment shopping experience for our residents.
 
 The successful candidate will have a strong sense of teamwork, personal integrity, accountability, and the ability to understand business functions and requirements, translating to innovative working applications while navigating competing priority tradeoffs.
 
 JOB DESCRIPTION


 What You Will do

 100% hands-on development – develop and unit test database code, including but not limited to T-SQL, stored procedures, functions and views.
 Create and maintain database structures
 As part of the Scrum team, you will work with BAs, Scrum Master, Leads and engineers to provide data support to our products and build creative solutions and features to move our product roadmap forward.
 Participate in the design of databases, using first, second or third normalized form as needed to support business requirements.
 Create and deploy ADF pipelines, adhering to Greystar’s standards and documented best practices.
 Perform analysis of complex data and document findings.
 Prepare data for prescriptive and predictive modeling.
 Combine raw data from different external sources.
 Collaborate with data scientists and architects.
 Play a direct role in the maintenance, technical support, documentation, and administration of databases.



 Who You Are

 Strong problem solver with excellent communication skills
 Have a growth mindset with a desire to learn and embrace challenges.
 Innovative and passionate about your work
 ""Self-starter"" attitude and the ability to make decisions independently.

 What You Have

 Minimum of 3 years of relevant experience in database design and development
 Minimum of 2 years of relevant experience in working with Azure PaaS databases
 Minimum of 1 year of relevant experience working with Azure Data Factory.
 Minimum of 1 year of relevant experience working with Azure Data Lakes Gen 2.
 Working knowledge of Azure Synapse.
 Preferred: Experience with Customer Insights and/or Dataverse.
 Preferred: Experience with Power BI.
 Bachelor’s in Computer Science, related field, or equivalent work experience

 Technical Pre-screening test will be required for all candidates
 What the Right Candidate will Enjoy!

 100% Remote flexibility!
 Competitive pay, benefits, and overall compensation packages.
 The chance to be part of a technology team for a thriving organization that prioritizes accountability, respect, and operational excellence!
 The opportunity to join a thriving, highly visible organization during its technology transformation!


 The base compensation rate will vary based on education, experience, skills, and geographic location, as applicable. 

Greystar seeks to attract, recruit, advance and retain top talent. Greystar’s compensation strategy is tailored to appropriately reward the skillset and experience that a team member will bring to the organization.

 Depending on the position offered, regular full-time and part-time team members may be eligible to participate in a bonus program in addition to their salary. Team members may also participate in the 401k plan, once eligible. Regular, full-time team members are offered a range of medical, financial, and other benefits from which to choose.

 For Union and Prevailing Wage roles compensation and benefits may vary from the listed information above due to Collective Bargaining Agreements and/or local governing authority.

 Greystar will consider for employment qualified applicants with arrest and conviction records."
Data Engineer - 3,Hearst Media Services,"Dallas, TX 75206 (M Streets area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=946ddb11b8f1d4b8&fccid=02bd2bdda97deebf&vjs=3,"About Us 
Homecare Homebase, a subsidiary of Hearst Corporation is a market leader in healthcare software development providing mobile cloud-based solutions for clinical, operational, and financial improvement of homecare and hospice agencies throughout the United States. Our software enables real time solutions for wireless information exchange and communication between office staff, field staff, and physicians. 
Our success is fueled by our talented technology teams that are driven by their passion to make a difference in patient care. Our employees work in a culture that is guided by values of caring, action, respect, excellence, and smile (a positive attitude). If you want to work in a role where your skills have a direct influence on patient care, Homecare Homebase is the next step in your career. We are hiring technologists that want to make a difference. 

DATA ENGINEER 
As a Data Software Engineer at HCHB, you will be designing and building data and analytics solutions to enable better business outcomes for our customers. You will work across organization stakeholders to understand solution requirements and determine solution design. 
During development of data sets for business analysis and operations, you will collect, aggregate, and organize structured/unstructured data from multiple internal and external sources to enable the presentation of patterns, insights, and trends to decision-makers. 
Your goal is to support the use of data-driven insights by data analysts, visualization designers and reports users both at our customer agencies and internally. These insights help our customers achieve business outcomes and objectives. 
The development environment is SAFe Agile. Our focus is to excel at quality through a focus on automated testing, codifying operational requirements in our development (DevOps) and use of continuous integration. 

ESSENTIAL DUTIES AND RESPONSIBILITIES: 
Responsibilities for the (job title) can vary, but should include: 

Translate business requirements to technical solutions leveraging strong business acumen. 
Analyze current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics PaaS and DaaS. 
Work in a hybrid Azure environment to deliver data solutions. 
Use on-premises expertise in Microsoft SSIS, SQL Server dimensional models and relational models to deliver quality solutions. 
Deliver tight, quality T-SQL code optimized to the operational environment. 
Deliver designs for transformations and modernizations of enterprise data solutions using Azure cloud data technologies. 
Provide expertise and value in migrating to a full Azure PaaS/DaaS solution including Data Lake, Azure SQL Server, other Azure databases, and Azure Synapse. 
Design and Build Modern Data Pipelines and Data Streams 


REQUIRED SKILLS: 

Relevant professional experience with clinical and/or financial data, preferably in the healthcare field. 
Demonstrated experience of turning business use cases and requirements to technical solutions. Experience in business process mapping of data and analytics solutions. 
Hands-on experience in SQL Server relational and dimensional environments. 
Hands-on experience with Azure Data & Analytics PaaS Services: Azure Data Factory, Azure Data Lake, Azure DataBricks, Azure Cosmos DB, Azure SQL DW, and Azure SQL. 
Experience with Git/Azure DevOps. 
Knowledge of Lambda and Kappa architecture patterns. 
Knowledge of Master Data Management (MDM) and Data Quality tools and processes. 
Working experience with Visual Studio, PowerShell Scripting, and ARM templates. 


Preferred Capabilities: 

One or more of the Microsoft Azure Role-Based Certifications (https://www.microsoft.com/en-us/learning/browse-all-certifications.aspx?certificationtype=role-based) 
Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows. 
The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration. 
Demonstrated experience preparing data and building data pipelines for AI Use Cases (text, voice, image, etc.…). 
Designing and building Data Pipelines using streams of data. 
Preferred degree in software engineering, computer operations. 


EXPERIENCE: 

Experience in an Information Technology environment working with relational and dimensional data modeling and data programming experience utilizing SQL Server, Azure SQL Server, Azure Cosmos DB and Azure Synapse 
Experience developing using Microsoft T-SQL, Microsoft SSIS packages or other ETL products, Azure Data Factory and Azure Data Bricks. 
Experience working in a HIPAA/HI-TECH compliant environment preferred. 
Experience working in an Agile DevOps environment, preferring SAFe Agile. 


HCHB requires all applicants to be US citizens or have a green card allowing them to work in the US without being subject to export control restrictions.



Degree Level : Bachelor's Degree"
"Data Engineer II, Store Operations - Dallas, TX",H-E-B,"Dallas, TX 75220 (Northwest Dallas area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=2bd15e1ce281b509&fccid=c629e32155ebd42c&vjs=3,"Overview: 
 H-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, 
 H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. 
 H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.
  Responsibilities: 
 
 About H-E-B


H-E-B is one of the largest, independently owned food retailers in the nation operating over 400 stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.
 


 H-E-B Digital is seeking new team members (Partners)! Since our inception, we’ve been investing heavily in our customers’ digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital, and we’re hiring across the stack: front-end web and mobile, full-stack, and backend engineering. We’re using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. Our digital solutions are growing in popularity and adoption—like Curbside and Home Delivery—so you’ll get the opportunity to define the user experience for millions of customers and hundreds of thousands of Partners. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.
  
 Our Partners thrive The 
  H-E-B Way. In the 
  Data Engineer II position, that means you have a…
 


 HEART FOR PEOPLE… you can organize multiple engineers, negotiate solutions, and provide upward communication
 

 HEAD FOR BUSINESS… you consistently demonstrate and uphold the standards of coding, infrastructure, and process
 

 PASSION FOR RESULTS… you’re capable of high-velocity contributions in multiple technical domains
  

What you’ll do


 Work with HEB Digital teams to provide data solutions for store operations
 Contribute to existing data platforms and implement new technologies
 Develop a deep understanding of HEB’s data and become a domain expert
 Ensure data is distributed in a timely and accurate manner
 Make data discoverable and accessible to business users
 Operate independently with assistance from more experienced developers as needed
 Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications
 Mentor and provide support to junior developers


 Projects you’ll impact


 POS Data Transformation - Architect and develop a streaming point-of-sale (POS) data pipeline to support all downstream systems and analytics. The system will be fault tolerant and involve a cloud/on-premise hybrid approach making the data more accessible.
 New Labor Modeling Focus - Building out a new cloud data management process for labor modeling that will provide the future for data science & analytics of labor rates, scheduling, financial budgeting, and A/B testing. This will include a cloud architected approach to the analytics and supporting data.
 Store Operations Cloud First Integrations - Migrate existing data warehousing pipelines and patterns from existing ETL tools to a cloud approach using a data lake architecture with dynamic processing capacity. Use various new tools and technologies to implement the next generation of data analytical systems at H-E-B. Work with other data engineers on retooling, new standards, and new processes around data management/pipelines in AWS.


 Who You Are?


 2+ years of data engineering experience
 Proficient with data technologies (e.g. Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)
 Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.
 Strong understanding of SQL and data modeling
 Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes
 Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)
 Bachelor's degree in computer science or comparable field or equivalent experience
 A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling


 What are the Perks?


   A robust Benefits plan with coverage starting Day One
   Dental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage
   Partner Care Team, for any time you have healthcare or coverage questions
   Telehealth offers 24/7 access to board-certified doctors by phone
   Partner Guidance allows free counselor visits
   Funeral leave, jury duty, and military pay (subject to applicable law)
   Maternal / paternal leave for new parents, including adoptions
   10% off H-E-B brand products in-store and online
   Eligibility to participate in 401(k)
   Opportunity to become a “Partner-Owner” after 12 months
 


 Who We Are
 H-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billion
   We hire talented people (109,000+ Partners), and give them autonomy to be creative in how they impact the business
   We’re a Partner-driven company with a Bold Promise – Because People Matter
   We embrace Diversity and Inclusion as core values, and support them with thriving company-wide programs
   We’re a truly original Texas-based company that created the Spirit of Giving to help Texas communities every day
   Once eligible, our Partners become Owners in the company. “Partner-owned” means our most important resources—People—drive the innovation, growth, and success that make H-E-B The Greatest Retailing Company
 


 Hiring in Dallas, Austin, & San Antonio locations!



 DATA3232"
Data Science Engineer,Abbott Laboratories,"Plano, TX",Posted 30+ days ago,"$71,300 - $142,700 a year",https://www.indeed.com/rc/clk?jk=07c60fe67d590b13&fccid=77426fa86bb11d7c&vjs=3,"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 115,000 colleagues serve people in more than 160 countries.

 Data science Engineer


 


Make an impact! At Abbott we’re empowering people to manage their health with digital technologies that interface with the nervous system. This opportunity directly supports Abbott’s Neuromodulation business, a fast-growing business, offering the most innovative Spinal Cord and Deep Brain Simulation platforms in the market. Abbott’s Neuromodulation business has a large Product Development site in Plano (Dallas), Texas, where this position is based. Abbott is creating groundbreaking high-quality products designed and built with the betterment of human life as the primary focus. It’s our people who make this all happen. We are thinkers, problem solvers, and innovators working together to constantly challenge the status quo.


 The Data Scientist is responsible for analyzing and interpreting complex raw data, transforming it into valuable insights and algorithms that align with neural technologies. The Data Scientist contributes technical expertise in the innovation, design, enhancement, testing, and implementation of data-driven solutions that integrate with the system. Additionally, they collaborate closely with business stakeholders to identify the key components of successful product designs and substantiate claims related to the design and its underlying mechanisms. By cultivating and disseminating their expertise, the Data Scientist ensures that all data-driven decisions are supported by robust and credible scientific evidence.




 DUTIES:



 Develop scalable machine learning pipelines and employ feature engineering and optimization techniques to enhance data set performance
 Collaborate with Technologists to integrate and process data feeds, creating and testing new scalable data sets
 Utilize various programming languages to transform big data into actionable insights
 Generate extensive databases from diverse sources of structured and unstructured data






 Analyze data and produce statistical information to identify trends and patterns
 Understand product delivery phases and provide expert analysis across product life cycles, contributing to decision-making processes
 Translate technical data into simple language, delivering recommendations and conclusions to stakeholders
 Create interactive dashboards that incorporate real-time data and visuals, experience with Power BI is a plus
 Assess complex technical challenges and define solutions to address business imperatives






 Work with cross-functional teams, including product management, development, quality, regulatory, and operations, to develop innovative strategies and business architectures
 Engage with non-technical stakeholders to comprehend business needs and define technical architecture requirements for next-generation products
 Interface with third-party software companies to integrate, optimize, and stabilize platform system software
 Mentor cross-functional teams on system usage and product integration across regulated and non-regulated software products
 Provide senior management with data on current trends and threats in the Firmware and Software industry






 QUALIFICATIONS



 Bachelor's degree in Computer Science, Electrical Engineering, Computer Engineering, Biomedical Engineering (with neural experience), Data Science, or a related technical field
 4+ years of experience as an engineer or data scientist
 2+ years of experience with data analytics and Machine Learning, including linear and non-linear regression, logistic regression, models, classification techniques, clustering, dimensionality reduction, k-NN, and pipeline development






 Experience in AI techniques and their application in data analysis and processing
 Experience in data visualization and presenting data in graphical or pictorial formats like power bi
 Proficiency in designing algorithms and using statistical and problem-structuring methods
 Ability to clean and validate data for uniformity and accuracy
 Proficiency in various programming languages, such as Java, C, C++, Python, R, and strong coding skills






 Basic understanding of statistical methods for analyzing and drawing conclusions from research data
 Ownership mentality with the ability to independently manage complex initiatives and projects, proactively seeking solutions to arising issues
 Solid verbal, written, and interpersonal communication skills, with the ability to effectively communicate at multiple levels within the organization
 Ability to leverage and/or engage others to achieve project goals
 Capacity to contribute to multiple projects and demands simultaneously






 Ability to work both within a team and independently in a fast-paced, agile environment
 Strong technical leadership and the ability to continuously build, leverage, and influence a large network of senior technology and business experts across a diverse enterprise and external partners: vendors, standards bodies, analysts, and academia



 The base pay for this position is $71,300.00 – $142,700.00. In specific locations, the pay range may vary from the range posted."
"Principal, Data Engineer",MedeAnalytics,"Remote in Dallas, TX",Posted 13 days ago,,https://www.indeed.com/rc/clk?jk=353dea0498d57f34&fccid=e0da9fa1dc69faed&vjs=3,"MedeAnalytics is a leader in healthcare analytics, providing innovative solutions that enable measurable impact for healthcare payers and providers. With the most advanced data orchestration in healthcare, payers and providers count on us to deliver actionable insights that improve financial, operational, and clinical outcomes. To date, we've helped uncover millions of dollars in savings annually.

 As a senior member of the data engineering team, our Principal, Data Engineer, will be the key technical expert developing and overseeing Mede's data product build & operations. This role will build data pipelines into various source systems, rest data on the Mede/Analytics Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. A key component will be driving a strong vision for how data engineering can proactively create a positive impact on the business. This role will also help lead the development of large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of Mede's flagship data products.
 You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems. Experience working on very large scale AWS based Data lake and Data initiatives along with being hands on is required for this role.
 Responsibilities: 

Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the Data Lake/data repository
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
Active contributor to code development in projects and services
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance
Responsible for implementing best practices around systems integration, security, performance and data management
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners
Develop and optimize procedures to ""productionalize"" data science models
Define and manage SLA's for data products and processes running in production
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale
Research in state-of-the-art methodologies
Create documentation for learnings and knowledge transfer
Create and audit reusable packages or libraries

Qualifications

Bachelor's degree preferred; Experience with building solutions in the healthcare space is a plus
Fluent with AWS cloud services; AWS Certification is a plus
6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools and Snowflake required
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.
2+ years in cloud data engineering experience in Oracle OCI/AWS
Experience with integration of multi cloud services with on-premises technologies
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience with MPP database technologies such as AWS Redshift, Vertica or SnowFlake
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes
Experience with version control systems like Github and deployment & CI tools.
Experience with Glue, Data Factory, Databricks and Machine learning tools
Working knowledge of agile development, including DevOps and DataOps concept
Strong change manager. Comfortable with change, especially that which arises through company growth

Benefits Include:

Incredible Medical, Dental, Vision benefits - Effective on the first of the month after your start
FREE single healthcare coverage!!!
Company paid Basic Life & AD&D Insurance, STD/LTD
ROBUST Employee Assistance Program (EAP)
401k with company match
9 paid holidays AND 3 floating holidays = 12 total!
Paid time off accrual
Employee Referral Bonus
Professional Development
and more!

This job description reflects management's assignment of essential functions. Flexibility is a necessary understanding with the natural growth of MedeAnalytics, and deviation/delegation of tasks will be presented as necessary.

 At MedeAnalytics we deeply value each and every one of our committed, inspired and passionate employees. If you're looking to make an impact doing work that matters, you're in the right place. Help us shape the future of healthcare by joining #TeamMede."
Senior Data Engineer,EXL Services,"Remote in Dallas, TX 75247",Posted 30+ days ago,"$120,000 - $165,000 a year",https://www.indeed.com/rc/clk?jk=a871e70206d4b1ea&fccid=e3e300fc88f0e813&vjs=3,"Company Overview and Culture 

EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com. 

 For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses. 


Responsibilities:


Leading and managing High Performing Teams of analysts and engineers
Work with executives for the clients
Have full command of technical competencies of the company to better provide value to the clients.
Make executive designs that impact business
Set and achieve hiring goals and build efficient onboarding for new team members.
Architect high-quality solutions, provide estimates for solutions, and manage the scope and delivery against business milestones
Work as conduit between the clients’ business demands and technical solutions

Requirement:


Knowledge and Experience of Agile Management.
Ability to handle multiple teams across technical and non-technical domains.
Knowledge of Database Systems
RDBMS and NoSQL databases
SQL including creating and running stored procedures as well as interfacing with other applications using ODBC.
Understanding of Data Warehousing, Data Lakes
Understanding of Data Models (e.g. Fact-Dimension), Data Topology (e.g. Star, Hub and Spoke) and ER (Entity Relationship) Diagrams.
Develop procedures and scripts for data migration
Knowledge of best practices of Database Design and their implementation.
Experience with Big Data Technologies such as AWS Redshift, RDS, S3, Glue, Athena, EMR, Spark and PySpark, Snowflake, Hive etc.
Experience with Redshift is a must
Experience in AWS Model deployment using step function, AWS SageMaker and other cloud services
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc
Design, develop, test, deploy, maintain, and improve data integration pipeline
Develop pipeline objects using Apache Spark / Pyspark / Python or Scala
Experience designing/implementing solutions using one or more databases (i.e. Snowflake, AWS Redshift, Synapse, Big Query, Oracle, SQL Server, Teradata, Netezza, Hadoop, Mongo DB or Cassandra).
Hands-on object-oriented programming experience using Scala, Python, R, or Java.
Professional work experience building real-time data streams using Kafka, Spark, AWS Kinesis
Ability to solve problems and quickly and efficiently.
Knowledge of Big-Data systems and analytics.
Experience in AI Machine Learning, real time analytics and Big Data platforms is a plus
Experience with model development and maintenance, and code debug
Experience with client management
 Qualification 


Graduate/Postgraduate Degree in Computer Science/Mathematics/Economics and allied fields
Diploma/Degree in Business Management preferred
7+ years of total experience.
 EEO/Minorities/Females/Vets/Disabilities 


Base Salary Range Disclaimer: The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL's total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits. 

 Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy. 

 Application & Interview Impersonation Warning – Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s)."
Principal Data Science Engineer,Verizon,"Remote in Irving, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=01b3ebf69a9a834c&fccid=f7029f63fe5c906e&vjs=3,"When you join Verizon
 Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

 What you’ll be doing...

 In this role you’ll apply cutting-edge machine learning techniques and AI technology and will lead data science projects to reduce fraud loss and prevent future fraud loss across all Verizon wireless/wireline channels for both consumer and business line of businesses. You will work closely with a team of talented data scientists, Big Data engineers, and software developers and play a key role in developing and delivering the next generation AI/ML solutions to the fraud IT/business.

 You will:

 Create data models to predict customer behavior and risky events/transactions and use them to improve performance and predictability.
 Develop resolutions to complex problems, using your sharp judgment to develop methods, techniques, and evaluation criteria, enabling you to deliver solutions that make a big impact.
 Engage with key business stakeholders in discussions on business strategies and opportunities.
 Build strong working relationships and develop deep partnership with the other IT teams and business.
 Lead small-to-medium portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.
 Lead the design and development of machine learning/statistical models and ensure best performance.
 Work closely with engineers to deploy models in production both in real time and in batch process, and systematically track model performance.
 Be a subject matter expert on machine learning and predictive modeling and a mentor to junior data scientists.
 Drive technical innovation through active research and applications of new theories, techniques, and technologies.


 What we’re looking for...

 With an eye towards improving performance and predictability, you like the science of analytics. You’re able to communicate technical information to non-technical audiences, and you take pride in your ability to share your considerable knowledge with your peers.

 What you'll need to have...

 Bachelor’s degree and 6 or more years of work experience.
 Hands on classification model development using Python or equivalent programming languages
 General SQL proficiency one or many (Postgres, Oracle, SQL Server, Teradata).
 Five or more years of experience in practicing machine learning and data science in business.
 Experience with Relational Databases.
 Experience in leading medium-scale data science projects and delivering end-to-end.
 Strong communication and interpersonal influencing skills.


 Even Better if you have one or more of the following:

 Master’s degree in a quantitative field or relevant field.
 Strong foundational quantitative knowledge and skills; extensive training in math, statistics, physical science, engineering, or other related fields.
 Development experience with PL/SQL and SQL*Plus.
 Experience with UNIX shell scripting using Python.
 Experience interacting with the AWS ecosystem
 Technical experience in machine learning and statistical modeling.
 Experience in data management and data analysis in relational database.
 Excellent problem solving and critical thinking capabilities.
 Strong experience in SQL and database management.
 Exposure to the scikit-learn, pandas and numpy ecosystems
 Experience with other data analysis software and scripting languages (e.g., R, SAS).
 Strong background in statistics.
 Fraud or risk management/modeling experience.
 Ability to bring in new technologies and streamline the existing and new applications.
 Ability to take immediate ownership of unanticipated fraud scenarios and quickly develop ad-hoc solutions when necessary.


 If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

 This role may be considered as part of the Department of Defense SkillBridge Program.








 Where you’ll be working






 In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
 Scheduled Weekly Hours 40
 
 Equal Employment Opportunity
 We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more."
"Engineer, Data - Clinical Analytics",Concentra,"Addison, TX 75001",Posted 6 days ago,Full-time,https://www.indeed.com/rc/clk?jk=4336fc9041c92e0b&fccid=f5a72b33f94db075&vjs=3,"Overview: 
 
   The Data Engineer- Clinical Analytics is primarily focused on analytical processes with ability to implement database solutions and best practices in the realm of data science and machine learning projects. Essential software engineering skills with strong foundational knowledge on data movement and orchestration both on-premises and cloud environment. The Data Engineer supports and aligns with business decisions within Concentra by analyzing raw data, constructing, and maintaining data systems, and improving data quality and efficiency. Implements programming languages to develop and test architectures that enables data operations for predictive (i.e., machine learning/AI) or prescriptive modeling.
  Responsibilities: 
 
Analyze, develop, combine raw information, and maintain various data sources
 Improve data quality and efficiency to build data systems and pipelines
 Identify opportunities for data acquisition and collaborate with Application owners and Subject Matter Experts (SME) to document data domain knowledge
 Implement ETL methods to prepare both structured and unstructured data for predictive and prescriptive modeling
 Leverage data serialization techniques to meet project needs for use in various reporting platforms
 Collaborate with Business Intelligence (BI) ETL Developers/Data Architect, Data Scientists, Reporting Analysts, and Subject Matter Experts (SME) to understand business goals
 Understand enterprise project life cycle and prepare for integration and user acceptance testing methods.
 Produce technical documentation by following enterprise standards and guidelines
 Participate in relevant information-sharing activities
 Serve as escalation point for application support and troubleshooting
 Proactive identification of issues and opportunities that will have an impact on the business use of reports and ensure managerial awareness
 Daily review outstanding issues to assure that troubleshooting and resolutions are current
 Ensure all changes comply with change management policies and procedures
 This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
 Qualifications: 
  Education Level: Bachelor’s Degree
 Major: Computer Science or Computer Engineering
 Degree must be from an accredited college or university

 Job-Related Experience
 Customarily has at least the following experience:

 Customarily has at least three or more years in Software development / Data-Centric pipelines / Model-Centric pipelines
 Relational Database experience
 Documentation and publication


 Job-Related Skills/Competencies

 Concentra Core Competencies of Service Mentality, Attention to Detail, Sense of Urgency, Initiative and Flexibility
 Ability to make decisions or solve problems by using logic to identify key facts, explore alternatives, and propose quality solutions
 Outstanding customer service skills as well as the ability to deal with people in a manner which shows tact and professionalism
 The ability to properly handle sensitive and confidential information (including HIPAA and PHI) in accordance with federal and state laws and company policies
 Strong SQL development and performance tuning skills
 Competencies with Oracle, SQLServer, SSIS, Sybase, NoSQL, Python, Azure, Docker, Git, and Visual Studio
 Experience with Data Lakes, Lake Houses, and ELT is preferred
 Experience with Azure ML, Azure Data Factory, Azure Data Bricks, Azure Data Flow, and Azure Functions is preferred
 Concentra Core Competencies of Service Mentality, Attention to Detail, Sense of Urgency, Initiative and Flexibility
 Ability to make decisions or solve problems by using logic to identify key facts, explore alternatives, and propose quality solutions
 Outstanding customer service skills as well as the ability to deal with people in a manner which shows tact and professionalism
 The ability to properly handle sensitive and confidential information (including HIPAA and PHI) in accordance with federal and state laws and company policies
 Highly organized
 Communication skills to be able to effectively speak and write in a clear and professional manner
 Skilled at listening and providing feedback
 Additional Data: 
 

 Employee Benefits


 401(k) Retirement Plan with Employer Match
 Medical, Vision, Prescription, Telehealth, & Dental Plans
 Life & Disability Insurance
 Paid Time Off
 Colleague Referral Bonus Program
 Tuition Reimbursement
 Commuter Benefits
 Dependent Care Spending Account
 Employee Discounts





     We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation, if required.
   


 *This job requires access to confidential and sensitive information, requiring ongoing discretion and secure information management*
   


 Concentra is an Equal Opportunity Employer, including disability/veterans"
Sr Data Engineer with Vertex.AI,Gannett,"Remote in Plano, TX 75093",Posted 19 days ago,"$140,000 - $160,000 a year",https://www.indeed.com/rc/clk?jk=c00bc988b784e5ba&fccid=1230acb7e56c6df5&vjs=3,"Gannett Co., Inc. (NYSE: GCI) is a subscription-led and digitally-focused media and marketing solutions company committed to empowering communities to thrive. With an unmatched reach at the national and local level, Gannett touches the lives of millions with our Pulitzer Prize-winning content, consumer experiences and benefits, and advertiser products and services. 

 Our current portfolio of media assets includes The USA TODAY NETWORK, which includes USA TODAY, and local media organizations in 43 states in the United States, and Newsquest, a wholly-owned subsidiary operating in the United Kingdom. We also own digital marketing services companies under the brand LocaliQ, which provide a cloud-based platform of products to enable small and medium-sized businesses to accomplish their marketing goals. In addition, our portfolio includes one of the largest media-owned events businesses in the U.S., USA TODAY NETWORK Ventures. 

 Gannett open roles are featured on various external job boards. When applying to a position at Gannett, you should be completing an application on Gannett Careers via Dayforce. Job postings directing you to complete an application on other external sites may not be valid. 

 To connect with us, visit www.gannett.com 


Job Specification: Sr Data Engineer with Vertex.AI 

Location: Remote 

Salary: $140,000-$160,000 based on skills, experience, location, and union representation, if applicable. 


Position Overview:
 We are seeking a skilled and experienced Data Engineer to join our team at Localiq DMS. The ideal candidate should have a strong background in data engineering, with specific exposure in working with the Vertex.AI platform. As a Data Engineer, you will play a crucial role in developing and maintaining our data infrastructure, ensuring the efficient extraction, transformation, and loading of data from various sources. Your expertise with Vertex.AI will be instrumental in leveraging the platform's capabilities to enhance our data processing and analysis workflows. 


Responsibilities:


Develop and maintain data pipelines and ETL processes using Vertex.AI platform to support data acquisition, transformation, and loading.
Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to design and implement efficient data processing solutions.
Design and optimize data models and schemas to support business requirements and data analysis needs.
Implement data quality checks and ensure data integrity throughout the data pipeline.
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner.
Work closely with stakeholders to understand data requirements and translate them into technical specifications.
Research and evaluate new technologies and tools related to data engineering and provid recommendations for improvements.
Stay up to date with industry best practices and emerging trends in data engineering and data management.

Requirements:


Bachelor's degree in Computer Science, Engineering, or a related field. Equivalent work experience will also be considered.
Proven experience as a Data Engineer, with a focus on designing and implementing data pipelines and ETL processes.
Strong expertise in working with the Vertex.AI platform, including familiarity with its capabilities and features.
Proficiency in programming languages such as Python, Java, or Golang for data processing and manipulation.
Experience with distributed computing frameworks like Apache Spark for large-scale data processing.
Knowledge of SQL and database systems (e.g., PostgreSQL, MySQL) for data querying and manipulation.
Familiarity with cloud platforms such as AWS, GCP, or Azure and their respective data services.
Understanding of data warehousing concepts and experience with data modeling and schema design.
Excellent problem-solving and troubleshooting skills, with the ability to identify and resolve data-related issues.
Strong communication skills and the ability to collaborate effectively with cross-functional teams.

Preferred Qualifications:


Advanced degree in a relevant field, such as Data Science or Computer Engineering.
Experience with other data engineering platforms and tools, in addition to Vertex.AI.
Knowledge of machine learning concepts and experience with integrating ML models into data pipelines.
Familiarity with agile development methodologies and version control systems (e.g., Git).
Experience working in an advertising or marketing data environment will be a plus.
 #LI-REMOTE 
#LI-SG1 
#PRODUCTGNT 

 The annualized base salary for this role will range between $90,000 and $213,900. Variable compensation is not reflected in these figures and based on the role, may be applicable. Exact compensation may vary based on skills, experience, location, and union representation, if applicable. 

 Gannett Co., Inc. is a proud equal opportunity employer committed to building and maintaining a diverse workforce. As such, we will consider all qualified applicants for employment and do not discriminate in connection with employment decisions on the basis of an applicant or employee’s race, color, national origin, ethnicity, ancestry, citizenship status, sex, gender, gender identity, gender expression, religion, age, marital status, personal appearance (including height and weight), sexual orientation, family responsibilities, physical or mental disability, medical condition, pregnancy status (including childbirth, breastfeeding or related medical conditions), education, genetic characteristics or information, political affiliation, military or veteran status or other classifications protected by applicable federal, state and local laws in the jurisdictions where Gannett employs employees. In addition, Gannett Co., Inc. will provide applicants who require a reasonable accommodation, as a result of an applicant’s disability or religion, to complete this employment application and/or any other process in connection with an individuals’ application for employment with Gannett Co., Inc. Applicants who require such accommodation should contact Gannett Co., Inc.’s Recruitment Department at Recruit@gannett.com."
Azure Data Integration Engineer,AT&T,"Dallas, TX",Posted 30+ days ago,"$128,400 - $231,100 a year",https://www.indeed.com/rc/clk?jk=cc99cdd9b7b5ad6c&fccid=25b5166547bbf543&vjs=3,"About the Company
 Join AT&T and reimagine the communications and technologies that connect the world. Our Consumer Technology eXperience team delivers innovative and reliable technology solutions to power differentiated, simplified customer experiences. Bring your bold ideas and fearless risk-taking to redefine connectivity and transform how the world shares stories and experiences that matter. When you step into a career with AT&T, you won’t just imagine the future-you’ll create it.


 About the Team
 The Data Analytics and Engineering team provides the critical data points that communicate key insights into the SAFe | Agile operations of CTX teams to increase velocity, improve quality and drive continuous improvement. Provide technical solutions for processes and tools as the organization transforms and grows.


 About the Job
 The Data Analytics and Engineering team is seeking an energetic and experienced individual for a position in data analytics. This role is responsible for collecting, managing, and convert raw data into information that can be used by data analyst and business analysts. This role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.


 Responsibilities and Day-to-Day View

 Assembling large, complex sets of data that meet non-functional and functional business requirements


 Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes


 Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using Azure and SQL technologies


 Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition


 Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues


 Working with stakeholders including the Executive, Product, and Delivery teams to support their data infrastructure needs while assisting with data-related technical issues



 Qualifications

 Bachelor's degree in technical or business discipline or equivalent experience, technical degree preferred


 Minimum of Three (3) years of experience with Azure Cloud


 Minimum of Three (3) years of experience leveraging PowerBI.


 Experience in custom ETL design, implementation, and maintenance


 Experience with schema design and dimensional data modeling


 Experience with machine learning and automation


 Experience with Python, SCALA, SPARK, .NET


 Experience with SQL programming


 Knowledge of IT concepts, strategies, methodologies


 Understanding of agile data engineering concepts and processes


 Strong analytical and problem-solving skills with a demonstrable passion to be a change champion


 Must be team oriented with strong collaboration, prioritization, and adaptability skills required



 Our Senior-System Engineers earn between $128,400 - $231,100. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training.


 Joining our team comes with amazing perks and benefits:

 Medical/Dental/Vision coverage


 401(k) plan


 Tuition reimbursement program


 Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)


 Paid Parental Leave


 Paid Caregiver Leave


 Additional sick leave beyond what state and local law require may be available but is unprotected


 Adoption Reimbursement


 Disability Benefits (short term and long term)


 Life and Accidental Death Insurance


 Supplemental benefit programs: critical illness/accident hospital indemnity/group legal


 Employee Assistance Programs (EAP)


 Extensive employee wellness programs


 Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone



 AT&T is leading the way to the future – for customers, businesses, and industry. We're developing new technologies to make it easier for our customers to stay connected to their world. Together, we’ve built a premier integrated communications and entertainment company and an amazing place to work and grow. Team up with industry innovators every time you walk into work, creating the world you always imagined. Ready to #transformdigital with us? Apply now!"
Azure Data engineer - Streaming,Cognizant Technology Solutions,"Remote in Dallas, TX 75215",Posted 30+ days ago,"$130,000 - $150,000 a year",https://www.indeed.com/rc/clk?jk=b21d9ccfecd71f36&fccid=2df6a1e69a70a1e7&vjs=3,"We are Cognizant Artificial Intelligence
 We at AIA help our clients Rethink, Reimagine and Reinvent their business processes Building a technology platform that is the foundation for innovation, faster time-to-market, and a thriving ecosystem.
 Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. We help clients build new business models from analyzing customers and business operations at every angle to really understand them. With the power to apply AI and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks .
 Job title - Azure Data engineer - Streaming
 Location - remote !!

 Job summary Pull data from first and third party data sources stitch and wrangle it for advanced analytics activities. Leverage data best practices and tools by collaborating with analytics translator data scientist and cloud engineer.
 Roles & Responsibilities

Bachelor's or master’s degree in Computer Science, Data Science, or a related field.
10 + years of experience in data streaming, data engineering, and database design.
Focus on building real time use cases leveraging Databricks and Spark Structured streaming – Must have
Build API’s – Must Have
Subject matter expert on architecting streaming solutions. Has great understanding of different stream processing solutions including Kafka Streams, Flink, Samza, Dataflow, Kineses – Must have
Understand the internals of stream processing . Databricks preferred – Must have
Design, develop, and implement data streaming solutions using Kafka Streams, Flink, Samza, Dataflow, Kineses.
Collaborate with data architects, data scientists, and business analysts to identify data streaming requirements and design data architectures that align with business objectives.
Develop and maintain data pipelines, ETL processes, and data streaming frameworks.
Perform data analysis, data modeling, and database design to ensure optimal performance and scalability of data streaming systems.
Ensure data quality and integrity by implementing data governance and data management processes.
Solid understanding of data structures, algorithms, and data modeling.

#LI-PT1 #CB #Ind123
 Salary and Other Compensation:
 The annual salary for this position is between $130,000.00 - $150,000.00 depending on experience and other qualifications of the successful candidate.
 This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.


 Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:

Medical/Dental/Vision/Life Insurance 
Paid holidays plus Paid Time Off 
401(k) plan and contributions 
Long-term/Short-term Disability 

Employee Status : Full Time Employee
 Shift : Day Job
 Travel : No
 Job Posting : Jun 28 2023


 About Cognizant
 Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
 Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.

 Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
 If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information."
Data Integration Engineers II,Texas Health Resources,"Arlington, TX 76011 (North area)",Posted 16 days ago,Full-time,https://www.indeed.com/rc/clk?jk=c908d67230d4ad2d&fccid=47c7346c8483e993&vjs=3,"Texas Health Resources seeks Data Integration Engineers II for Arlington, TX. Send resume to HR, 612 E. Lamar Blvd., Ste. 400, Arlington, TX 76011 or apply online at www.texashealth.org/careers. EOE.
 
Data Integration Engineer II: design, develop, implement, and support data integration solutions that support resiliency, stability, and supportability of information technology systems; perform rapid development and refinement of data integration solutions using Infosphere Datastage or equivalent ETL software, Relational Databases, SQL Scripting and other database utilities, Unix Scripting and related technologies; work with business sponsors and application development teams including business requirements review and interpretation, availability, quality, and lineage of source system data analysis, data mapping from source to target by applying data warehousing methodologies and related data integration solutions development and implementation, and socialization of data integration standards; support Data Warehouse applications and utilize tools to manage source system changes, optimize applications and ensure availability, accuracy, security, and performance; work with business to ensure standards and best practices, and data governance requirements compliance; and serve as subject matter expert with regard to data integration technical questions and capabilities.
 Education Requirements: Bachelor’s degree in Engineering, Computer Engineering, Computer Science or a closely related field, or equivalent to a Bachelor’s degree in Engineering, Computer Engineering, Computer Science or a closely related field based on a combination of education, training, and/or experience as determined by a professional evaluation service.


Work Experience Requirements: Three (3) years of professional work experience in computer systems, including InfoSphere Information Server products DataStage or equivalent ETL software, Data Warehousing including Relational Databases, SQL Scripting and other Database utilities, Unix Scripting, source and target models translation into technical specifications, and technical specifications analysis and solutions development by applying warehousing methodologies."
Data Engineer,Genpact,"Irving, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=e668ff77a468e4f1&fccid=afe30394bfba1470&vjs=3,"Data Engineer –

 day one onsite location is Irving, TX 75039. 



Candidate must have strong in SQL
Should have Python scripting work expr
Should have Pyspark scripting work expr
Should have ETL background - Not specific to Informatica, Any ETL skills set will work
Cloud – Experience (GCP/Azure if any)
Domain – healthcare would be added advantage
Must have minimum 7-8 Year's exp


 GCP Services and expertise-

Cloud Dataproc
BigQuery
Composer/Airflow
Google cloud storage (GCS)"
Data Engineer III,Iron Systems,"Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=51ca8086a71eab57&fccid=39e7cc3fd9e2cd7d&vjs=3,"Date Posted:


    5/3/2023 
   




Job Function:


    Software Development 
   




Location:


    Dallas TX TX - USA 
   




Offered Salary:


    Competitive
   













    Iron Systems is an innovative, customer-focused provider of custom-built computing infrastructure platforms such as network servers, storage, OEM/ODM appliances & embedded systems. For more than 15 years, customer have trusted us for our innovative problem solving combined with holistic design, engineering, manufacturing, logistic and global support services.
    

Work Location: US - TX - Central Metro - Remote

Job Title: Data Engineer III


Job Description: 

The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization.
 The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.

Job Responsibilities:

Design, construct, install, test and maintain highly scalable data management systems.
 Ensure systems meet business requirements and industry practices.
 Design, implement, automate and maintain large scale enterprise data ETL processes.
 Build high-performance algorithms, prototypes, predictive models and proof of concepts.

Skills:

Ability to work as part of a team, as well as work independently or with minimal direction.
 Excellent written, presentation, and verbal communication skills.
 Collaborate with data architects, modelers and IT team members on project goals.
 Strong PC skills including knowledge of Microsoft SharePoint.

Education/Experience:

Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
 Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI."
Security Data Engineer,Koch Global Services,"Dallas, TX 75201",Posted 27 days ago,,https://www.indeed.com/rc/clk?jk=d6c894de5b715a40&fccid=4cefc6a8866933ab&vjs=3,"Your Job
 The Koch Cyber Security Team is seeking a Security Data Engineer to join our global cyber security team. As a member of this team your primary duties will be to attend to the health and support of our SIEM tool and work closely with our analyst to ensure that data is available to them when needed.
 Our Team
 The Koch Cyber Security team is a dynamic and proactive force, fueled by an unwavering commitment to Koch's vision for value creation. With a relentless drive, we tackle cyber threats head-on, always ready to protect our stakeholders from any potential harm. Our team members are trailblazers, spearheading transformational efforts in areas such as Incident Response, Automation, exposure management, awareness, and the ever-evolving cyber landscape. We thrive on challenges and constantly seek innovative solutions to safeguard our organization and its interests.
 What You Will Do

 Oversee the ingestion and normalization of new data sources. 
Maintain data availability
 Detect and remediate any drop in data ingestion
 Support, maintain and improve infrastructure for the data collection tools overall health
 Respond to customer inquiries surrounding the collection of data
 Work closely with Cyber Security team to ensure that analyst have access to the data they need and that it's presented in a clear and concise manner
 Stay up to date with the latest trends and technologies in data engineering and cloud infrastructure management and applying them to our data collection tool stack
 Actively seek ways to improve our current data collection tool stack.


Who You Are (Basic Qualifications)


 Experience supporting a SIEM tool
 Experience deploying Infrastructure in cloud environments
 Experience building and troubleshooting data pipelines.


What Will Put You Ahead


 Experience with Global Information\Cyber Security Teams
 Experience with Machine Learning 
Experience with Multiple Operating Systems
 Knowledge with PowerShell, JavaScript, or Python Scripting
 Windows Or Linux Administration Experience
 SOAR Platform Experience
 Familiar with CIM date compliance

 At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
  Hiring Philosophy  All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here .
 Who We Are As a Koch company, Koch Global Services (KGS) creates solutions spanning technology, human resources, finance, project management and anything else our businesses need. With locations in India, Mexico, Poland and the United States, our employees have the opportunity to make a global impact.
 At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
 Our Benefits Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
 Equal Opportunities  Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf
 #LI-CB1"
Manager - Data Science - Software Engineer Manager with Data Science experience,"Michaels Stores, Inc.","Irving, TX 75063 (Freeport/Hackberry area)",Posted 2 days ago,Full-time,https://www.indeed.com/rc/clk?jk=cbc85f8c40c292aa&fccid=7bec89cb1b3b9073&vjs=3,"Support Center - Irving 

 We are looking for an experienced Manager - Data Science to join our team at Michaels. The ideal candidate will have a proven track record of leading data-driven insights to help drive business growth and innovation. As a Manager - Data Science, you will work with our team of data scientists and analysts to create predictive models, analyze large datasets, and provide insights and recommendations to key stakeholders. 

 Major Activities 
Lead a team of data scientists responsible for analyzing large data sets to extract insights that inform business decisions at Michaels 
Collaborate with cross-functional teams, including marketing, sales, and product development, to identify key business questions and translate them into data science projects 
Design and implement predictive models that support forecasting, customer segmentation, and product recommendations for Michael's retail business 
Conduct exploratory data analysis to identify trends and patterns that can inform business strategy at Michaels 
Communicate insights and recommendations to key stakeholders, including executives, to influence business decisions at Michaels 
Stay up-to-date on emerging trends in data science and retail to inform the development of new analytical approaches and tools for Michaels retail business 
Manage relationships with external vendors and contractors to ensure project timelines and deliverables are met at Michaels 

 Other duties as assigned 

 Minimum Knowledge/Skills/Abilities 

 Minimum Education 
Bachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field 

 Minimum Type of Experience the Job Requires 
5#43; years of experience in data science or a related field, with at least 2 years in a leadership role in the retail industry. 
Strong experience with statistical analysis, machine learning, and data mining techniques, such as regression analysis, clustering, and decision trees in the retail industry 
Proficiency in data science programming languages such as Python, R, or SQL 
Experience with data visualization tools such as Looker, Tableau or Power BI 
People management experience 
Software Engineering experience 

 Other 
Strong communication and leadership skills, with the ability to collaborate effectively with cross-functional teams 
Excellent analytical and problem-solving skills, with the ability to identify and resolve complex technical issues 
Proven ability to manage projects and teams to deliver high-quality results on time and within budget at Michaels 
#LI-LS1 

 Applicants in the U.S. must satisfy federal, state, and local legal requirements of the job. 

 For nearly 50 years, Michaels has been the destination where Makers get inspired, learn, shop, and create. We strive to cultivate an inclusive shopping environment for all Makers and work environment for all Team Members, providing a place of belonging and empowering everyone to bring their creative dreams to life. At Michaels, every Team Member is encouraged to hone their craft with opportunities for personal and professional growth. From our Stores and Distribution Centers to Artistree and our Support Center, our best-in-class team is passionate about leaving the world a better, more creative place by contributing to every “make”. 

 Michaels is an Equal Opportunity Employer. We are here for all Team Members and all Makers to create, innovate and be better together. 

 Michaels is committed to the full inclusion of all qualified individuals. In keeping with this commitment, Michaels will assure that people with disabilities are provided reasonable accommodations. Accordingly, if a reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the job, and/or to receive all other benefits and privileges of employment, please contact Customer Care at 1-800-642-4235 (1800-MICHAEL). 

 EEOC Know Your Rights Poster in English 
EEOC Know Your Rights Poster in Spanish 
EEOC Poster Optimized for Screen Readers 
Federal FMLA Poster 
Federal EPPAC Poster"
Staff Data Engineer - Data Ventures,Walmart,"Hybrid remote in Dallas, TX",Posted 14 days ago,,https://www.indeed.com/rc/clk?jk=e508ddb0310ad879&fccid=822bc5d9a49270ea&vjs=3,"Position Summary...


What you'll do...

 As a ""Staff Data Engineer"", you should be able to technically help and assist team to steer through correct technical directions following the best practices. You will have deeper understanding of Data Engineering approaches along with hands on experience in building highly scalable solutions.
 

About Team: Data Ventures
 Our team creates reusable technologies to help with customer acquisition, onboarding, and empowering merchants, while ensuring a seamless experience for both stakeholders. We also optimize tariffs and assortment in accordance with Walmart's Everyday Low-Cost philosophy. We not only create affordability, but we also deliver customized experiences for customers across all channels - in-store, mobile app, and websites. Our team is responsible for providing support to US Marketplace sellers. We focus on providing immediate solutions to the cases/tickets created by sellers. We interact with multiple teams across the company to provide excellent seller experience.
 

What you'll do:


 You will lead the work of other small groups of ten to twelve engineers, including offshore associates, for assigned Engineering projects by providing pertinent. documents, direction, and examples; identifying short- and long- term solutions and timeline; reviewing and approving proposed solutions.
 You will drive the execution of multiple business plans and projects by identifying customer and operational needs, developing, and communicating business. 
Leads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements, translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing. code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross. functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery and hand-offs: interacting with project manager to provide input on project plan; and providing leadership to the project team. plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring. progress and adjusting performance; accordingly, developing contingency plans; and demonstrating adaptability and supporting continuous learning.
 Implementing new architectural patterns; and performing design and code reviews of changes.
 Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy.
 Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives, consulting with business. partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost effectiveness. and participating in and supporting community outreach events.


 What you'll bring:


 Knowledge of Databricks, Snowflake is an added advantage.
 Experience with ThoughtSpot, Druid, Big Query and ClickHouse is added advantage. 
Hands on knowledge in NoSQL like Cosmos DB along with RDBMS like MySQL, Postgres is plus.
 Hands on working experience in any messaging platform like Kafka is preferred.
 Increase the efficiency of the team by setting right Processes of Software Development, Requirement Intake, Effort Estimation
 Demonstrating creative, critical thinking & troubleshooting skills.


 About Walmart Global Tech
 Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
 
 Flexible, hybrid work: We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
 

Benefits:
 Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
 

Equal Opportunity Employer:
 Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.
 
 The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
 

Minimum Qualifications... 

 Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
 
 Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field.
  3 years' experience in data engineering, database engineering, business intelligence, or business analytics.
 

Preferred Qualifications... 

 Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
 
 Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 4 years' experience in software engineering or related field
 

Primary Location... 
 1901 N HENDERSON AVE, DALLAS, TX 75206-7319, United States of America"
Senior Engineer - AI/ML/Data Scientist,Fission Labs,"Dallas, TX",PostedJust posted,Full-time,https://www.indeed.com/rc/clk?jk=5c80d8dab82627ea&fccid=e503a562aef6cc4b&vjs=3,"Dallas,Texas
    

      6-8 Years
    



      Full Time
    





Roles and Resposibilities


 Design and develop AI models and algorithms using machine learning techniques and tools, with a focus on NLP.
 Work with large datasets, including cleaning, preprocessing, and transforming data to ensure its quality and relevance.
 Collaborate with cross-functional teams to identify and understand business requirements, use cases, and project objectives.
 Implement and integrate machine learning models into production systems and workflows, and optimize their performance.
 Develop tools and frameworks for automating the data engineering process, data ingestion, and feature engineering.
 Stay up-to-date with the latest research and industry trends in machine learning and NLP, and apply them to improve our models and solutions.
 Document and communicate technical specifications, algorithms, and results to both technical and non-technical stakeholders.

 ‍





 Qualifications Required


 Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, or related fields.
 4 to 10 years of experience in building AI models with NLP and data engineering.
 Strong programming skills in Python, and familiarity with libraries such as PyTorch, TensorFlow, and Scikit-Learn.
 Solid understanding of machine learning concepts, algorithms, and statistical methods. Experience with data preprocessing, cleaning, and feature engineering techniques.
 Strong analytical and problem-solving skills, with attention to detail.
 Excellent communication skills, with the ability to present complex technical concepts to both technical and non-technical audiences.

 ‍





 Skills and Experience Required


 Experience with cloud-based machine learning platforms and services, such as AWS Sage Maker or Google Cloud ML Engine.
 Knowledge of distributed computing, parallel processing, and big data technologies such as Spark and Hadoop.
 Familiarity with natural language processing techniques and tools, such as spaCy and NLTK.
 Experience with containerization and orchestration technologies such as Docker and Kubernetes.
 If you are an experienced AI and ML Engineer with a passion for building innovative solutions and want to join a dynamic and collaborative team, we encourage you to apply for this exciting opportunity.

 ‍





 Why you'll love working with us:


 BRING YOUR PASSION AND FUN. Corporate culture woven from highly diverse perspectives and insights.
 BALANCE WORK AND PERSONAL TIME LIKE A BOSS. Resources and flexibility to more easily integrate your work and your life.
 BECOME A CERTIFIED “SMARTY PANTS.” Ongoing training and development opportunities for even the most insatiable learner.
 START-UP SPIRIT (Good ten plus years, yet we maintain it)
 FLEXIBLE WORKING HOURS"
Principal 5G RAN Data Engineer,AT&T,"Dallas, TX",Posted 30+ days ago,$144 a day,https://www.indeed.com/rc/clk?jk=0e392a4a3aeb6f78&fccid=25b5166547bbf543&vjs=3,"Join AT&T and reimagine the communications and technologies that connect the world. We’re committed to those who seek to discover the undiscoverable and dare to disrupt the norm. Bring your bold ideas and fearless risk-taking to redefine connectivity and transform how the world shares stories and experiences that matter. When you step into a career with AT&T, you won’t just imagine the future – you’ll create it.
 As a Principal Member of Technical Staff , you’ll provide solutions to complex business and technical problems or issues of complex scope using advanced engineering and technical principles.
 Key Responsibilities:

Understanding of network data, data collection, data collection system, network elements and network systems in a large and diverse carrier network including fiber, wireless RAN and wireless core networks
Ability to assess and evaluate the technology that facilitates the NM&A platform to capture, move, store, use, and manage network data in support of network automation, operational support, data insights, and AI use to deliver a geographically distributed and scalable platform.
Plan, development, and socialize a roadmap to support the introduction and rollout of the NM&A in AT&T’s network in a managed fashion to provide real tangible benefits to AT&T as quickly as possible
Providing a comparative assessment, benefits, and recommendation for keeping the data management capabilities either separate from or integrated with the RAN NM&A Platform, both in the short term (1-2 years out) and the long term (3+ years out)
Recommend options and selection criteria to consolidate the many data management solutions currently in use within AT&T’s network, considering the benefits, costs, and organizational difficulty with different levels of integrations, and drive the consolidation effort.
As part of the above, evaluate the benefits and costs of using vendor based solutions versus open source solutions, provide recommendations and architectural support for implementation
Apply principles and practice in broad areas of complex technical project work that requires in-depth analysis using advanced techniques, knowledge, and expertise.
Participate in data management standards, coordinate architecture, & communicate out to internal team, vendors, and standards bodies such as O-RAN and ONAP
Communicate and provide documentation for issues identified to both vendor and internal teams\
Represent the group internally across the company to provide functional and technical leadership and externally to submit vendor feature requests





Required Education:

Bachelor’s degree or higher degree in Engineering, Computer Science, Informational Technology, Data Communications from an accredited university.
PhD in an approved field with 5 years of experience or Masters in an approved field with a minimum of 6 years of relevant experience or a Bachelors in an approved field with a minimum of 8 years of relevant experience.

Required Experience:

Extensive familiarity with network data management capabilities and systems such as Kafka (vendor and open source products), ETL tools, Snowflake, and Databricks.
Expert experience in network data, data collection, data collection system, network elements and network systems.
Expert level oral and written communication with internal stakeholder and external vendors
Proficiency in standard Microsoft Office products is expected, while mastery of advanced techniques and Office features is highly beneficial.


 Our Principal Member Technical Staff earn between $144,00-$289,000. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training.

     Joining our team comes with amazing perks and benefits:
    

Medical/Dental/Vision coverage
401(k) plan
Tuition reimbursement program
Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)
Paid Parental Leave
Paid Caregiver Leave
Additional sick leave beyond what state and local law require may be available but is unprotected
Adoption Reimbursement
Disability Benefits (short term and long term)
Life and Accidental Death Insurance
Supplemental benefit programs: critical illness/accident hospital indemnity/group legal
Employee Assistance Programs (EAP)
Extensive employee wellness programs
Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone
Long Term Grants and Deferred Compensation

 A career with us, a global leader in communications and technology, comes with big rewards. As part of our team, you’ll lead transformation surrounded by trailblazing industry leaders like you. You’ll be empowered to go above and beyond – making a difference through company-sponsored initiatives or connecting and networking through one of our many employee groups. And regardless of where you’re at in your career trajectory, you’ll be rewarded by the impact that comes with making a difference in the lives of millions. With AT&T, you’ll be a part of something greater, do incredible things and be rewarded with a chance to change the world.
   
 Ready to join us? Apply today!"
Senior Data Quality Engineer,Avantax,"Dallas, TX",Posted 30+ days ago,"$110,000 - $157,000 a year",https://www.indeed.com/rc/clk?jk=0a47c09169ab9ec7&fccid=f589ccbeaf041c0b&vjs=3,"We love finance, software, and (believe it or not), taxes – that’s why we love what we do! At Avantax, we celebrate our diverse experience and unique contributions and use our combined knowledge to blaze new trails in tax-focused investing. As individuals, we contribute valuable insight from our career history to achieve the Avantax mission. As a team, we work together to make a real impact and have a lot of fun along the way. Our collaborative work features decades of expertise in wealth management.
 


   We bring our unique experiences together to form the heart of Avantax - working in unison toward the goal of creating services that make it easy for clients and professionals to pursue their financial and business goals. At Avantax, we are committed as a company and individuals to the diversity, equity and inclusion of our clients, team members and the communities in which we work.
 


   Are you an innovative problem-solver who's eager to take on new challenges, collaborate with diverse team members and achieve excellence? If so, life at Avantax may be the opportunity for you.
 


   Position Summary
 


   Avantax is seeking a Senior Data Quality Engineer to power information accuracy across our enterprise data landscape while supporting data integration activities covering shared mastered data for application development as well as our Enterprise Analytics & Reporting platforms. The core responsibilities will include collaborating with data engineers, architects, & stakeholders to understand the scope, requirements, and function of the components of our enterprise data solutions. The Senior Data Quality Engineer will lead the design and implementation of a robust data quality (DQ) methodology to enable high-quality data deliverables and will identify patterns of data quality issues and recommend effective data quality-driven solutions. The Senior Data Quality Engineer is expected to have in-depth knowledge of enterprise-grade data environments including large-scale data warehouse and BI systems, attention to detail, robust technical skills, a strong development & analytical mindset\, and an above-average ability to foresee technical problems when business rules and domain knowledge are applied. The selected incumbent is expected to be an expert in collaborating with business and technical resources to identify appropriate parameters, functions, and data to test, validate, and automate appropriate test cases; parameters may include general function, the validity of results, accuracy, and reliability, for us to be able to deliver high-quality, zero-defect solutions to our stakeholders.
 


   Essential Duties & Responsibilities
 



     Collaborates/ communicates directly with the business, data architecture, and data engineering to understand the scope, requirements, and function of the product and/or services with a mind toward accuracy, scalability, and high performance.
   


     Work with the product owner and data stakeholders throughout the discovery sessions to understand and document data quality rules, visibility requirements, and success criteria used to validate data solutions.
   


     Identify & build test cases to thoroughly test the data solution and implement automated Data Quality verifications throughout the development lifecycle ensuring the final product meets or exceeds the stakeholders’ expectations.
   


     Participate in and represent data quality engineering in the project process communicating qualitative/quantitative findings from test results to the development teams while identifying and recommending solutions, improvements, and updates.
   


     Leverage a DevOps mindset and DataOps principles to monitor efforts, resolve product issues, track progress, and spot areas for improvement to enhance the product's quality and efficiency through a continuous improvement strategy.
   


     Provide technical guidance, thought leadership, and mentoring to junior members of the team.
   



   Education & Experience
 



     Bachelor’s Degree in Computer Science or equivalent experience.
   


     5+ years of hands-on experience with large-scale Data Warehousing testing (preferably with a cloud-based warehouse e.g., Redshift or Snowflake).
   


     5+ years of strong experience with SQL and the ability to write efficient code for high-volume data testing/processing.
   


     3+ years of hands-on experience supporting high-performing ETL processes, including data quality and testing processes.
   


     3+ years of experience working with data testing and automation tools (QuerySurge, IceDQ, etc.).
   


     2+ years of hands-on experience with AWS Big Data technologies (Glue, Kinesis, EMR, AWS, etc.).
   


     2+ years of programming experience with an advanced language (Python preferred)
   


     Data Quality related Certification preferred (ex. Total Data Quality (TDQ), Data Quality Certification (DQC), Certified Data Management Professional (CDMP), etc.)
   


     Extensive knowledge of software and database testing including designing and manipulating test data, validating stored jobs, and error replication.
   


     Experience ensuring high data quality across multiple datasets used for analytical purposes.
   


     Experience in agile development methodologies.
   


     Experience implementing and/ or leveraging DevOps/ DataOps strategy for continuous improvement.
   


     Knowledge of statistical concepts and their application in reporting and data mart applications.
   


     Excellent troubleshooting and problem-solving skills.
   


     Good communication (oral and written) and interpersonal skills.
   


     Enthusiastic attention to detail.
   


     Ability to acquire new skills quickly and thrive in a collaborative team environment.
   



   Physical Demand & Work Effort
 



     Keying/typing, standing, walking.
   


     Sitting for an extended period.
   


     Constant mental and/or visual attention; the work is either repetitive or diversified requiring constant alertness in an office environment.
   


     The job is typically performed under comfortable working conditions; any disagreeable elements are generally absent during the normal performance of the job.
   


     Compliance with company attendance standards.
   



   Comprehensive Benefits
 


   We offer a competitive salary, and an outstanding benefits package that includes medical, dental, vision, life insurance, paid vacation and sick days, paid holidays, tuition reimbursement, and 401(k) with company match.
 


   Base Salary Range:
 


   $110,000 - $157,000; Variable incentive compensation eligible.
 

   The salary range shown is representative of what Avantax expects to pay for this job. Actual salary may vary based on job-related knowledge, skills, experience, and geographic location.
 


   #Avantax
 


   At Avantax®, we believe our work benefits from the diverse perspectives of our employees. As such, Avantax welcomes and celebrates diversity and inclusion and is committed to equal opportunity employment. At Avantax, you can expect a supportive, open, and inclusive atmosphere and a team that values your contributions.
 


   Avantax and its business lines, Avantax Wealth Management® and Avantax Planning Partners℠ are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants without regard to race, color, religion, sex, national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, veteran status, and any other status protected under federal or state law.
 


   Avantax considers information gathered in the hiring process, including information on this application, confidential, and only shares it on a need-to-know basis or as required by law.
 


   If you need assistance or accommodation due to a disability, you may contact us at 
  
   HR@Avantax.com
  , or by calling 972-870-6000 to speak with a member of the HR Talent Acquisition team."
Lead Software Engineer - Spark Data & Analytics,Wells Fargo,"Addison, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=ccb29d1f05e75740&fccid=78bbcd26e39621f5&vjs=3,"The Wells Fargo Consumer Technology (CT) Head of Data has been tasked with transforming and modernizing operational and analytic data across Consumer Technologies. The mission for CT Data is to implement best in class engineered technologies and best practices that are Agile-minded, commercially structured, compliance-oriented, and focused on the rapid delivery of high-quality, safe products and services which are consumable at scale across consumer lines of business. 
 
 The 
 Lead Software Engineer - Spark Data & Analytics role will report to an Engineering Manager. The role will be responsible for providing technical expertise for the data engineering team, solution design for medium to complex data products, hands-on development of reusable frameworks, performing design and code reviews, driving and accelerating Data and Analytics Strategy & Roadmap across Consumer Technology. 
 
 Key responsibilities include: 
 
Implementing Data Strategy for private and public cloud 
Leading and perform current state data assessments, contribute to future state vision/strategy. 
Partnering with Enterprise Data Services organization on Enterprise Data Strategy 
Creating and manage reusable frameworks for data engineering and leverage common design patterns across traditional and bigdata solutions. 
Partnering with Enterprise data teams and Line of Business partners on customer-centric view of the data, data reuse, and data consistency across all customer interactions 
Collaborating with Enterprise Data organization on supplying and governing Consumer Lending data going into the Enterprise Data Lake 
Establishing processes and tools to support consistency, lineage, collection, processing, sharing, and organization of data across entire Consumer Lending portfolio. 
Establishing Consumer Lending wide data standards and governance. Manage data as a strategic asset and operationalize data governance, data quality, and controls across the organization. 
Driving consistency, efficiencies, and cost benefits through Data Integration practices and standards 

Required Qualifications: 

5+ years of Software Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education. 
4+ years of experience in the Data Engineering, Data Management, Spark, Python, and MPP/ETL 
2+ year of experience creating or designing reusable product solutions. 

Desired Qualifications: 

A Bachelor's and/ or Master's Degree in Data Science, Statistics or relevant field 
5+ years' software engineering experience as well as demonstrated experience in Big Data Engineering, Application Support and/or Technology Operations 
2+ years of technical leadership or architecture experience, in a data and technical environment 
2+ years of experience with Agile practices 
2+ years' experience with Data architecture, design, and management 
Cloud certification 
Cloud experience with Azure or GCP 
End-to-End data, analytics and machine learning technology and business acumen including Data Warehousing, ODS, Big Data platforms and related technology stack such as Cloudera, Python, Spark, Hive and cloud native data capabilities. 

Job Expectations: 

This is a hybrid working position in the approved location 
This position is not eligible for visa sponsorship 
@RWF22 
 

We Value Diversity 

 At Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law. 
 
 Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements. 
 
 Candidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. 
 
 Candidates applying to job openings posted in Canada: Applications for employment are encouraged from all qualified candidates, including women, persons with disabilities, aboriginal peoples and visible minorities. Accommodation for applicants with disabilities is available upon request in connection with the recruitment process. 
 

Drug and Alcohol Policy 

 Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more."
Machine Learning / Data Science Engineer,Vistra Corporate Services Company,"Irving, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=e23452f0b2d5d298&fccid=2ab67877a366fc2e&vjs=3,"If you have what it takes to become part of the Vistra family and would like to start a promising career with a global leader, take a look at the exciting employment opportunities that are currently available and apply online.
 


   Job Summary
  Responsible for helping the company discover information hidden in vast amounts of data published by the electric system operators, integrate it with the internal data collected from various sensors into the a decision making system. The primary focus will be applying data mining techniques, feature engineering and building high quality time series prediction tools to assist wholesale trading and retail marketing functions.
 

   Job Description
 

   Key Accountabilities
 

Deliver advanced analytics solutions to business problems with little or no reliance on supervision
Lead a group of external developers / contractors in implementing advanced algorithms/models in the proprietary algorithmic trading platform.
Demonstrate business value from his/her models and ensure that stakeholders are aligned with the expected and actual results.
Summarize multi-dimensional data (i.e. bids & offers, fuel prices and system demand driving factors), find driving factors and forecast them
Ability to generalize solutions across markets i.e. in other ISOs
Collaborates with teams across organization, effectively communicate with stakeholders and implement solutions
Monitor and improve implemented solutions and recalibrate all models
Build ensembles of models for load and price prediction
· Must be able to create effective documentation.
 Understanding of Machine Learning principles and contribute to the development of energy forecasting tools
 Build / improve current load and price prediction models



   Education, Experience, & Skill Requirements
 

Bachelors degree in Mathematics, Computer Science, Operations Research, Statistics or a related discipline
2-5 years of working experience with data mining, analytics or modeling
Experience with AWS architecture and tools as well as Python programming language
Understanding of convolutional and recurrent neural networks is a plus as well as the understanding of simulation based optimization techniques (Q-learning, Stochastic Optimization)
Working knowledge of Tensorflow/PyTorch
The engineer must be eager to learn, assimilate constructive feedback and grow through roles of greater responsibility
Must be an independent thinker, with a strong bias towards action and avoiding analysis-paralysis.



   Key Metrics
 

Models’ accuracy, timely execution of forecasting models
Continue integration of various data sources into existing models
Effective collaboration internally and with outside developers and contractors


   Passionate about solving real business problems via advanced analytics and data science
 


   Job Family
  Corp Dev & Planning
 

   Company
  Vistra Corporate Services Company
 

   Locations
  Irving, Texas
  Texas
 







 We are a company of people committed to: Exceeding Customer Expectations, Great People, Teamwork, Competitive Spirit and Effective Communication. If this describes you, then apply today!










 If you currently work for Vistra or its subsidiaries, please apply via the internal career site.



   It is the policy of the Company to comply with all employment laws and to afford equal employment opportunity to individuals in all aspects of employment, including in selection for job opportunities, without regard to race, color, religion, sex, sexual orientation, gender identity, pregnancy, national origin, age, disability, genetic information, military service, protected veteran status, or any other consideration protected by federal, state or local laws.
 



 If you are an individual with a disability and need assistance submitting an application or would like to request an accommodation, please email us at assistance@vistraenergy.com to make a request."
Senior Data Engineer (Remote First),European Wax Center,"Remote in Plano, TX 75024",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=dbf9a2e6c0eadd57&fccid=ff7e03c5b0c5f71c&vjs=3,"Perks & Benefits 

Remote-First Workplace 
Flexible Fridays 
Diversity, Equity & Inclusion Council 
Monthly Remote Stipend 
Professional Development Stipend (up to $500 annually) 
1 Wellness/Mental Health Paid Day Off 
1 Volunteer Paid Day Off 
Health Benefits (Medical, Dental, Vision) 
HDHP with HSA plan (annual employer contribution to HSA) 
Employer-Paid Basic Life Insurance and AD&D 
Employer-Paid Short- and Long-term Disability 
Employer-Paid Wellness Reward Program 
Employer-Paid Mental Health Benefit 
Employer-Paid Employee Assistance Program 
Employer-Paid Out of State Medical Travel Benefit 
401(k) Safe-Harbor Matching 
Ancillary Benefits (pet insurance, legal coverage, identity theft protection, accident, hospital, and critical illness coverages) 
Paid Time Off (increases with tenure) 
Paid Parental, Adoption, and Foster Leave 
Out of State Medical Travel Benefit 



About The Role 
EWC is looking for a motivated Senior Data Engineer to join our growing team of data experts. In this role, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and engineer who enjoys optimizing data systems and building them from the ground up. The Sr. Data Engineer will support our data analysts/scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. 

A Day In The Life 

Collecting, organizing, managing, and converting raw data into a format that can be easily analyzed by Business Intelligence analysts and data scientists. 
Building and maintaining data pipelines that collect and transport data from various sources to EWC’s data storage systems. 
Using algorithms and programming languages such as SQL and Python to prepare data for analysis. 
Working closely with the management and end-users to understand and address business requirements related to data storage, management, and analysis. 
Creating data analysis tools and developing new data validation methods to ensure data accuracy and completeness. 
Identifying ways to make data more reliable, efficient, and accessible to relevant stakeholders. 
Creating and maintaining the organization’s software and hardware architecture to support efficient and secure data storage and management. 
Conducting research and troubleshooting to address potential problems that may arise in the data storage and management systems. 
Play a key role in designing and crafting a modern Data and Information Delivery and Analytics platform in the cloud to support retail service and product distribution for the US market. 
Create and maintain optimal data pipeline architecture. 
Assemble large, complex data sets that meet functional/non-functional business requirements. 
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. 
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. 
Work with stakeholders including the Executive, Operations, FP&A, and Supply Chain teams to assist with data-related technical issues and support their data infrastructure needs. 
Keep our data separated and secure during transmission and at rest. 
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. 
Work with data and analytics experts to strive for greater functionality in our data systems. 

What Sets You Apart 

Adopts EWC values in personal work behaviors, decision making, contributions and interpersonal interactions. 
Helps shape a positive work environment by demonstrating and influencing others to reward performance and value ""can do"" people, accountability, diversity and inclusion, flexibility, continuous improvement, collaboration, creativity, and fun. 
Experience with commercial data engineering/science solution initiatives. 
Ability to manage a broad range of deliverables with ambiguous task symptomatology while consistently achieving collaborative success with others to accomplish goals. 
Works well in a team environment and takes pride in participating in projects that employ the skills of all team members. 
Ability to learn quickly in a dynamic environment and to troubleshoot issues. 
Business savvy communications skills and concise written communication skills. 
Ability to be self-sufficient and self-driven in a small team. 
Understanding of the current threat and vulnerability landscape. 
Excellent organization and presentation skills. 

Education And Experience 

BS in Computer Science, Data Science, or equivalent. 
7+ years of professional software development or data engineering experience. 
5+ years of experience using and strategizing the use of DBT and Airflow. 
Strong working knowledge of SQL, of datastores and their tradeoffs (including relational, columnar, and document stores), data modeling, data structures, data manipulation. 
Strong knowledge of Extract, Transform, Load (ETL) pipeline design, tooling, and support. 
Experience designing, building and optimizing ‘big data’ data pipelines, architectures and data sets. 
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. 
Strong analytic skills related to working with unstructured datasets. 
Build processes supporting data transformation, data structures, metadata, dependency and workload management. 
Proven ability to architect, implement, and optimize high throughput data pipelines. 
Experience deploying production systems in the cloud (i.e., AWS, Azure). 
Strong communication skills in writing and conversation. 
Experience with tools we use every day: 
            
Storage: Snowflake, AWS Storage Services (e.g., S3, RDS, Glacier) 
ETL/BI: Astronomer, DBT, Domo, Tableau, PowerBI 
Proven passion and talent for teaching fellow engineers and non-engineers. 

Experience with encryption at rest, including multiple approaches and tradeoffs. 
Experience in Retail operations. 




European Wax Center is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, protected veteran status, or any other characteristic protected by law 


This job description is a general description of essential job functions. It is not intended to describe all duties someone in this position may perform. All employees of EWC and operating subsidiaries are expected to perform tasks as assigned by supervisory/management personnel, regardless of job"
Supply Chain Fleet Assoc Data Engineer Part time,PepsiCo,"Plano, TX",Posted 20 days ago,Full-time,https://www.indeed.com/rc/clk?jk=306c570c19169368&fccid=2973259ddc967948&vjs=3,"Overview

   PepsiCo offers the world's largest portfolio of billion-dollar food and beverage brands, including 22 different product lines that each generates more than $1 billion in annual retail sales. Our main businesses - Quaker, Tropicana, Gatorade, Frito-Lay, and Pepsi Cola – also make hundreds of other enjoyable foods and beverages that are respected household names throughout the world. With net revenues of approximately $65 billion, PepsiCo's employees are united by our unique commitment to sustainable growth; we believe that investing in a healthier future for our planet and its people also means a more successful future for PepsiCo. We call this commitment Performance with Purpose: PepsiCo's promise to deliver sustained value by providing a wide range of foods and beverages, from treats to healthy eats; finding innovative ways to minimize our impact on the environment and lower our costs through energy and water conservation as well as reduce our use of packaging material; providing a safe and inclusive workplace for our employees globally; and respecting, supporting and investing in the local communities in which we operate.The PepsiCo Supply Chain Operations group provides a demanding, fast-paced environment in a competitive industry, where growth equals opportunity and fun accompanies the challenge. We are proud to say that PepsiCo has one of the most advanced and sophisticated supply chain environments in all of North America. The areas of focus for this position are designed to build future leaders within our organization and groom candidates for executive leadership roles.
 

   What PepsiCo Supply Chain Fleet Technology does:
 

 Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
 Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
 Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
 Increase awareness about available data and democratize access to it across the company



 Job Description



   As a member of the Fleet Technology Team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. You will help lead the development of very large and complex data applications into internal cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like cost and asset management. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house as well as virtual and cloud data sources and remote systems.
 
Responsibilities

 Active contributor to code development in projects and services.
 Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
 Understand and adapt existing frameworks for data engineering pipelines in the organization.
 Responsible for adopting best practices around systems integration, security, performance, and data management defined within the organization.
 Collaborate with the team and learn to build scalable data pipelines.
 Support data engineering pipelines and quickly respond to failures.
 Collaborate with the team to develop new approaches and build solutions at scale.
 Create documentation for learning and knowledge transfer.
 Learn and adapt automation skills/techniques in day-to-day activities.

Qualifications

 1+ years of development experience in programming languages like Python, PySpark, Scala, etc. Experience or knowledge in Data Modeling, SQL optimization, performance tuning is a plus.
 6+ months of cloud data engineering experience in Azure Certification is a plus.
 Experience with version control systems like GitHub and deployment & CI tools.
 Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
 Experience with data profiling and data quality tools is a plus.
 Experience in working with large data sets and scaling applications like Kubernetes is a plus.
 Experience with Statistical/ML techniques is a plus.
 Experience with building solutions in the retail or in the supply chain space is a plus.
 Understanding metadata management, data lineage, and data glossaries is a plus.
 Familiarity with business intelligence tools (such as PowerBI).



 Education



 BA/BS in Computer Science, Math, Physics, or other technical fields.



 Skills, Abilities, Knowledge



 Excellent communication skills, both verbal and written, and the ability to influence and demonstrate confidence in communications with senior-level management.
 Comfortable with change, especially that which arises through company growth.
 Ability to understand and translate business requirements into data and technical requirements.
 High degree of organization and ability to coordinate effectively with the team.
 Positive and flexible attitude and adjust to different needs in an ever-changing environment.
 Foster a team culture of accountability, communication, and self-management.
 Proactively drive impact and engagement while bringing others along.
 Consistently attain/exceed individual and team goals.
 Ability to learn quickly and adapt to new skills.



 Competencies



 Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
 Understands both the engineering and business side of the Data Products released.
 Places the user in the center of decision making.
 Teams up and collaborates for speed, agility, and innovation.
 Experience with and embraces agile methodologies.
 Strong negotiation and decision-making skill.
 Experience managing and working with globally distributed teams.

EEO Statement

   All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
 

   PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity.
 

   If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
 

   Please view our Pay Transparency Statement"
Data Engineer - 3,Homecare Homebase,"Dallas, TX (Upper Greenville area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=044ed33df86d5aa8&fccid=b3ee0969751e4f33&vjs=3,"About Us 
Homecare Homebase, a subsidiary of Hearst Corporation is a market leader in healthcare software development providing mobile cloud-based solutions for clinical, operational, and financial improvement of homecare and hospice agencies throughout the United States. Our software enables real time solutions for wireless information exchange and communication between office staff, field staff, and physicians. 
Our success is fueled by our talented technology teams that are driven by their passion to make a difference in patient care. Our employees work in a culture that is guided by values of caring, action, respect, excellence, and smile (a positive attitude). If you want to work in a role where your skills have a direct influence on patient care, Homecare Homebase is the next step in your career. We are hiring technologists that want to make a difference. 

DATA ENGINEER 
As a Data Software Engineer at HCHB, you will be designing and building data and analytics solutions to enable better business outcomes for our customers. You will work across organization stakeholders to understand solution requirements and determine solution design. 
During development of data sets for business analysis and operations, you will collect, aggregate, and organize structured/unstructured data from multiple internal and external sources to enable the presentation of patterns, insights, and trends to decision-makers. 
Your goal is to support the use of data-driven insights by data analysts, visualization designers and reports users both at our customer agencies and internally. These insights help our customers achieve business outcomes and objectives. 
The development environment is SAFe Agile. Our focus is to excel at quality through a focus on automated testing, codifying operational requirements in our development (DevOps) and use of continuous integration. 

ESSENTIAL DUTIES AND RESPONSIBILITIES: 
Responsibilities for the (job title) can vary, but should include: 

Translate business requirements to technical solutions leveraging strong business acumen. 
Analyze current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics PaaS and DaaS. 
Work in a hybrid Azure environment to deliver data solutions. 
Use on-premises expertise in Microsoft SSIS, SQL Server dimensional models and relational models to deliver quality solutions. 
Deliver tight, quality T-SQL code optimized to the operational environment. 
Deliver designs for transformations and modernizations of enterprise data solutions using Azure cloud data technologies. 
Provide expertise and value in migrating to a full Azure PaaS/DaaS solution including Data Lake, Azure SQL Server, other Azure databases, and Azure Synapse. 
Design and Build Modern Data Pipelines and Data Streams 


REQUIRED SKILLS: 

Relevant professional experience with clinical and/or financial data, preferably in the healthcare field. 
Demonstrated experience of turning business use cases and requirements to technical solutions. Experience in business process mapping of data and analytics solutions. 
Hands-on experience in SQL Server relational and dimensional environments. 
Hands-on experience with Azure Data & Analytics PaaS Services: Azure Data Factory, Azure Data Lake, Azure DataBricks, Azure Cosmos DB, Azure SQL DW, and Azure SQL. 
Experience with Git/Azure DevOps. 
Knowledge of Lambda and Kappa architecture patterns. 
Knowledge of Master Data Management (MDM) and Data Quality tools and processes. 
Working experience with Visual Studio, PowerShell Scripting, and ARM templates. 


Preferred Capabilities: 

One or more of the Microsoft Azure Role-Based Certifications (https://www.microsoft.com/en-us/learning/browse-all-certifications.aspx?certificationtype=role-based) 
Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows. 
The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration. 
Demonstrated experience preparing data and building data pipelines for AI Use Cases (text, voice, image, etc.…). 
Designing and building Data Pipelines using streams of data. 
Preferred degree in software engineering, computer operations. 


EXPERIENCE: 

Experience in an Information Technology environment working with relational and dimensional data modeling and data programming experience utilizing SQL Server, Azure SQL Server, Azure Cosmos DB and Azure Synapse 
Experience developing using Microsoft T-SQL, Microsoft SSIS packages or other ETL products, Azure Data Factory and Azure Data Bricks. 
Experience working in a HIPAA/HI-TECH compliant environment preferred. 
Experience working in an Agile DevOps environment, preferring SAFe Agile. 


HCHB requires all applicants to be US citizens or have a green card allowing them to work in the US without being subject to export control restrictions."
Senior Lead Data Engineer,JPMorgan Chase & Co,"Plano, TX 75024",Posted 13 days ago,Full-time,https://www.indeed.com/rc/clk?jk=b86675e6b30eabbc&fccid=c46d0116f6e69eae&vjs=3,"JOB DESCRIPTION
 Embrace this pivotal role as an essential member of a high performing team dedicated to reaching new heights in data engineering. Your contributions will be instrumental in shaping the future of one of the world’s largest and most influential companies.
 Job Summary
 As a Senior Lead Data Engineer at JPMorgan Chase Commercial Bank, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics in a secure, stable, and scalable way. Leverage your deep technical expertise and problem solving capabilities to drive significant business impact and tackle a diverse array of challenges that span multiple data pipelines, data architectures, and other data consumers.
 Job Responsibilities 

Provides recommendations and insight on data management, governance procedures, and intricacies applicable to the acquisition, maintenance, validation, and utilization of data
Designs and delivers trusted data collection, storage, access, and analytics data platform solutions in a secure, stable, and scalable way 
Defines database back-up, recovery, and archiving strategy 
Generates advanced data models for one or more teams using firmwide tooling, linear algebra, statistics, and geometrical algorithms 
Approves data analysis tools and processes 
Creates functional and technical documentation supporting best practices
Advises junior engineers and technologists 
Evaluates and reports on access control processes to determine effectiveness of data asset security 
Adds to team culture of diversity, equity, inclusion, and respect

 Required Qualifications, Capabilities, and Skills

Working experience with both relational and NoSQL databases 
Advanced understanding of database back-up, recovery, and archiving strategies 
Advanced knowledge of linear algebra, statistics, and geometrical algorithms
Experience presenting and delivering visual data
Strong background in a Big Data technologies (Spark, Impala, Hive, Redshift, Kafka, etc.)
Expertise in Big Data technologies in the AWS ecosystem ( Lake Formation, Redshift, Lambda, SQS, EMR etc.)

 Preferred Qualifications, Capabilities, and Skills

Proficiency across the full range of database and business intelligence tools; publishing and presenting information in an engaging way is a plus
Financial Services and Commercial banking experience is a plus
Familiarity with NoSQL database platforms(DynamoDB, Cassandra) is a plus

ABOUT US

 JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
 



  We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.
  




  The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the “WELL Health-Safety Rating” for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment. 
 


As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm’s current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm’s vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.

Equal Opportunity Employer/Disability/Veterans 
 

ABOUT THE TEAM

 Commercial Banking is focused on helping our clients succeed and making a positive difference in our communities. We provide credit and financing, treasury and payment services, international banking and real estate services to clients including corporations, municipalities, institutions, real estate investors and owners, and nonprofit organizations."
Senior Principal Data Engineer,Shutterfly,"Plano, TX 75074",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=724f7e8f2dad5268&fccid=a2be2e0191cb5bc4&vjs=3,"Description At Spoonflower (part of the Shutterfly family of brands) our mission is to inspire individuals to make, buy and sell unique products built around color and pattern. Our e-commerce website makes it possible for individuals to create, sell, and shop for unique fabrics, wallpaper, and home décor. Spoonflower's global marketplace connects our customers to more than 1 million designs by independent artists from around the world.
  Spoonflower is seeking an experienced and highly motivated Senior Principal Data Engineer to be a player/coach to our Data and Analytics engineering team. This position is responsible to conceptualize, feature design and development, maintenance, and production support of all data integration platform, tool, and ETL/ELT processes. This position is responsible for leading a team of data engineers to build and maintain the data lake that will support analytical and reporting use cases across the Spoonflower organization. 
What You'll Do Here: 

Work in a dynamic & agile environment to develop creative data-driven solutions using AWS Tech stack, Snowflake, Databricks, SQL, as well as other modern data technologies
 Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues
 Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
 Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, Python, Scala
 Design and implement data engineering, ingestion and curation functions on AWS cloud using AWS native or custom programming.
 Knowledge in NoSQL databases - MongoDB, Dyanamo DB is a plus
 Experience in managing and troubleshooting issues with database like Aurora, Redshift
 Experience in Unix, AWS Cloud platform tools, storage and security (SNS, S3, Athena, QuickSight, Secret manager).
 Demonstrates leadership and active pursuit of optimizing CI/CD process and tools, testing frameworks and practices
 Understanding of Cloud / Hybrid data architecture and eCommerce concepts is a plus
 Manage a small team of onshore and offshore engineers

 The Skills You'll Bring: 

Bachelors or master's degree in computer science, Information Technology or other relevant fields
 7-10 yrs. of hands-on experience in building batch as well as NRT/RT data ingestion pipeline using Kafka, Kinesis/firehose, AWS DMS, Spark-ETL, AWS Glue and Lambda and proficient in optimizing and tuning Redshift database
 3 years of experience managing a team of engineers
 Has experience in AWS Athena and Glue, Pyspark, EMR, DynamoDB, Redshift, Kinesis, Lambda, Snowflake, Aurora, S3, Glue, Athena, DynamoDB
 Knowledge of BI tools - Power BI, Looker is a plus
 Knowledge of AWS security and IAM roles 

Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it's the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.  #SFLYTechnology"
Vice President; Data Engineer II,Bank of America,"Plano, TX 75024",Posted 7 days ago,Full-time,https://www.indeed.com/rc/clk?jk=e4ce386e5d97824b&fccid=5bd99dfa21c8a490&vjs=3,"Job Description:

 At Bank of America, we are guided by a common purpose to help make financial lives better through the power of every connection. Responsible Growth is how we run our company and how we deliver for our clients, teammates, communities and shareholders every day.
 One of the keys to driving Responsible Growth is being a great place to work for our teammates around the world. We’re devoted to being a diverse and inclusive workplace for everyone. We hire individuals with a broad range of backgrounds and experiences and invest heavily in our teammates and their families by offering competitive benefits to support their physical, emotional, and financial well-being.
 Bank of America believes both in the importance of working together and offering flexibility to our employees. We use a multi-faceted approach for flexibility, depending on the various roles in our organization.
 Working at Bank of America will give you a great career with opportunities to learn, grow and make an impact, along with the power to make a difference. Join us!

 RESPONSIBILITIES:

 Develop and deliver data/ reporting solutions to accomplish technology and business goals.
 Code solutions to integrate, clean, transform, and control data in operational and/or analytics data systems per the defined acceptance criteria.
 Assemble large, complex data sets to meet functional reporting requirements.
 Build processes supporting data transformation, data structures, metadata, data quality controls, dependency, and workload management.
 Define and build reporting applications that enable better data-informed decision-making.
 Contribute to existing test suites (integration, regression, performance), analyze test reports, identify any test issues/errors, and triage the underlying cause.
 Document and communicate required information for deployment, maintenance, support, and business functionality.
 Work closely with business partners to help translate functional requirements into technical approach, design, and decisions- Banking & Markets business acumen.
 Create MicroStrategy schema objects, complex attributes / metrics, conditional and level metrics, and their use within a report.
 Develop MicroStrategy dossiers & Tableau dashboards and 3rd party application report integration.
 Use SQL, data warehouse concepts /architecture, dimensional modeling, and ETL solution design.
 Tune and optimize query performance for large datasets-cubes, caching, aggregate structures within MicroStrategy, Tableau and various RDBMS, Hadoop backend systems.


 REQUIREMENTS:

 Bachelor’s degree or equivalent in Computer Science, Computer Information Systems, Management Information Systems, Engineering (any) or related; and
 5 years of progressively responsible experience in the Job offered or a related IT occupation.
 Working closely with business partners to help translate functional requirements into technical approach, design, and decisions- Banking & Markets business acumen;
 Creating MicroStrategy schema objects, complex attributes / metrics, conditional and level metrics, and their use within a report;
 Developing MicroStrategy dossiers & Tableau dashboards and 3rd party application report integration;
 Using SQL, data warehouse concepts /architecture, dimensional modeling, and ETL solution design; and,
 Tuning and optimizing query performance for large datasets-cubes, caching, aggregate structures within MicroStrategy, Tableau and various RDBMS, Hadoop backend systems.


 If interested apply online at www.bankofamerica.com/careersor email your resume to bofajobs@bofa.comand reference the job title of the role and requisition number.

 EMPLOYER: Bank of America N.A.

 Job Band: H5
 
 Shift: 1st shift (United States of America)
 
 Hours Per Week: 40
 
 Weekly Schedule:

 Referral Bonus Amount: 0"
Data Engineer,Virtualware Innovations,"Dallas, TX 75204 (M Streets area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=2231661f3e8645ab&fccid=57077bd28d6e3f33&vjs=3,"Job Description
5+ years of experience in IT
Excellent knowledge in SQL and SSIS packages
Should have worked in GCP for more than 6 month
Good to have knowledge in Hadoop/spark ( any of this)
Should have basic knowledge in ETL
Good to have skill set: DB2 & Informix
Skillset Required – GCP, Spark , PySpark and Python, ETL tools , SQL, SSIS"
Data Engineer (Full Time; Multiple Openings),RingCentral,"Dallas-Fort Worth, TX",Posted 30+ days ago,"$153,000 a year",https://www.indeed.com/rc/clk?jk=3c5a820ca6623b13&fccid=1cbb498b08d4e46a&vjs=3,"JOB DESCRIPTION: Responsible for building and maintaining solutions around the various RingCentral distributed database technologies.
  




 JOB DUTIES: Develop and enhance RingCentral’s distributed databases and data infrastructure; Develop high-quality, high-performance distributed systems in Python, SQL and Java; Build robust, reliable, automated data pipelines using Kafka and Spark streaming; Develop upon and integrate with other services within the RingCentral application and development stacks; Work closely with other teams to understand and mitigate issues and improve performance; Work closely with RingCentral’s operations teams to help develop and optimize solutions; Work with large data volumes, including processing, transforming and transporting large-scale data using big data stacks; Design, build and launch new data models in production; Design, build and launch new data extraction, transformation and loading processes in production.
  




 REQUIRED SKILLS/TOOLS: Experience with Hadoop, HDFS, Hive, HBase, MongoDB, ElasticSearch, Vertica, Amazon Redshift, Oracle, Python, SQL, Java, Kafka, Spark, MapReduce, Hive SQL, Python, C/C++, Unix, and Linux is required.
  




 EDUCATION/EXPERIENCE /
  



    QUALIFICATIONS: U.S. Master’s degree or foreign equivalent in Computer Science, Information Systems or a related field plus two (2) years of related experience, or U.S. Bachelor’s degree or foreign equivalent in Computer Science, Information Systems or a related field plus five (5) years of related experience is required. Within the foregoing parameters, any suitable combination of education, training and experience is acceptable.
  




 *** Documentary verification of education/experience required ***
  




 JOB LOCATION: Dallas, TX
  




 SALARY: $153,000 per year"
GCP IBM Streams Data Engineer || Hybrid role,Cat software service,"Irving, TX",EmployerActive 7 days ago,$55 - $60 an hour,https://www.indeed.com/company/Cat-software-service/jobs/Data-Engineer-c5cf2ad420147f27?fccid=e497ab2d4dc4f4d9&vjs=3,"Job Title: GCP IBM Streams Data Engineer
Location: Irving, TX | Basking Ridge, NJ | Temple Terrace, FL | Alpharetta, GA | Piscataway, NJ | Colorado Springs, CO | Ashburn, VA | Lone Tree, CO
Duration : 12 months Contract
Skillset:
IBM Streams, Apache Flink, Google Cloud Platform, Linux, Apache Airflow, Google Compute Engine, GitLab, BigQuery, Python, Pyspark, Spark & Kafka
Job Description :

Must have 5 + years of experience as GCP Data Engineer.
The Data Engineer will be responsible for developing and supporting database applications to drive automated data collection, storage, visualization and transformation as per business needs.
Understand the existing IBM streams pipelines
Able to write the Custom templates in Apache Data flow (Apache Beam)
Migrate the IBM Stream pipelines to Data Flow
Able to handle Billions of records / day volume in real-time streaming systems and write the pipelines in Flink / Dataflow over GCP
Able to perform the Quality check on the data (Data validation)
Work experience over the CICD like jenkins and construct them over the cloud Database design, Data Modelling and Mining.
Consolidate data across multiple sources and databases to make it easier to locate and access.
Implement automated data collection and data storage systems.
Write complex SQL queries and stored procedures.
Work with multiple data systems and large relational databases.

Best Regards,
Sam wilson| IT Recruiter
CAT Software Services INC.
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:

8 years

Schedule:

No weekends

Experience:

IBM Streaming: 2 years (Required)
Apache: 3 years (Required)
Google Cloud Platform: 1 year (Required)

Work Location: On the road"
Junior Big Data Platform Engineer,G-Research,"Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=afab2f6f4dc5fee6&fccid=4fab22f542766288&vjs=3,"Location : Dallas, TX 

This is a hybrid role, based in G-Research’s office in Dallas.






 G-Research is Europe’s leading quantitative finance research firm. We hire the brightest minds in the world to tackle some of the biggest questions in finance. We pair this expertise with machine learning, big data, and some of the most advanced technology available to predict movements in financial markets.

 Opened in 2022, the Dallas office is a key infrastructure hub where we work on the latest technologies in a cutting-edge environment.

 The role

 Our business focuses on forecasting financial markets and we use an ever-growing amount of data and processing to achieve this.

 The Big Data Platform team provides open source analytics and storage platforms that are key to enabling business-critical functions. It is responsible for cutting-edge clusters, which underpin diverse use-cases such as quantitative research, risk analysis and cyber security.

 As a Junior Big Data Platform Engineer, you will work closely with other teams across the business, such as Quantitative Research and various development teams; it is vital to maintain effective close relationships with these teams in order to understand their use-cases and challenges, and help users get the most out of Big Data Platforms.

 At such a scale, automation is key, and there is a focus on containerisation, container orchestration, configuration management, orchestration, Infrastructure as Code and CI/CD for this role.

 Key responsibilities of the role include:

 Researching and implementing new technologies in line with key business objectives
 Helping to shape and engineer the Big Data Platform, ensuring it is scalable, stable, and performant, as well as easy to use and maintain
 Providing metrics, documentation, and self-service infrastructure to help our users work at pace and get the most out of the platform
 Implementing and maintaining automation
 Using advanced troubleshooting skills to diagnose and fix problems


 Who are we looking for?

 We’re looking for an aspiring Platform Engineer who is enthusiastic about contributing to an automated, scalable, reliable and high performing Big Data Platform.

 The ideal candidate will have:

 A strong desire to continually learn about new technologies, approaches and systems, along with the agility to work across multiple disciplines
 A willingness to learn by working with users across different business areas, experiences and cultures to drive towards the best outcome for G-Research
 Experience with at least one programming language


 The following technology experience is beneficial but not essential:

 Big Data:
       
 Spark
 Trino/Hive
 Cloud technologies (e.g. AWS EMR, Dataproc)

 Automation/operational:
       
 Kubernetes
 Dev Ops principals
 Linux OS core principles
 Prometheus / ELK



 Why should you apply?

 Market-leading compensation plus annual discretionary bonus
 Informal dress code and excellent work/life balance
 Excellent paid time off allowance
 Sick days, military leave, and family and medical leave
 Generous 401(k) plan
 12-weeks’ fully paid parental leave
 Medical and Prescription, Dental, and Vision insurance
 Life and Accidental Death & Dismemberment (AD&D) insurance
 Employee Assistance and Wellness programs
 Generous relocation allowance and support
 Great selection of office snacks, and hot and cold drinks
 On-site gym and car park





G-Research is committed to cultivating and preserving an inclusive work environment. We are an ideas-driven business and we place great value on diversity of experience and opinions.
We want to ensure that applicants receive a recruitment experience that enables them to perform at their best. If you have a disability or special need that requires accommodation please let us know in the relevant section."
Senior Software Engineer (Senior Azure Data Engineer),"Vizient, Inc.","Irving, TX 75062 (Las Colinas Urban Center area)",Posted 19 days ago,"$102,400 - $152,200 a year",https://www.indeed.com/rc/clk?jk=d63742ebd05f18c7&fccid=ab22e7c357e67bd4&vjs=3,"When you’re the best, we’re the best. We instill an environment where employees feel engaged, satisfied and able to contribute their unique skills and talents while living and working as their authentic selves. We provide extensive opportunities for personal and professional development, building both employee competence and organizational capability to fuel exceptional performance through an inclusive environment both now and in the future.
 


   Summary:
 


   In this role, you will use best practices and knowledge of internal and external business operations to improve products and/or services. You will solve complex problems and bring a new perspective to the usage of existing solutions. You will also mentor employees in the immediate group.
 


   Responsibilities:
 

 Design, develop, enhance, code, test, deliver and debug software independently across multiple products.
 Implement larger, more complex, or new stories for multiple products.
 Play an active role in story breakup and refinement sessions.
 Drive and lead story level architecture/design sessions.
 Participate in feature level architecture/design sessions.
 Recommend actions to improve procedures and standards.
 Stay up to date on technical trends and emerging technology.
 Proactively improve standards and procedures.


 Qualifications:
 

 Relevant degree preferred.
 5 or more years of experience in a software development role required.
 Exceptional analytical and conceptual thinking along with logical and physical database design skills required.
 Strong aptitude and experience in writing and troubleshooting SQL and T-SQL required.
 Data analysis, data modeling, and data integration using Azure technologies like Azure Data Factory (ADF) required.
 Experience with Azure SQL and cloud data solutions within a data warehouse environment, using multiple data sources, data lakes, etc. preferred.
 Hands on experience with Azure Synapse, Azure Stream Analytics, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2, & Logic Apps strongly preferred.
 Experience with Python, Spark, Hive, Hbase and other bigdata technologies preferred.
 Extensive experience working with enterprise solution delivery in a large-scale distributed software design environment is preferred.
 Experience working in an Agile based development environment, using Agile concepts such as Continuous Integration, TDD (Test Driven Development), and Paired Programming preferred.
 You must be authorized to work in the United States without sponsorship.



   Estimated Hiring Range:
  $102,400.00 - $152,200.00
 

   This position is also incentive eligible.
 


   Vizient has a comprehensive benefits plan! Please view our benefits here:
 



    http://www.vizientinc.com/about-us/careers
  



   Equal Opportunity Employer: Females/Minorities/Veterans/Individuals with Disabilities
 


   The Company is committed to equal employment opportunity to all employees and applicants without regard to race, religion, color, gender identity, ethnicity, age, national origin, sexual orientation, disability status, veteran status or any other category protected by applicable law."
Senior Data Engineer,Camden Homes,"Dallas, TX 75243 (North Dallas area)",EmployerActive 29 days ago,Full-time,https://www.indeed.com/rc/clk?jk=3ff6fae6e8732b36&fccid=17ad8c79a686f6af&vjs=3,"The Position:
 The purpose of the position is to review, optimize, and maintain our business information systems to progress the development and efficient aggregation of all our data so that inter-department tasks and operations across applications support data integrity. Once data integrity is established, implement a system that will provide analysis from our data. warehouse. In addition to a database administrator, we're looking for someone who is will fill the gap between a database admin and an analyst. Someone who has the education background with technical knowledge to address specific technology related limitations, consider broader business information systems, and communicate with non-technical personnel to help leadership make better choices and approvals to processes, etc. 
Main Responsibilities:

 Data Warehouse Architecture

 Deep understanding of the sources, operations, and applications that generate the data to better architect our data warehouse.
 
Technical Support

 Aid with managing technical support requests from other members of the company and elevating cases to our IT vendor
 
Information Systems Support

 Work with our team on creating processes and learning best practices resulting in more efficient aggregation of data from our various sources to create better reporting
 

Authority:

 Correct inconsistencies concerning data integrity.
 Monitor and correct automations between different applications.
 Submit ideas or suggestions on better integrations and products.
 Submit ideas or suggestions on process improvement.

 Competencies:

 Microsoft Excel experience, experience in writing SQL Queries.
 Having additional knowledge in creating dashboards in Microsoft Power BI or Tableau is a plus.
 Experience in writing validation rules, creating flows, Process builders and Workflows within Salesforce.
 Experience in Salesforce architecture, reporting, and dashboards is a plus.
 Experience in managing IP phone system.
 Experience in writing API’s for Salesforce, experience in developing RPA is a plus.
 Minimum 7 years experience working as a data engineer.
 Masters Degree in Computer Science or related-field .

 What we provide:

 Competitive compensation
 Health Insurance, Vision, Dental, Life Insurance
 Short-Term Disability, Long-Term Disability
 PTO
 Gym
 Nice work environment



 After finishing up the Application Form, please proceed to complete the Candidate Survey to continue on with the hiring process. You must complete the Candidate Survey in order to move on to the next hiring step.


 Who We Are:
 Camden Homes is a vertically integrated privately-owned company that is in the business of providing housing solutions to the workforce of America. For more than 20 years, we have been achieving the goal of changing people's lives one house at a time. In order to help fulfill the American dream of becoming a homeowner, we build quality homes and sell them at an affordable price. We believe our people play a major role in our success, and to continue this our company follows and believes in the 6 Core Values. Teamwork makes the dream work, Go all-in, Always do the right thing, Sweat the small stuff, Create happy energy, Deliver ""wow"".
 We are proud to be an equal opportunity employer. Camden Homes highly respects and welcomes diversity and believes it to enhance the community we live and work in. Our applicants will not be considered for the positions based on their race, ethnicity, national origin, sex, sexual orientation, gender identity, age, disability, religion, or any other characteristic that is protected by the law."
Principal Analytics Data Engineer,CarOffer,"Addison, TX 75001",EmployerActive 2 days ago,"Up to $130,000 a year",https://www.indeed.com/company/CarOffer/jobs/Principal-Software-Engineer-c01159936de915e9?fccid=bc531b7769dde0ea&vjs=3,"We are an innovative and customer-centric company that revolutionizes car selling and buying. With a transparent and trustworthy approach, we simplify the process, providing fair valuations and quick offers for sellers, and an extensive, thoroughly inspected inventory for buyers. Join us in driving towards a sustainable automotive future, where the car trading experience becomes seamless and enjoyable for all.
Role overview
As a Principal Analytics Data Engineer at our organization, you will be at the forefront of revolutionizing the way we leverage data to drive insights and decision-making. Your primary focus will be designing, developing, and maintaining a robust and scalable data analytics infrastructure, enabling efficient data storage, retrieval, and analysis. Collaborating closely with data analysts, scientists, and cross-functional teams, you will craft interactive visualizations and dashboards to empower stakeholders with actionable insights. With a keen eye for data governance and performance optimization, you will ensure that our analytics ecosystem operates transparently, delivering a seamless and rewarding experience for all users. Your expertise and leadership will be instrumental in shaping our data-driven future and guiding our organization towards data excellence.
What you'll do

Data Analytics Infrastructure: Lead the design, development, and maintenance of a cutting-edge data analytics infrastructure. You will build robust data models, warehouses, and pipelines to enable efficient data storage, retrieval, and processing, ensuring the seamless flow of data throughout the organization.
Actionable Insights: Collaborate closely with data analysts, scientists, and business stakeholders to understand data requirements and translate them into actionable insights. You will develop interactive visualizations and dashboards that empower teams to make informed decisions and drive business growth.
Data Governance and Performance Optimization: Ensure adherence to data governance principles, data security, and privacy standards while continuously optimizing data pipelines and analytics queries for top-notch performance. Your focus on efficiency will enhance the speed and accuracy of data-driven processes.
Innovation and Best Practices: Stay at the forefront of data engineering and analytics technologies, evaluating and implementing emerging tools and best practices. Your expertise will shape our data strategy, maximizing the value derived from our data assets.
Leadership and Mentorship: Act as a subject matter expert and mentor for data engineering teams, providing guidance, support, and technical leadership. Your collaborative approach will foster a culture of innovation and continuous improvement within the organization.
Cross-Functional Collaboration: Work closely with cross-functional teams, including product managers, developers, and data scientists, to align data engineering efforts with business objectives. Your ability to communicate complex technical concepts to non-technical stakeholders will be invaluable in driving successful projects.
Scalable Solutions: Architect data solutions that can scale to accommodate growing data volumes and evolving business needs, ensuring our analytics ecosystem remains agile and adaptable.

What you'll bring

The successful candidate will be required to perform their duties onsite in the CarOffer's Addison, Tx office.
Bachelor's degree in Computer Science, Data Science, Statistics, or a related field required. Master's preferred or equivalent practical experience and relevant certifications will also be considered.
4+ years of experience as a software engineer, including 3+ years as a Data Engineer
Professional experience developing and executing on a data warehouse and/or data platform strategy using a modern data stack
Confident and experienced looking at data platform architecture as a whole and helping to drive strategic roadmap decisions
Ability to move quickly in a dynamic and cross-functional team environment
Expertise in SQL, Python, and system architecture – with the ability to understand the context of a business need and build a system that addresses it

Job Type: Full-time
Pay: Up to $130,000.00 per year
Benefits:

401(k) matching
Health savings account
Life insurance
Paid time off
Referral program
Retirement plan
Vision insurance

Compensation package:

Bonus pay

Experience level:

5 years

Schedule:

Monday to Friday

Ability to commute/relocate:

Addison, TX 75001: Reliably commute or planning to relocate before starting work (Required)

Experience:

SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)

Work Location: In person"
Support Engineer-Informatica Data Quality,Persistent Systems,"Dallas, TX",PostedToday,Rotating shift,https://www.indeed.com/rc/clk?jk=c2ee6ed859c2325a&fccid=b90433ed96d63887&vjs=3,"Job Description




""Production migration/deployment of ETL objects & associated files through CICD pipelines- Provide L1 (Monitoring), L1.5 (Basic Dev & incident management) support, L2 (Complex Incident Management), L3 (Complex Dev) support from the Persistent Offshore Delivery Center(ODC) set up.- Continuous Platform Optimizations to improve Resiliency, Up-time, and resolution of Vulnerabilities within SLA- Enforce platform governance & standards- Platform Support for Informatica EDC Major, Minor & Ad hoc releases (EBFs)- Development and refinement of dashboards for monitoring capacity planning and platform resiliency- Execute annual Disaster Recovery & Password rotation exercises- Assist with Audit requests- Platform Health Reporting, Weekly and Monthly Executive Status Reporting - Implement Continuous Improvement & Automation opportunities- Platform Upgrade Support- To manage and support the platforms 24x7 on rotational shift basis. - To learn new platform management technologies and tools as a part of the cross-skilling exercise in Project life cycle. - Customer / Stakeholder management"""
Data Engineer,Evaya Data Systems,"Allen, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=762dcef94f0ba06f&fccid=d3963513239ea6de&vjs=3,"The engineering team consists of talented, team-oriented individuals who are empowered to take advantage of the latest cloud and distributed technologies to deliver reliable, high-throughput applications.  As a Data Engineer, you’ll employ your skills on a daily basis to design and build data processing and storage applications to handle millions of transactions per day. You will analyze business requirements and consult with the broader team to ensure successful processing, storage and reporting of our Big Data. You’ll have a wide variety of languages and technologies at your disposal that you can use to solve problems. Your work will directly shape and create our data architecture to ultimately deliver systems that stand up to unpredictable environments at massive scale.
 Technical Skills Needed:

5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.
Growing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:
Strong Educational Background
Hands-on Engineering experience in

Problem solving and debugging skills
Writing and deploying code the Linux, Windows, or cloud environments
Familiarity with algorithms and performance analysis
Willingness to contribute to the operational responsibility of the team’s applications

Some experience with one or more of the following:

Relational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)
Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase"
SiteOps Data Center Production Operations Engineer,Meta,"Garland, TX",Posted 28 days ago,"$72,488 - $122,990 a year",https://www.indeed.com/rc/clk?jk=a01a9a5792332e28&fccid=ba07516c418dda52&vjs=3,"Facebook is seeking a forward thinking experienced Engineer to join the Production Operations team within Data Center Operations. Our data centers, and the tens of thousands of servers installed in them, are the foundation upon which our rapidly scaling infrastructure efficiently operates and upon which our innovative services are delivered. Facebook is at the leading edge of the global data center industry both in terms of how data centers are designed and operated. This person should enjoy working in a fast paced environment where adaptability and flexibility will be key to their success. We seek an IT professional with advanced hands-on technical skills in Networks, Server Hardware and Linux (ideally in a Data Center environment). Having extensive knowledge of managing servers and performing complex projects in a large-scale distributed data center environment is a core competency of this individual. The candidate should also have deep knowledge and experience in at least one of the following core areas: Networking, Project Management, Tool and Automation, Hardware and OS repair.
 


SiteOps Data Center Production Operations Engineer Responsibilities:  

Perform deep dives and analyze complex technical issues within the data center, ranging from automated tooling to hardware failures and network issues.
 Work as a technical lead with cross functional teams on large scale data center projects and initiatives.
 Provide cross data center support and identify potentially larger issues, displaying effective communication when something is identified.
 Work with internal hardware teams and vendors to help resolve complex technical issues, maintain high hardware quality levels and influence future design to ensure ease of serviceability.
 Understand/analyze issues and be able to update and develop scripts and smaller sets of software.
 Use data to drive maximum server fleet up-time and utilization rates, by understanding hardware failure rates and SLAs to customers. Identify trends and systemic issues in the fleet and drive resolution.
 Mentor team members to evaluate and identify better ways to resolve issues and define updates to tools and processes.
 Provide guidance and mentor technical leads and the go-to technical resource for management.
 Build cross functional relationships and have the ability to influence policies and procedures to improve global data center operations.
 Participate in an on-call rotation.




Minimum Qualifications: 

 Knowledge of Linux and hardware systems support in an Internet operations environment.
 Knowledge of the interdependencies of data center functions and technologies.
 Knowledge of out-of-band/lights-out server communication methods, such as IPMI and serial console.
 Experience managing multiple projects within the same time schedule.
 Knowledge of enterprise level networking and storage equipment installations.
 BS, BA or BEng in technical field or commensurate experience.
 Proven communication skills.
 Time and project management experience.
 5+ years of infrastructure or related experience.
 Experience in modifying and developing in commonly used scripting or programming languages.




Preferred Qualifications: 

 Experience in a large-scale data center environment.
 Experience in providing technical guidance to external vendors.







  Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. We may use your information to maintain the safety and security of Meta, its employees, and others as required or permitted by law. You may view Meta's Pay Transparency Policy, Equal Employment Opportunity is the Law notice, and Notice to Applicants for Employment and Employees by clicking on their corresponding links. Additionally, Meta participates in the E-Verify program in certain locations, as required by law"
Connected Data Compliance Engineer,PACCAR,"Lewisville, TX 75067",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=b4683c40cfbe93a8&fccid=c2c6a7536e4d9df3&vjs=3,"Company Information


 PACCAR is a Fortune 500 company established in 1905. PACCAR Inc is recognized as a global leader in the commercial vehicle, financial, and customer service fields with internationally recognized brands such as Kenworth, Peterbilt, and DAF trucks. PACCAR is a global technology leader in the design, manufacture and customer support of premium light-, medium- and heavy-duty trucks under the Kenworth, Peterbilt and DAF nameplates and also provides customized financial services, information technology and truck parts related to its principal business.

 Whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services, you can develop the career you desire with PACCAR. Get started!




 Requisition Summary


 The PACCAR Powertrain Organization is seeking a talented engineer to join the newly formed Connected Compliance team to help with new upcoming On-Board Diagnostics (OBD) data reporting to the California Air Resources Board (CARB). This position will utilize data analytics to analyze complex datasets to help validate regulatory compliance of the product and continuously monitor performance to identify issues & improvement opportunities. This position is located in beautiful Lewisville, TX.




 Job Functions / Responsibilities



 Perform OBD data analysis for regulatory compliance reporting of heavy-duty PACCAR Engines operating in North America.
 Conduct statistical data analysis on both, internal test development and production vehicle fleets to inform and report on trends/patterns observed.
 Support testing, validating, and reporting for Production Vehicle Evaluation, Heavy-Duty Inspection & Maintenance, and other programs as required.
 Collaborate with outside suppliers to perform data verification/validation for different vehicle fleets.
 Assimilate, aggregate and query data from remote diagnostics to develop status dashboards.
 Present information using data visualization techniques for easy interpretation of complex datasets.
 Prepare reports, as appropriate, incorporating results, conclusions, and recommendations.
 Establish and maintain cooperative and productive work relations with other departments and divisions.
 Participate in compliance data reviews with other team leads.
 Support other regulatory projects with different teams in North America and Europe as needed.
 Support responding to regulatory agency questions on product compliance.
 Some travel (up to 10%) required





 Qualifications



 Preferred 2 to 5 years of relevant work experience in analyzing and presenting complex datasets from multiple sources.
 Working knowledge of Tableau and SQL preferred.
 Proficiency in programming languages such as MATLAB, VBA, R, or Python is a plus.
 Familiarity with OBD, emissions regulations, and certification.
 Knowledge of vehicle CAN network technologies and associated communication protocols (UDS, J1979, J1939, KWP2000, etc.).
 Strong verbal and written communication skills.
 Ability to thrive in a team environment.
 Ability to manage personal workload and have the willingness, flexibility, and initiative to respond to shifting time & project demands 





 Education


 BS degree in electrical, mechanical, or similar technical field required; Advanced degree preferred.




 PACCAR Benefits


 As a U.S. PACCAR employee, you have a full range of benefit options including:

 401k with up to a 5% company match
 Fully funded pension plan that provides monthly benefits after retirement
 Comprehensive paid time off – Minimum of 10 paid vacation days (additional days are provided with additional seniority/years of service), 12 paid holidays, and sick time
 Tuition reimbursement for continued education
 Medical, dental, and vision plans for you and your family
 Flexible spending accounts (FSA) and health savings account (HSA)
 Paid short-and long-term disability program
 Life and accidental death and dismemberment insurance
 EAP services including wellness plans, estate planning, financial counseling and more





 Additional Job Board Information



 PACCAR is an Equal Opportunity Employer/Protected Veteran/Disability and E-Verify Employer.
 Hiring Location: Lewisville, TX"
Data Science/ Machine Learning Engineer,ICS Global Soft,"Irving, TX 75038 (Cottonwood area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=1b3a3771dab71412&fccid=0e79b4b653bf4742&vjs=3,"Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you.


Responsibilities:


Study and transform data science prototypes
Design machine learning systems
Research and implement appropriate ML algorithms and tools
Develop machine learning applications according to requirements
Select appropriate datasets and data representation methods
Run machine learning tests and experiments
Perform statistical analysis and fine-tuning using test results


Requirements:


Proven experience as a Machine Learning Engineer or similar role
Understanding of data structures, data modeling and software architecture
Deep knowledge of math, probability, statistics and algorithms
Ability to write robust code in Python, Java and R
Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)
Excellent communication skills
Ability to work in a team

Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038."
Data Engineer - Need on w2 only,Vidu Solutions LLC,"Dallas, TX",EmployerActive 2 days ago,"$66,357 - $146,846 a year",https://www.indeed.com/company/Vidu-Solutions-LLC/jobs/Data-Engineer-040fb07f4898d185?fccid=324290a0b0d0068d&vjs=3,"Need 3 Data Engineers
internal positions. Min 2 to 3 years of experience is fine.
Must be W2 only.
Good to be onsite. Immediate interview.
Job Type: Contract
Salary: $66,356.86 - $146,845.57 per year
Schedule:

8 hour shift

Work Location: In person"
Sr. Data Engineer,"Evergreen Residential Holdings, LLC","Hybrid remote in Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=2a3fe808051fe586&fccid=dd616958bd9ddc12&vjs=3,"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
 We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.

 This is not a consulting position. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time for this position.
 This position will be (Full-time/Permanent employment) based in our downtown Dallas offices, and we will consider a hybrid work schedule.
 As a technical/engineering expert, you also pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs. You will design and support data warehouse systems, perform data extraction and ensure data accuracy, enabling real-time insights from both internal and market data that will drive revenue growth and capital efficiency. This position plays a critical role in working with our analytics and reporting specialists to help Evergreen Residential make the best investment decisions.
 The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.


 The role includes, but is not limited to, the following responsibilities:

Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository
Developing and maintaining data processing and storage infrastructure
Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes
Collaborating with data analysts to ensure that data is readily available for analysis and modeling
Optimizing database performance and troubleshooting issues as they arise
Implementing data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Staying up-to-date with emerging trends and technologies in data engineering
Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance
Ensure scientific method and research are key drivers of the product roadmap

 What You Will Bring to the Table:

Bachelor's Degree in a relevant field required
Min 5 years of experience in data engineering or a related field
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases
Experience in real estate investment and/or rental sector highly desirable
Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry
Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function
Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level
A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk
Collaborative team player, you are truly a ""do-er"", happy to be a hands-on problem-solver to move the data program forward
Excellent communication skills – verbal and written


 About Evergreen Residential
 Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
 The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
 Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
 Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.


 Equal Opportunities and Other Employment Statements
 We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law."
Supply Chain Fleet Assoc Data Engineer Part time,PepsiCo,"Plano, TX 75024",Posted 20 days ago,Part-time,https://www.indeed.com/rc/clk?jk=e27100fd5808d258&fccid=2973259ddc967948&vjs=3,"Overview: 
 
   PepsiCo offers the world's largest portfolio of billion-dollar food and beverage brands, including 22 different product lines that each generates more than $1 billion in annual retail sales. Our main businesses - Quaker, Tropicana, Gatorade, Frito-Lay, and Pepsi Cola – also make hundreds of other enjoyable foods and beverages that are respected household names throughout the world. With net revenues of approximately $65 billion, PepsiCo's employees are united by our unique commitment to sustainable growth; we believe that investing in a healthier future for our planet and its people also means a more successful future for PepsiCo. We call this commitment Performance with Purpose: PepsiCo's promise to deliver sustained value by providing a wide range of foods and beverages, from treats to healthy eats; finding innovative ways to minimize our impact on the environment and lower our costs through energy and water conservation as well as reduce our use of packaging material; providing a safe and inclusive workplace for our employees globally; and respecting, supporting and investing in the local communities in which we operate.
  
 The PepsiCo Supply Chain Operations group provides a demanding, fast-paced environment in a competitive industry, where growth equals opportunity and fun accompanies the challenge. We are proud to say that PepsiCo has one of the most advanced and sophisticated supply chain environments in all of North America. The areas of focus for this position are designed to build future leaders within our organization and groom candidates for executive leadership roles.
 

 What PepsiCo Supply Chain Fleet Technology does:
 

 Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
 Responsible for day-to-day data collection, transportation, maintenance/curation and access to the PepsiCo corporate data asset
 Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
 Increase awareness about available data and democratize access to it across the company


 Job Description


   As a member of the Fleet Technology Team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. You will help lead the development of very large and complex data applications into internal cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like cost and asset management. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house as well as virtual and cloud data sources and remote systems.
  Responsibilities: 
 
Active contributor to code development in projects and services.
 Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
 Understand and adapt existing frameworks for data engineering pipelines in the organization.
 Responsible for adopting best practices around systems integration, security, performance, and data management defined within the organization.
 Collaborate with the team and learn to build scalable data pipelines.
 Support data engineering pipelines and quickly respond to failures.
 Collaborate with the team to develop new approaches and build solutions at scale.
 Create documentation for learning and knowledge transfer.
 Learn and adapt automation skills/techniques in day-to-day activities.
 Qualifications: 
 
1+ years of development experience in programming languages like Python, PySpark, Scala, etc. Experience or knowledge in Data Modeling, SQL optimization, performance tuning is a plus.
 6+ months of cloud data engineering experience in Azure Certification is a plus.
 Experience with version control systems like GitHub and deployment & CI tools.
 Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
 Experience with data profiling and data quality tools is a plus.
 Experience in working with large data sets and scaling applications like Kubernetes is a plus.
 Experience with Statistical/ML techniques is a plus.
 Experience with building solutions in the retail or in the supply chain space is a plus.
 Understanding metadata management, data lineage, and data glossaries is a plus.
 Familiarity with business intelligence tools (such as PowerBI).


 Education


 BA/BS in Computer Science, Math, Physics, or other technical fields.


 Skills, Abilities, Knowledge


 Excellent communication skills, both verbal and written, and the ability to influence and demonstrate confidence in communications with senior-level management.
 Comfortable with change, especially that which arises through company growth.
 Ability to understand and translate business requirements into data and technical requirements.
 High degree of organization and ability to coordinate effectively with the team.
 Positive and flexible attitude and adjust to different needs in an ever-changing environment.
 Foster a team culture of accountability, communication, and self-management.
 Proactively drive impact and engagement while bringing others along.
 Consistently attain/exceed individual and team goals.
 Ability to learn quickly and adapt to new skills.


 Competencies


 Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
 Understands both the engineering and business side of the Data Products released.
 Places the user in the center of decision making.
 Teams up and collaborates for speed, agility, and innovation.
 Experience with and embraces agile methodologies.
 Strong negotiation and decision-making skill.
 Experience managing and working with globally distributed teams.
 >: 
 
   All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
 


 PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity.
 


 If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
 


 Please view our Pay Transparency Statement"
Data Engineer,Greystar Real Estate Partners LLC,"Remote in Irving, TX 75038",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=aa4c76bbb279dc6d&fccid=2acd6fee0f6b700e&vjs=3,"At Greystar, we've launched a program aimed at bringing the real estate leasing experience for residents into the digital era.
 
 As a data engineer, you will join the global Enterprise Data Organization and Apex Organization, which builds a resident-centric ecosystem of products that enable a 360 view of the prospect/resident to improve operational efficiency and resident satisfaction.
 
 You will provide data capabilities and build out a common data model that supports 360 view of our prospect/resident powered by Azure SQL, Synapse data warehouse, and Microsoft Customer Insights. You will also work and support our industry-changing products and features designed to make shopping for an apartment more streamlined, e-commerce friendly, and efficient.
 
 With your help, we will improve our customer’s apartment shopping journey and enable business intelligence to help us personalize the apartment shopping experience for our residents.
 
 The successful candidate will have a strong sense of teamwork, personal integrity, accountability, and the ability to understand business functions and requirements, translating to innovative working applications while navigating competing priority tradeoffs.
 
 JOB DESCRIPTION


 What You Will do

 100% hands-on development – develop and unit test database code, including but not limited to T-SQL, stored procedures, functions and views.
 Create and maintain database structures
 As part of the Scrum team, you will work with BAs, Scrum Master, Leads and engineers to provide data support to our products and build creative solutions and features to move our product roadmap forward.
 Participate in the design of databases, using first, second or third normalized form as needed to support business requirements.
 Create and deploy ADF pipelines, adhering to Greystar’s standards and documented best practices.
 Perform analysis of complex data and document findings.
 Prepare data for prescriptive and predictive modeling.
 Combine raw data from different external sources.
 Collaborate with data scientists and architects.
 Play a direct role in the maintenance, technical support, documentation, and administration of databases.



 Who You Are

 Strong problem solver with excellent communication skills
 Have a growth mindset with a desire to learn and embrace challenges.
 Innovative and passionate about your work
 ""Self-starter"" attitude and the ability to make decisions independently.

 What You Have

 Minimum of 3 years of relevant experience in database design and development
 Minimum of 2 years of relevant experience in working with Azure PaaS databases
 Minimum of 1 year of relevant experience working with Azure Data Factory.
 Minimum of 1 year of relevant experience working with Azure Data Lakes Gen 2.
 Working knowledge of Azure Synapse.
 Preferred: Experience with Customer Insights and/or Dataverse.
 Preferred: Experience with Power BI.
 Bachelor’s in Computer Science, related field, or equivalent work experience

 Technical Pre-screening test will be required for all candidates
 What the Right Candidate will Enjoy!

 100% Remote flexibility!
 Competitive pay, benefits, and overall compensation packages.
 The chance to be part of a technology team for a thriving organization that prioritizes accountability, respect, and operational excellence!
 The opportunity to join a thriving, highly visible organization during its technology transformation!


 The base compensation rate will vary based on education, experience, skills, and geographic location, as applicable. 

Greystar seeks to attract, recruit, advance and retain top talent. Greystar’s compensation strategy is tailored to appropriately reward the skillset and experience that a team member will bring to the organization.

 Depending on the position offered, regular full-time and part-time team members may be eligible to participate in a bonus program in addition to their salary. Team members may also participate in the 401k plan, once eligible. Regular, full-time team members are offered a range of medical, financial, and other benefits from which to choose.

 For Union and Prevailing Wage roles compensation and benefits may vary from the listed information above due to Collective Bargaining Agreements and/or local governing authority.

 Greystar will consider for employment qualified applicants with arrest and conviction records."
Data Engineer,RumbleOn,"Irving, TX 75038",EmployerActive 2 days ago,"$115,000 - $125,000 a year",https://www.indeed.com/rc/clk?jk=55c451d0f50bfab6&fccid=bd9b67a2bfe2ef7e&vjs=3,"As a Senior Data Engineer, you will be responsible for architecting, building, and maintaining scalable data infrastructure to support our data-driven initiatives. You will collaborate closely with cross-functional teams, including analysts, stakeholders, and software engineers, to design and implement data solutions that meet business requirements. Your expertise will be essential in ensuring data integrity, performance, and security while optimizing data processes for efficiency and scalability.
 Responsibilities:

Design, develop, and maintain data infrastructure, including data pipelines, ETL processes, and data warehouses, to support data-intensive applications, analytics, and business intelligence.
Collaborate with stakeholders to gather data requirements and translate them into scalable data solutions.
Identify, evaluate, and implement appropriate tools and technologies for data ingestion, storage, processing, and analysis.
Ensure the reliability, availability, and performance of data platforms through monitoring, troubleshooting, and optimization.
Develop and enforce data governance policies, including data quality standards, privacy, and security protocols.
Lead and mentor junior data engineers, providing technical guidance and fostering a culture of continuous learning and improvement.
Collaborate with cross-functional teams to understand data needs and design efficient data models, schemas, and data transformation processes.
Stay up-to-date with industry trends, emerging technologies, and best practices in data engineering and recommend enhancements to existing systems and processes.
Participate in code reviews, perform unit testing, and maintain documentation to ensure code quality, maintainability, and knowledge sharing.
Proactively identify and resolve performance bottlenecks, data issues, and technical challenges related to data processing and storage.

Requirements

Bachelor's degree in Computer Science, Engineering, or a related field. A Master's degree is a plus.
Proven experience (5+ years) as a Data Engineer or similar role, with a strong focus on building scalable data infrastructure.
Strong proficiency in data modeling, data warehousing concepts, and SQL.
Expertise in designing and implementing data pipelines, ETL processes, and workflow orchestration using tools such as Apache Airflow, AWS Glue or similar.
Proficiency in programming languages such as Python, Java, or Scala for data processing and scripting.
Solid understanding of distributed systems, cloud platforms (e.g., AWS, Azure, GCP), and big data technologies (e.g., Hadoop, Spark, Kafka).
Experience with data integration and streaming technologies, such as AWS Kinesis Data Streams/Firehose, Apache Nifi, or similar.
Strong knowledge of database technologies (e.g., SQL databases, NoSQL databases, columnar databases).
Familiarity with containerization (e.g., Docker, Kubernetes) and infrastructure-as-code tools (e.g., Terraform) is a plus.
Excellent problem-solving skills, attention to detail, and the ability to work in a fast-paced, dynamic environment.

Benefits
 What RumbleOn Offers You:
 A fun, relaxed, and casual work environment with awesome people by your side working as a team to ensure the entire group's success! Plus...

Healthcare, Dental, & Vision Insurance (RumbleOn pays a generous portion of medical premium!)
PTO Plan & Public Holidays Off
Close knit, open, inviting environment where you can make your mark and where your ideas are heard!
Employee discounts on purchases
Extremely competitive compensation packages commensurate with experience and skillset
Fully stocked kitchen with drinks and snacks all day
Fun company events
The opportunity for growth and a solid long term career...we promote from within!!
Casual Dress code
Training and full support while you learn
And more…

*All applicants must pass pre-employment testing to include: background checks, MVR, and drug testing in order to qualify for employment*·"
Data Engineer - 3,Hearst Media Services,"Dallas, TX 75206 (M Streets area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=946ddb11b8f1d4b8&fccid=02bd2bdda97deebf&vjs=3,"About Us 
Homecare Homebase, a subsidiary of Hearst Corporation is a market leader in healthcare software development providing mobile cloud-based solutions for clinical, operational, and financial improvement of homecare and hospice agencies throughout the United States. Our software enables real time solutions for wireless information exchange and communication between office staff, field staff, and physicians. 
Our success is fueled by our talented technology teams that are driven by their passion to make a difference in patient care. Our employees work in a culture that is guided by values of caring, action, respect, excellence, and smile (a positive attitude). If you want to work in a role where your skills have a direct influence on patient care, Homecare Homebase is the next step in your career. We are hiring technologists that want to make a difference. 

DATA ENGINEER 
As a Data Software Engineer at HCHB, you will be designing and building data and analytics solutions to enable better business outcomes for our customers. You will work across organization stakeholders to understand solution requirements and determine solution design. 
During development of data sets for business analysis and operations, you will collect, aggregate, and organize structured/unstructured data from multiple internal and external sources to enable the presentation of patterns, insights, and trends to decision-makers. 
Your goal is to support the use of data-driven insights by data analysts, visualization designers and reports users both at our customer agencies and internally. These insights help our customers achieve business outcomes and objectives. 
The development environment is SAFe Agile. Our focus is to excel at quality through a focus on automated testing, codifying operational requirements in our development (DevOps) and use of continuous integration. 

ESSENTIAL DUTIES AND RESPONSIBILITIES: 
Responsibilities for the (job title) can vary, but should include: 

Translate business requirements to technical solutions leveraging strong business acumen. 
Analyze current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics PaaS and DaaS. 
Work in a hybrid Azure environment to deliver data solutions. 
Use on-premises expertise in Microsoft SSIS, SQL Server dimensional models and relational models to deliver quality solutions. 
Deliver tight, quality T-SQL code optimized to the operational environment. 
Deliver designs for transformations and modernizations of enterprise data solutions using Azure cloud data technologies. 
Provide expertise and value in migrating to a full Azure PaaS/DaaS solution including Data Lake, Azure SQL Server, other Azure databases, and Azure Synapse. 
Design and Build Modern Data Pipelines and Data Streams 


REQUIRED SKILLS: 

Relevant professional experience with clinical and/or financial data, preferably in the healthcare field. 
Demonstrated experience of turning business use cases and requirements to technical solutions. Experience in business process mapping of data and analytics solutions. 
Hands-on experience in SQL Server relational and dimensional environments. 
Hands-on experience with Azure Data & Analytics PaaS Services: Azure Data Factory, Azure Data Lake, Azure DataBricks, Azure Cosmos DB, Azure SQL DW, and Azure SQL. 
Experience with Git/Azure DevOps. 
Knowledge of Lambda and Kappa architecture patterns. 
Knowledge of Master Data Management (MDM) and Data Quality tools and processes. 
Working experience with Visual Studio, PowerShell Scripting, and ARM templates. 


Preferred Capabilities: 

One or more of the Microsoft Azure Role-Based Certifications (https://www.microsoft.com/en-us/learning/browse-all-certifications.aspx?certificationtype=role-based) 
Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows. 
The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration. 
Demonstrated experience preparing data and building data pipelines for AI Use Cases (text, voice, image, etc.…). 
Designing and building Data Pipelines using streams of data. 
Preferred degree in software engineering, computer operations. 


EXPERIENCE: 

Experience in an Information Technology environment working with relational and dimensional data modeling and data programming experience utilizing SQL Server, Azure SQL Server, Azure Cosmos DB and Azure Synapse 
Experience developing using Microsoft T-SQL, Microsoft SSIS packages or other ETL products, Azure Data Factory and Azure Data Bricks. 
Experience working in a HIPAA/HI-TECH compliant environment preferred. 
Experience working in an Agile DevOps environment, preferring SAFe Agile. 


HCHB requires all applicants to be US citizens or have a green card allowing them to work in the US without being subject to export control restrictions.



Degree Level : Bachelor's Degree"
"Data Engineer II, Store Operations - Dallas, TX",H-E-B,"Dallas, TX 75220 (Northwest Dallas area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=2bd15e1ce281b509&fccid=c629e32155ebd42c&vjs=3,"Overview: 
 H-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, 
 H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. 
 H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.
  Responsibilities: 
 
 About H-E-B


H-E-B is one of the largest, independently owned food retailers in the nation operating over 400 stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.
 


 H-E-B Digital is seeking new team members (Partners)! Since our inception, we’ve been investing heavily in our customers’ digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital, and we’re hiring across the stack: front-end web and mobile, full-stack, and backend engineering. We’re using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. Our digital solutions are growing in popularity and adoption—like Curbside and Home Delivery—so you’ll get the opportunity to define the user experience for millions of customers and hundreds of thousands of Partners. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.
  
 Our Partners thrive The 
  H-E-B Way. In the 
  Data Engineer II position, that means you have a…
 


 HEART FOR PEOPLE… you can organize multiple engineers, negotiate solutions, and provide upward communication
 

 HEAD FOR BUSINESS… you consistently demonstrate and uphold the standards of coding, infrastructure, and process
 

 PASSION FOR RESULTS… you’re capable of high-velocity contributions in multiple technical domains
  

What you’ll do


 Work with HEB Digital teams to provide data solutions for store operations
 Contribute to existing data platforms and implement new technologies
 Develop a deep understanding of HEB’s data and become a domain expert
 Ensure data is distributed in a timely and accurate manner
 Make data discoverable and accessible to business users
 Operate independently with assistance from more experienced developers as needed
 Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications
 Mentor and provide support to junior developers


 Projects you’ll impact


 POS Data Transformation - Architect and develop a streaming point-of-sale (POS) data pipeline to support all downstream systems and analytics. The system will be fault tolerant and involve a cloud/on-premise hybrid approach making the data more accessible.
 New Labor Modeling Focus - Building out a new cloud data management process for labor modeling that will provide the future for data science & analytics of labor rates, scheduling, financial budgeting, and A/B testing. This will include a cloud architected approach to the analytics and supporting data.
 Store Operations Cloud First Integrations - Migrate existing data warehousing pipelines and patterns from existing ETL tools to a cloud approach using a data lake architecture with dynamic processing capacity. Use various new tools and technologies to implement the next generation of data analytical systems at H-E-B. Work with other data engineers on retooling, new standards, and new processes around data management/pipelines in AWS.


 Who You Are?


 2+ years of data engineering experience
 Proficient with data technologies (e.g. Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)
 Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.
 Strong understanding of SQL and data modeling
 Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes
 Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)
 Bachelor's degree in computer science or comparable field or equivalent experience
 A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling


 What are the Perks?


   A robust Benefits plan with coverage starting Day One
   Dental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage
   Partner Care Team, for any time you have healthcare or coverage questions
   Telehealth offers 24/7 access to board-certified doctors by phone
   Partner Guidance allows free counselor visits
   Funeral leave, jury duty, and military pay (subject to applicable law)
   Maternal / paternal leave for new parents, including adoptions
   10% off H-E-B brand products in-store and online
   Eligibility to participate in 401(k)
   Opportunity to become a “Partner-Owner” after 12 months
 


 Who We Are
 H-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billion
   We hire talented people (109,000+ Partners), and give them autonomy to be creative in how they impact the business
   We’re a Partner-driven company with a Bold Promise – Because People Matter
   We embrace Diversity and Inclusion as core values, and support them with thriving company-wide programs
   We’re a truly original Texas-based company that created the Spirit of Giving to help Texas communities every day
   Once eligible, our Partners become Owners in the company. “Partner-owned” means our most important resources—People—drive the innovation, growth, and success that make H-E-B The Greatest Retailing Company
 


 Hiring in Dallas, Austin, & San Antonio locations!



 DATA3232"
Data Science Engineer,Abbott Laboratories,"Plano, TX",Posted 30+ days ago,"$71,300 - $142,700 a year",https://www.indeed.com/rc/clk?jk=07c60fe67d590b13&fccid=77426fa86bb11d7c&vjs=3,"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 115,000 colleagues serve people in more than 160 countries.

 Data science Engineer


 


Make an impact! At Abbott we’re empowering people to manage their health with digital technologies that interface with the nervous system. This opportunity directly supports Abbott’s Neuromodulation business, a fast-growing business, offering the most innovative Spinal Cord and Deep Brain Simulation platforms in the market. Abbott’s Neuromodulation business has a large Product Development site in Plano (Dallas), Texas, where this position is based. Abbott is creating groundbreaking high-quality products designed and built with the betterment of human life as the primary focus. It’s our people who make this all happen. We are thinkers, problem solvers, and innovators working together to constantly challenge the status quo.


 The Data Scientist is responsible for analyzing and interpreting complex raw data, transforming it into valuable insights and algorithms that align with neural technologies. The Data Scientist contributes technical expertise in the innovation, design, enhancement, testing, and implementation of data-driven solutions that integrate with the system. Additionally, they collaborate closely with business stakeholders to identify the key components of successful product designs and substantiate claims related to the design and its underlying mechanisms. By cultivating and disseminating their expertise, the Data Scientist ensures that all data-driven decisions are supported by robust and credible scientific evidence.




 DUTIES:



 Develop scalable machine learning pipelines and employ feature engineering and optimization techniques to enhance data set performance
 Collaborate with Technologists to integrate and process data feeds, creating and testing new scalable data sets
 Utilize various programming languages to transform big data into actionable insights
 Generate extensive databases from diverse sources of structured and unstructured data






 Analyze data and produce statistical information to identify trends and patterns
 Understand product delivery phases and provide expert analysis across product life cycles, contributing to decision-making processes
 Translate technical data into simple language, delivering recommendations and conclusions to stakeholders
 Create interactive dashboards that incorporate real-time data and visuals, experience with Power BI is a plus
 Assess complex technical challenges and define solutions to address business imperatives






 Work with cross-functional teams, including product management, development, quality, regulatory, and operations, to develop innovative strategies and business architectures
 Engage with non-technical stakeholders to comprehend business needs and define technical architecture requirements for next-generation products
 Interface with third-party software companies to integrate, optimize, and stabilize platform system software
 Mentor cross-functional teams on system usage and product integration across regulated and non-regulated software products
 Provide senior management with data on current trends and threats in the Firmware and Software industry






 QUALIFICATIONS



 Bachelor's degree in Computer Science, Electrical Engineering, Computer Engineering, Biomedical Engineering (with neural experience), Data Science, or a related technical field
 4+ years of experience as an engineer or data scientist
 2+ years of experience with data analytics and Machine Learning, including linear and non-linear regression, logistic regression, models, classification techniques, clustering, dimensionality reduction, k-NN, and pipeline development






 Experience in AI techniques and their application in data analysis and processing
 Experience in data visualization and presenting data in graphical or pictorial formats like power bi
 Proficiency in designing algorithms and using statistical and problem-structuring methods
 Ability to clean and validate data for uniformity and accuracy
 Proficiency in various programming languages, such as Java, C, C++, Python, R, and strong coding skills






 Basic understanding of statistical methods for analyzing and drawing conclusions from research data
 Ownership mentality with the ability to independently manage complex initiatives and projects, proactively seeking solutions to arising issues
 Solid verbal, written, and interpersonal communication skills, with the ability to effectively communicate at multiple levels within the organization
 Ability to leverage and/or engage others to achieve project goals
 Capacity to contribute to multiple projects and demands simultaneously






 Ability to work both within a team and independently in a fast-paced, agile environment
 Strong technical leadership and the ability to continuously build, leverage, and influence a large network of senior technology and business experts across a diverse enterprise and external partners: vendors, standards bodies, analysts, and academia



 The base pay for this position is $71,300.00 – $142,700.00. In specific locations, the pay range may vary from the range posted."
Data Engineer - (ETL & Pyspark),Maveric NXT INC,"Irving, TX 75039 (Freeport/Hackberry area)",EmployerActive 1 day ago,"$75,000 - $90,000 a year",https://www.indeed.com/company/Maveric-NXT-Inc/jobs/Data-Engineer-25befd8855b33277?fccid=71c90b19b09e328e&vjs=3,"Role: Data Engineer ( ETL & Pyspark)
Location: Irving, TX
Mode of hire: Fulltime
Nature of Industry / End Client: Banking Domain.
Mode of work: currently Hybrid Monday & Wednesday ( but should be ready to work as per employer work policy).
Start date: at the earliest since this is a backfill.
Interview: 2 rounds of discussion internal + customer round of discussion.
Must to have lead experience
Must to have good communication & articulation skills.
* We prefer candidates who are local to Irving TX or ready to relocate this Job location.
Responsibilities:
Title: Lead ETL Automation Tester.
Primary Skill: Pyspark along with Abinitio & Informatica.

7-8 years of overall experience in ETL & Automation Testing using PySpark and other tools like Ab Initio and Informatica (preferably Ab Initio).
Must possess strong technical skills in PySpark(2-3 Years).
Strong Unix Shell scripting skills.
Excellent communication skills.
Experience working in client-facing roles.
Strong analytical skills.
Strong functional domain knowledge – Banks.
Knowledge of mainframe systems is a plus.
Ability to drive automation initiatives.

Responsibilities:

Develop, implement, and maintain ETL processes using PySpark and Ab Initio or Informatica.
Create and execute automation test scripts for ETL workflows.
Collaborate with cross-functional teams to design efficient ETL solutions.
Troubleshoot and resolve issues related to ETL processes and testing frameworks.
Serve as the primary contact for clients, providing technical expertise and support.
Communicate any risks to both technical and non-technical stakeholders proactively.
Analyze data to identify patterns, trends, and potential issues.
Proactively identify and implement automation initiatives.
Collaborate with offshore teams for efficient knowledge sharing and provide the required guidance for project specific needs.

Job Type: Full-time
Pay: $75,000.00 - $90,000.00 per year
Benefits:

Health insurance
Paid time off

Experience level:

10 years
6 years
7 years
8 years
9 years

Schedule:

8 hour shift

Ability to commute/relocate:

Irving, TX 75039: Reliably commute or planning to relocate before starting work (Required)

Experience:

Pyspark along with ETL: 2 years (Required)

Work Location: In person"
"Principal, Data Engineer",MedeAnalytics,"Remote in Dallas, TX",Posted 13 days ago,,https://www.indeed.com/rc/clk?jk=353dea0498d57f34&fccid=e0da9fa1dc69faed&vjs=3,"MedeAnalytics is a leader in healthcare analytics, providing innovative solutions that enable measurable impact for healthcare payers and providers. With the most advanced data orchestration in healthcare, payers and providers count on us to deliver actionable insights that improve financial, operational, and clinical outcomes. To date, we've helped uncover millions of dollars in savings annually.

 As a senior member of the data engineering team, our Principal, Data Engineer, will be the key technical expert developing and overseeing Mede's data product build & operations. This role will build data pipelines into various source systems, rest data on the Mede/Analytics Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. A key component will be driving a strong vision for how data engineering can proactively create a positive impact on the business. This role will also help lead the development of large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of Mede's flagship data products.
 You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems. Experience working on very large scale AWS based Data lake and Data initiatives along with being hands on is required for this role.
 Responsibilities: 

Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company
Responsible for day-to-day data collection, transportation, maintenance/curation and access to the Data Lake/data repository
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders
Increase awareness about available data and democratize access to it across the company
Active contributor to code development in projects and services
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance
Responsible for implementing best practices around systems integration, security, performance and data management
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners
Develop and optimize procedures to ""productionalize"" data science models
Define and manage SLA's for data products and processes running in production
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale
Research in state-of-the-art methodologies
Create documentation for learnings and knowledge transfer
Create and audit reusable packages or libraries

Qualifications

Bachelor's degree preferred; Experience with building solutions in the healthcare space is a plus
Fluent with AWS cloud services; AWS Certification is a plus
6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools and Snowflake required
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.
2+ years in cloud data engineering experience in Oracle OCI/AWS
Experience with integration of multi cloud services with on-premises technologies
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience with MPP database technologies such as AWS Redshift, Vertica or SnowFlake
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes
Experience with version control systems like Github and deployment & CI tools.
Experience with Glue, Data Factory, Databricks and Machine learning tools
Working knowledge of agile development, including DevOps and DataOps concept
Strong change manager. Comfortable with change, especially that which arises through company growth

Benefits Include:

Incredible Medical, Dental, Vision benefits - Effective on the first of the month after your start
FREE single healthcare coverage!!!
Company paid Basic Life & AD&D Insurance, STD/LTD
ROBUST Employee Assistance Program (EAP)
401k with company match
9 paid holidays AND 3 floating holidays = 12 total!
Paid time off accrual
Employee Referral Bonus
Professional Development
and more!

This job description reflects management's assignment of essential functions. Flexibility is a necessary understanding with the natural growth of MedeAnalytics, and deviation/delegation of tasks will be presented as necessary.

 At MedeAnalytics we deeply value each and every one of our committed, inspired and passionate employees. If you're looking to make an impact doing work that matters, you're in the right place. Help us shape the future of healthcare by joining #TeamMede."
Senior Data Engineer,EXL Services,"Remote in Dallas, TX 75247",Posted 30+ days ago,"$120,000 - $165,000 a year",https://www.indeed.com/rc/clk?jk=a871e70206d4b1ea&fccid=e3e300fc88f0e813&vjs=3,"Company Overview and Culture 

EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com. 

 For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses. 


Responsibilities:


Leading and managing High Performing Teams of analysts and engineers
Work with executives for the clients
Have full command of technical competencies of the company to better provide value to the clients.
Make executive designs that impact business
Set and achieve hiring goals and build efficient onboarding for new team members.
Architect high-quality solutions, provide estimates for solutions, and manage the scope and delivery against business milestones
Work as conduit between the clients’ business demands and technical solutions

Requirement:


Knowledge and Experience of Agile Management.
Ability to handle multiple teams across technical and non-technical domains.
Knowledge of Database Systems
RDBMS and NoSQL databases
SQL including creating and running stored procedures as well as interfacing with other applications using ODBC.
Understanding of Data Warehousing, Data Lakes
Understanding of Data Models (e.g. Fact-Dimension), Data Topology (e.g. Star, Hub and Spoke) and ER (Entity Relationship) Diagrams.
Develop procedures and scripts for data migration
Knowledge of best practices of Database Design and their implementation.
Experience with Big Data Technologies such as AWS Redshift, RDS, S3, Glue, Athena, EMR, Spark and PySpark, Snowflake, Hive etc.
Experience with Redshift is a must
Experience in AWS Model deployment using step function, AWS SageMaker and other cloud services
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc
Design, develop, test, deploy, maintain, and improve data integration pipeline
Develop pipeline objects using Apache Spark / Pyspark / Python or Scala
Experience designing/implementing solutions using one or more databases (i.e. Snowflake, AWS Redshift, Synapse, Big Query, Oracle, SQL Server, Teradata, Netezza, Hadoop, Mongo DB or Cassandra).
Hands-on object-oriented programming experience using Scala, Python, R, or Java.
Professional work experience building real-time data streams using Kafka, Spark, AWS Kinesis
Ability to solve problems and quickly and efficiently.
Knowledge of Big-Data systems and analytics.
Experience in AI Machine Learning, real time analytics and Big Data platforms is a plus
Experience with model development and maintenance, and code debug
Experience with client management
 Qualification 


Graduate/Postgraduate Degree in Computer Science/Mathematics/Economics and allied fields
Diploma/Degree in Business Management preferred
7+ years of total experience.
 EEO/Minorities/Females/Vets/Disabilities 


Base Salary Range Disclaimer: The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL's total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits. 

 Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy. 

 Application & Interview Impersonation Warning – Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s)."
Associate Systems Engineer - Audio Visual and Data Networks,ELB US Inc.,"Hybrid remote in Dallas, TX 75238",EmployerActive 2 days ago,"$60,000 - $80,000 a year",https://www.indeed.com/company/ELB-US-Inc./jobs/Audio-Visual-Engineer-0437cf5d8c35f7af?fccid=8e80bab487fe40c1&vjs=3,"ELB US Inc. is a world class integrated solutions provider, specializing in visual collaboration and unified communication services and solutions. We offer a full suite of value-added services, including needs assessment, solution design, installation, training, maintenance and support, to ensure integration success. We strive to create high quality integrated solutions for all our enterprise, government and education customers. At ELB we create experiences, we communicate ideas, and we collaborate across the world!
Our company is growing and we are looking to on-board an Associate Systems Engineer to join our team.
Key responsibilities include, but are not limited to:

Consult with clients and colleagues as necessary regarding audio visual requirements.
Assess briefs provided by customers, Account Manager, or Systems Engineers and identify options for potential solutions.
Assist with project design and/or build costing and project bidding activities.
Participate in the development of logical and innovative AV solutions to complex problems, ensuring that AV system designs meet customer business requirements.
Create, edit, and track Bill of Materials for pre, and post-sales support and for product release/ordering.
Create labor estimate for pre-sales support.
Assist in the development of detailed design documentation once a contract for a system has been awarded.
Assist Project Managers, where necessary, with planning the installation of systems
Provide technical writing services when requested.
Verify installation completion and completeness.
Some equipment installation and rack build as required.
Commission AudioVisual systems in the shop and in the field as required. System commissioning may include, but is not limited to: DSP configuration and audio system tuning; Video display and processor configuration and calibration; Loading and testing completed control system code; Testing and troubleshooting the system through satisfactory completion.

Qualifications:

Possess a four-year degree, or equivalent combination of education and related work experience in the Professional Audio, Professional/Broadcast Video, Audio Visual, Security, or Data Integration fields.
Excellent written and oral communication skills with an ability to define and articulate “the vision”.
Ability to create and evaluate specifications and present recommendations and solutions to customers in a clear and articulate way.
In depth knowledge of AV theory, technology and product.
Possess general work knowledge of electronics and/or construction.
Minimum CTS Certification or equivalent qualification.
Personal management of attitude, perseverance, and time.

Job Type: Full-time
Pay: $60,000.00 - $80,000.00 per year
Benefits:

Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance

Experience level:

3 years

Schedule:

8 hour shift
Monday to Friday

Ability to commute/relocate:

Dallas, TX 75238: Reliably commute or planning to relocate before starting work (Preferred)

Education:

Bachelor's (Preferred)

License/Certification:

CTS certification (Preferred)

Work Location: Hybrid remote in Dallas, TX 75238"
Principal Data Science Engineer,Verizon,"Remote in Irving, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=01b3ebf69a9a834c&fccid=f7029f63fe5c906e&vjs=3,"When you join Verizon
 Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

 What you’ll be doing...

 In this role you’ll apply cutting-edge machine learning techniques and AI technology and will lead data science projects to reduce fraud loss and prevent future fraud loss across all Verizon wireless/wireline channels for both consumer and business line of businesses. You will work closely with a team of talented data scientists, Big Data engineers, and software developers and play a key role in developing and delivering the next generation AI/ML solutions to the fraud IT/business.

 You will:

 Create data models to predict customer behavior and risky events/transactions and use them to improve performance and predictability.
 Develop resolutions to complex problems, using your sharp judgment to develop methods, techniques, and evaluation criteria, enabling you to deliver solutions that make a big impact.
 Engage with key business stakeholders in discussions on business strategies and opportunities.
 Build strong working relationships and develop deep partnership with the other IT teams and business.
 Lead small-to-medium portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.
 Lead the design and development of machine learning/statistical models and ensure best performance.
 Work closely with engineers to deploy models in production both in real time and in batch process, and systematically track model performance.
 Be a subject matter expert on machine learning and predictive modeling and a mentor to junior data scientists.
 Drive technical innovation through active research and applications of new theories, techniques, and technologies.


 What we’re looking for...

 With an eye towards improving performance and predictability, you like the science of analytics. You’re able to communicate technical information to non-technical audiences, and you take pride in your ability to share your considerable knowledge with your peers.

 What you'll need to have...

 Bachelor’s degree and 6 or more years of work experience.
 Hands on classification model development using Python or equivalent programming languages
 General SQL proficiency one or many (Postgres, Oracle, SQL Server, Teradata).
 Five or more years of experience in practicing machine learning and data science in business.
 Experience with Relational Databases.
 Experience in leading medium-scale data science projects and delivering end-to-end.
 Strong communication and interpersonal influencing skills.


 Even Better if you have one or more of the following:

 Master’s degree in a quantitative field or relevant field.
 Strong foundational quantitative knowledge and skills; extensive training in math, statistics, physical science, engineering, or other related fields.
 Development experience with PL/SQL and SQL*Plus.
 Experience with UNIX shell scripting using Python.
 Experience interacting with the AWS ecosystem
 Technical experience in machine learning and statistical modeling.
 Experience in data management and data analysis in relational database.
 Excellent problem solving and critical thinking capabilities.
 Strong experience in SQL and database management.
 Exposure to the scikit-learn, pandas and numpy ecosystems
 Experience with other data analysis software and scripting languages (e.g., R, SAS).
 Strong background in statistics.
 Fraud or risk management/modeling experience.
 Ability to bring in new technologies and streamline the existing and new applications.
 Ability to take immediate ownership of unanticipated fraud scenarios and quickly develop ad-hoc solutions when necessary.


 If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

 This role may be considered as part of the Department of Defense SkillBridge Program.








 Where you’ll be working






 In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
 Scheduled Weekly Hours 40
 
 Equal Employment Opportunity
 We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more."
Data Engineer - Mulesoft,American National Bank of Texas,"Plano, TX 75074",EmployerActive 7 days ago,Full-time,https://www.indeed.com/rc/clk?jk=419a48250e2f8b02&fccid=83f67c59739e4e6c&vjs=3,"The Data Engineer (Mulesoft) is responsible for the design, build, and optimization of systems for data collection, storage, access and analytics at scale. The position is responsible for creating pipelines, building algorithms for accessing raw data, and providing data that is accurate, congruent, reliable, and easily accessible. Additionally, the position is responsible for the full life cycle development, implementation, production support, and performance tuning of the Enterprise Data Warehouse, Data Marts, and Business Intelligence Reporting environments. The role supports the integration of those systems with enterprise application databases and real time processing

Designs, builds and optimizes systems for data collection, storage, access, and analytics AT SCALE (i.e., data that is collected, stored, secured, etc. across the entirety of the bank's technology systems)
Builds out new API integrations to support continuing increase in data volume and complexity
Utilizes the latest technology and information management methodologies to meet requirements for effective logical data modeling, metadata management and warehouse domains
Assists with the development of architectural strategies for data modeling, design and implementation for metadata management, operational data shores and Extract Transform Load (ETL) environments
Creates and test data models for a variety of business data, applications, database structures to meet operational or project goals
Performs data analysis required to troubleshoot data related issues and assist to resolve issues
May require work on physical bank premises

Qualifications:

5+ years IT experience preferably in the financial industry
5 years experience developing dashboards and reports with a leading business intelligence and analytics platform like DOMO, Tableau or QLIK
5 years experience with a low code solution, Mulesoft experience
5 years experience experience developing and/or supporting SQL databases preferably MS-SQL
5 years experience working with integration platforms or middleware
5 years user requirements gathering/user case development experience; experience converting legacy reporting platforms to dashboards; experience transforming from a manual process centric environment to a fully automated environment; experience with API integration development; experience developing and/or support data warehouse architecture, preferably Snowflake

Skills:

Advanced proficiency in the use of Microsoft Excel, Word, Access, SQL Database, modeling, data manipulation; basic keyboarding and calculator skills; must be able to perform advanced math and carry out complex written instructions
Travel to a variety of branch and vendor locations to perform work and/or attend meeting as required
Work occasionally requires more than 40 hours per week to perform the essential functions of the position
Lifting in an office setting may be required up to 30 lbs.

ANBTX strongly encourages candidates that are fluent in English and Spanish to apply. Jobs that specifically require candidates to be bilingual will be posted as a requirement."
Sr Data Engineer with Vertex.AI,Gannett,"Remote in Plano, TX 75093",Posted 19 days ago,"$140,000 - $160,000 a year",https://www.indeed.com/rc/clk?jk=c00bc988b784e5ba&fccid=1230acb7e56c6df5&vjs=3,"Gannett Co., Inc. (NYSE: GCI) is a subscription-led and digitally-focused media and marketing solutions company committed to empowering communities to thrive. With an unmatched reach at the national and local level, Gannett touches the lives of millions with our Pulitzer Prize-winning content, consumer experiences and benefits, and advertiser products and services. 

 Our current portfolio of media assets includes The USA TODAY NETWORK, which includes USA TODAY, and local media organizations in 43 states in the United States, and Newsquest, a wholly-owned subsidiary operating in the United Kingdom. We also own digital marketing services companies under the brand LocaliQ, which provide a cloud-based platform of products to enable small and medium-sized businesses to accomplish their marketing goals. In addition, our portfolio includes one of the largest media-owned events businesses in the U.S., USA TODAY NETWORK Ventures. 

 Gannett open roles are featured on various external job boards. When applying to a position at Gannett, you should be completing an application on Gannett Careers via Dayforce. Job postings directing you to complete an application on other external sites may not be valid. 

 To connect with us, visit www.gannett.com 


Job Specification: Sr Data Engineer with Vertex.AI 

Location: Remote 

Salary: $140,000-$160,000 based on skills, experience, location, and union representation, if applicable. 


Position Overview:
 We are seeking a skilled and experienced Data Engineer to join our team at Localiq DMS. The ideal candidate should have a strong background in data engineering, with specific exposure in working with the Vertex.AI platform. As a Data Engineer, you will play a crucial role in developing and maintaining our data infrastructure, ensuring the efficient extraction, transformation, and loading of data from various sources. Your expertise with Vertex.AI will be instrumental in leveraging the platform's capabilities to enhance our data processing and analysis workflows. 


Responsibilities:


Develop and maintain data pipelines and ETL processes using Vertex.AI platform to support data acquisition, transformation, and loading.
Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to design and implement efficient data processing solutions.
Design and optimize data models and schemas to support business requirements and data analysis needs.
Implement data quality checks and ensure data integrity throughout the data pipeline.
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner.
Work closely with stakeholders to understand data requirements and translate them into technical specifications.
Research and evaluate new technologies and tools related to data engineering and provid recommendations for improvements.
Stay up to date with industry best practices and emerging trends in data engineering and data management.

Requirements:


Bachelor's degree in Computer Science, Engineering, or a related field. Equivalent work experience will also be considered.
Proven experience as a Data Engineer, with a focus on designing and implementing data pipelines and ETL processes.
Strong expertise in working with the Vertex.AI platform, including familiarity with its capabilities and features.
Proficiency in programming languages such as Python, Java, or Golang for data processing and manipulation.
Experience with distributed computing frameworks like Apache Spark for large-scale data processing.
Knowledge of SQL and database systems (e.g., PostgreSQL, MySQL) for data querying and manipulation.
Familiarity with cloud platforms such as AWS, GCP, or Azure and their respective data services.
Understanding of data warehousing concepts and experience with data modeling and schema design.
Excellent problem-solving and troubleshooting skills, with the ability to identify and resolve data-related issues.
Strong communication skills and the ability to collaborate effectively with cross-functional teams.

Preferred Qualifications:


Advanced degree in a relevant field, such as Data Science or Computer Engineering.
Experience with other data engineering platforms and tools, in addition to Vertex.AI.
Knowledge of machine learning concepts and experience with integrating ML models into data pipelines.
Familiarity with agile development methodologies and version control systems (e.g., Git).
Experience working in an advertising or marketing data environment will be a plus.
 #LI-REMOTE 
#LI-SG1 
#PRODUCTGNT 

 The annualized base salary for this role will range between $90,000 and $213,900. Variable compensation is not reflected in these figures and based on the role, may be applicable. Exact compensation may vary based on skills, experience, location, and union representation, if applicable. 

 Gannett Co., Inc. is a proud equal opportunity employer committed to building and maintaining a diverse workforce. As such, we will consider all qualified applicants for employment and do not discriminate in connection with employment decisions on the basis of an applicant or employee’s race, color, national origin, ethnicity, ancestry, citizenship status, sex, gender, gender identity, gender expression, religion, age, marital status, personal appearance (including height and weight), sexual orientation, family responsibilities, physical or mental disability, medical condition, pregnancy status (including childbirth, breastfeeding or related medical conditions), education, genetic characteristics or information, political affiliation, military or veteran status or other classifications protected by applicable federal, state and local laws in the jurisdictions where Gannett employs employees. In addition, Gannett Co., Inc. will provide applicants who require a reasonable accommodation, as a result of an applicant’s disability or religion, to complete this employment application and/or any other process in connection with an individuals’ application for employment with Gannett Co., Inc. Applicants who require such accommodation should contact Gannett Co., Inc.’s Recruitment Department at Recruit@gannett.com."
Big Data Engineer with Active VOS,"Pyramid Consulting, Inc","Dallas, TX",Posted 14 days ago,"$115,000 a year",https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Bjic9BpODao-m9BEup4myv2yv9o6hanv70kCRpjMjSDb0tRlE4H9I0NFhFwwd29vaMPFSsGY5bemtuTxVH8QBWyQRQULpv_acqngu7TkPNrov50Fb5U5vPKzqBgo5q0vWB_jrwMBoZp3IGvxaFbl5gt4b7_hrx-MmOzYP4zXi2F9GdW1Aj5X2sLC0fKbY3aQNAEARxdk4hiFIoLowsb2ZjIVaV4LtoTo8sK8ju2j2Msja74Q0COc8M1ItXJ8KmZjp3ALywx3flF_3I-838uOAwXTQK5YdX5A06KR6tEeNEAb-elv2nyhc-ywocEyL6vPNAsXPsbhe5SivhtbaFaou124lMtwdaCrBpcWO6BqxyVnn4mHLN3BnOG8Q6vPn0qRMxwaQkiG12ionMv-taJgbHA70jq1L7GFFMPFZxM0F-DjF98CQ1nIH5-OitHxw_Iq4a4gDtykfXR2WhMlV43_pkvoX3fRrVQsD4VsqaiBDPPavbt5hKcPZ4xDEk3xWLxH-ahrSx0MZDMSnXctGeGC7ipaMd046bqyE5zj7VQVzlNBinogfF89UmK1PzIFSaDH0R0OVIBaAYRSp-A-WQSUZhpnwfprMqhymLTjYfYJ_SvRFYomNaKWhJii12NuQAy-h5zkJsjpT8RMPb0lJQlOMW8BWKlUYLu5FFDFeoVbD5PC0RPkejJwSqg4sH3MGeCMQq0tEeUiR2L4ZyhHyvooimZh9pLYE1x_YLJf7x7b6tkl2ZRCqapePICZfVgo6F72BMcgN_e2rUb_h2yWc9hS0QCd2-dI_2-pEEAEhkNLIiqi-ofwLTFmMmRruBCkPllTc29TocmC47BEz_JTE75VhYULQViEO_NKM0NAB_0jDpfdkzXny7edyRmfQS174IhWqLV1QlHVHdSRX4BQmbufZ2BBSyDyVtHJzNc0Ws908HTODnFRe8bDqZeDjxeAwT0PZpOZsMLtF6-CkiB6ovOSQ5ePSJkJuL-sBhob4UmWQNZkw81i2gcpVoWaZGNNVYcbSwf5D8cboESbnhVDoJlrkgAKyvJHdMTBa6rZnsdWCHdVszFGGODFo3&xkcb=SoCm-_M3ML2qsbxd6R0GbzkdCdPP&p=13&fvj=0&vjs=3,"Immediate need for a talented
   Big Data Engineer with Active VOS. This is a 
  06+ Months contract opportunity with long-term potential and is located in 
  Dallas, TX, Atlanta, GA (Initial Remote for 1 month and then Onsite). Please review the job description below and contact me ASAP if you are interested.
  

Job ID: 23-29355


Pay Range: $115k/Annum. Employee benefits include, but are not limited to, health insurance (medical, dental, vision), 401(k) plan, and paid sick leave (depending on work location).

 

Key Requirements and Technology Experience:


Overall, 8+ years of experience in Applications development/deployments/implementation with AIOPs being focus area.
Must have development experience with Java, Cramer, ActiveVOS, BPEL, Flex.
Well versed with product/solution implementation life cycle.
Experience in implementing applications with AIOPS open-source technologies Big Data, Python, NodeJS/Angular with NoSCH.
As part of implementation, should have good knowledge of Integrating front-end and back-end application components developed.
Experience with NoSQL databases such as MongoDB, Elasticsearch and Big data stacks like ELK.
Exposure to cloud-based deployments (GCP/AWS/AZURE/Private).
Knowledge of Machine learning and relevant certification.
Experience with Client is a plus.


  Our client is a leading 
  IT Industry and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration.
  
 Pyramid Consulting, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
  
 #DEL"
Data Engineer II,"AmTrust Financial Services, Inc.","Hybrid remote in Dallas, TX 75244",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=400dc68ef5de7147&fccid=e6e094c308c82aee&vjs=3,"Overview: 
 
   Responsible for design, development, testing, and maintenance of data architectures including large scale databases, data pipeline processes, and data delivery solutions. Maintains a solid understanding of AmTrust’s mission, vision, and values. Upholds the standards of the AmTrust organization.
  Responsibilities: 
 
Assemble large, complex sets of data that meet the reporting and analytics needs of the enterprise.
 Analyze raw data sources and develop, test, and implement optimized data pipeline solutions and controls.
 Design, develop, test, and implement queries, reports, cubes, and dashboards in support of business analytic needs.
 Continuous focus on improving data quality and efficiency.
 Interface with business analysts to discuss timelines and clarify requirements as it pertains to new projects, enhancements and bug fixes. 
Adherence to DevOps, SDLC, and Change Management practices 
Keeps current with market trends and demands.
 Performs other functionally related duties as assigned.
 Qualifications: 
 
Advanced knowledge of SQL, T-SQL and/or PL/SQL; Ability to write complex, highly optimized queries across large volumes of data.
 A thorough understanding of data pipeline construction and ETL processes. 
Proficiency with SSIS | SSAS | SSRS
 Expertise in database design methodologies.
 Excellent analytic skills.
 5+ years of data focused technical experience


 Preferred:

 Excellent oral and written communication skills.
 Ability to communicate complicated/ technical information to non-technical audiences in an efficient and simple method.
 Curiosity and passion for data, visualization and solving problems
 Highly creative in order to determine the best solutions for real-world problems with quantitative data.
 Enjoy collaborating with others in a team atmosphere.
 Eagerness to learn in a fast-paced environment.
 B.S. degree in Computer Science, Math, Statistics, or related technical field, or equivalent professional experience.
 Property & Casualty insurance experience a plus.



 This job description is designed to provide a general overview of the requirements of the job and does not entail a comprehensive listing of all activities, duties, or responsibilities that will be required in this position. AmTrust has the right to revise this job description at any time.
 


 #LI-GD1
 

   #LI-HYBRID
  What We Offer: 
 
   AmTrust Financial Services offers a competitive compensation package and excellent career advancement opportunities. Our benefits include: Medical & Dental Plans, Life Insurance, including eligible spouses & children, Health Care Flexible Spending, Dependent Care, 401k Savings Plans, Paid Time Off.
 


 AmTrust strives to create a diverse and inclusive culture where thoughts and ideas of all employees are appreciated and respected. This concept encompasses but is not limited to human differences with regard to race, ethnicity, gender, sexual orientation, culture, religion or disabilities.
 


 AmTrust values excellence and recognizes that by embracing the diverse backgrounds, skills, and perspectives of its workforce, it will sustain a competitive advantage and remain an employer of choice. Diversity is a business imperative, enabling us to attract, retain and develop the best talent available. We see diversity as more than just policies and practices. It is an integral part of who we are as a company, how we operate and how we see our future."
Data Engineer,Omnitech Consulting,"Plano, TX",EmployerActive 2 days ago,$50 - $60 an hour,https://www.indeed.com/company/Omnitech-Consulting/jobs/Data-Engineer-56a2e4470bb5667f?fccid=1587c0d5ad922a82&vjs=3,"About us
Omnitech Consulting LLC, a technology consulting firm dedicated to providing innovative solutions for businesses of all sizes. We work closely with our clients to understand their unique talent needs and develop customized approach and provide skilled talent that align with their business objectives.
Job Description
W2 Only, Hybrid Work at Plano TX.
The Data Engineer will be responsible for the following:

Data engineering and design, including data modeling, data cleansing, and data integration
Designing, implementing, and supporting data systems and tools to meet company’s business needs
Developing data models to support the collection, storage, analysis, and reporting of data
Implementing data integration and reporting tools to support business needs
Developing and maintaining data-driven reports and dashboards for management and internal use
Developing code that is efficient and maintainable for production use
Maintaining a strong understanding of the underlying database technologies, including SQL Server, MS SQL Server, Oracle, etc.
Working with the development team to ensure the application is built in a way that is scalable and maintainable.
Other duties as assigned.

Requirements:

Min 3 to 5 years of experience in Python
Min 3 to 5 years of experience in SQL
Min 3 to 5 years of experience in Cloud
Min 3 to 5 years of experience in ETL
Bachelor’s degree in Computer Science or related field; Master’s degree preferred. Master’s degree preferred.
5+ years of experience in a similar role.

Benefits:Omnitech Consulting offers competitive compensation based on experience.
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Benefits:

Paid time off

Compensation package:

Hourly pay

Experience level:

5 years

Schedule:

Monday to Friday

Ability to commute/relocate:

Plano, TX: Reliably commute or planning to relocate before starting work (Required)

Education:

Bachelor's (Required)

Experience:

Python: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Cloud computing: 3 years (Required)
Azure Data Lake: 3 years (Required)
Corporate finance: 3 years (Required)

Work Location: In person"
Data Engineer,"Stefanini, Inc","Dallas, TX",Posted 1 day ago,$60 - $65 an hour,https://www.indeed.com/rc/clk?jk=72eef199562b6fa5&fccid=7e646b8cb4a11b13&vjs=3,"Stefanini is looking for a Data Engineer in Dallas, TX (hybrid role). W2 only. US Citizens only


 For quick apply, please reach out to Vishal Sharma- 
Vishal.sharma@stefanini.com
 / 248.263.5616



 The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support. You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages. You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality. As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members.
 


 Responsibilities:


 Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns
 Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity
 Work under general guidance and clear framework of accountability with substantial autonomy
 Use best practices and knowledge of internal or external business issues to improve products or services
 Solve complex problems; takes a new perspective using existing solutions





Required Skills:


 An associate degree, a bachelor's degree in computer science or equivalent courses
 At least 4 years of experience in Data Engineering with SQL, Python
 Experience with relational SQL and NoSQL databases
 Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)
 At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)
 At least 2 years of experience with AWS cloud (with focus on Data services)
 Experience with Databricks a plus
 Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs
 Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix/Linux shell scripting
 Able to interpret user requirements and identify additional information needed in user requirements
 Able to see effects of current design with future requirements and see possible coding solutions to meet the requirements
 Detailed understanding of logical and physical data structures
 Highly skilled in tools, evaluates the need for various tools for continuous integration, testing, automation, deployment etc. and discuss with the team
 Highly skilled at designing tests for unfamiliar designs; Evaluates tests for weaknesses and continuously improves them
 Detailed understanding of how to effectively test against multiple tools/software
 Equivalent education and/or experience may be substituted for any of the above requirements



 ***Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives.

 Stefanini takes pride in hiring top talent and developing relationships with our future employees. Our talent acquisition teams will never make an offer of employment without having a phone conversation with you. Those face-to-face conversations will involve a description of the job for which you have applied. We also speak with you about the process including interviews and job offers."
Data Engineer,Kommforcesolutions,"Dallas, TX",EmployerActive 2 days ago,$65 - $70 an hour,https://www.indeed.com/company/Kommforcesolutions/jobs/Data-Engineer-e68f8198d648ddea?fccid=b9aa8c10996bb21c&vjs=3,"Job title: Data Engineer ( USC GC )
Location: Dallas, TX ( Hybrid )
Client: Southwest Airlines
Looking for strong experience with Abinitio and AWS.
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Work Location: On the road"
Data Engineer,OneSource Regulatory,"Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=ace7d2b8833db496&fccid=13c11dac3d323b9a&vjs=3,"Company Introduction
 OneSource Regulatory Technology hosts a number of innovative solutions to enhance job performance in the Pharmaceutical space. OSR Technology is looking for an experienced and dedicated data engineer to join our product solutions team!

 
Job Description
 OneSource Regulatory is trying to identify a full-time contractor with at least 4+ years of experience to assist us with ongoing R&D projects.
 We are looking for a data engineer to pull data from various sources and do all the necessary steps to clean, normalize, possibly annotate, and finally load the data into databases. The candidate should be able to develop and implement a strategy for testing the data integrity of the collected data. This role requires extreme attention to detail to ensure data quality is top priority.

 Responsibilities

Well versed in parsing and synthesizing of XML and/or JSON documents.
Curating of data that can involve some intermediate to advanced web scraping. (data may need to be fetched via SFTP, FTP, Wget, Curl, REST APIs, GraphQL queries from spots on the Internet)
Proficiency with Linux command line and various simple tools, such as grep, wc, sed, awk, find, ls, cat, piped commands and possibly some very light Bash shell scripting, setting up crontab schedules and programs
Must have basic knowledge of SQL with the following databases: PostGres, MySQL, Google BigQuery
Must have basic knowledge of No-SQL database knowledge such as MongoDB or similar
Familiarity with basic Cloud technology such as storage buckets, cloud serverless functions
Must have experience extracting text and images from PDF files
Knowledge of Puppeteer or other automatable web client technologies
Understanding JavaScript, HTML/CSS and HTTP methods (for understanding page structure for web scraping)


Skills

Solid experience with Python and Python Libraries such as Pandas, requests, etc
Skill set should match up with required responsibilities listed above
Strong English skills (e.g. grammatical analysis and rhetorical structure)
Team Player
Great communication skills



 Bonus Skills

Experience within the Pharmaceutical Space
Ability to expose data via C# NETCore and/or GraphQL
Google Cloud Platform (Cloud Buckets, Google Cloud Functions (.NET, Python, Node.JS))
Ability to parallelize data manipulation and scraping via Python multi-threading, etc.
Python BeautifulSoup
Scrapy
Docker (setting up Kubernetes style processing if warranted for data scraping/data ingestion/normalization)
Multithreading concepts"
Data Engineer,Garner Health,"Remote in Dallas, TX",Posted 30+ days ago,"$100,000 - $145,000 a year",https://www.indeed.com/rc/clk?jk=d872bc5b38737852&fccid=6e1d1accd5aeee95&vjs=3,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.

 We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.
 Main Responsibilities:

Build the data pipelines that power our business
Collaborate across disciplines to high-quality datasets
Protect our users' privacy and security through best practices
Support data pipelines in production

Our Tools:
 Python, AWS, Snowflake, dbt, Terraform, Postgres
 The ideal candidate has:

2+ years of experience building data pipelines in a fast-paced environment
Strong Python knowledge
Experience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricks
Ability to think in principles and frameworks to understand and decompose abstract problems
An aptitude to learn new technologies and tools quickly

Why You Should Join Our Team:

You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies

The salary range for this position is $100,000 - $145,000. Compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.


 If you are hired, we may require proof of full vaccination against COVID-19. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement, in accordance with applicable law."
Sr. Azure Data Engineer,Kairos Technologies,"Dallas, TX 75201 (Downtown area)",EmployerActive 24 days ago,$75 - $80 an hour,https://www.indeed.com/company/Kairos-Technologies/jobs/Data-Engineer-a379a589a4baee37?fccid=609eeb6b6a1a2e5b&vjs=3,"Hello,
Hope you are doing great!
If you are comfortable with the requirement, kindly respond with below information and your updated resume( word formatted ) ASAP
Full Name:
Current Location:
Contact No Primary:
Work Authorization:
S.S.N No( Last 4 digits ):
D.O.B( MM/DD ):
Availability( Any notice period required to join in project ):
Currently working(Yes/No):
LinkedIn:
Employer details( If on C2C ):
Please check the below requirement
Direct Client Requirement
Position: Sr. Data Engineer with Azure Cloud and Strong SQL( Hybrid/ Onsite for 2 days a week )
Location: Dallas TX or VA
Duration: 12+ Months Long Term Contract
Work Authorization: US Citizen/GC/GC-EAD/TN Visa( on C2C is fine )
Need Locals Candidates..
Job Summary:
We are seeking an experienced and highly skilled Senior Database Engineer with strong hands-on experience in modernizing and automating on-premises SQL databases, as well as a solid track record of successful migration projects from on-premises to Azure cloud. As a Senior Database Engineer, you will play a critical role in driving our database modernization efforts, ensuring scalability, performance, and security in the Azure cloud environment.
Responsibilities:

Lead the modernization and automation of on-premises SQL databases, implementing best practices and efficient processes to enhance performance, scalability, and reliability.
Design and execute successful migration strategies from on-premises databases to Azure cloud, ensuring seamless data transfer and minimal downtime.
Develop and implement automation solutions using tools such as Python, PowerShell, and Azure DevOps (ADO) to streamline database management tasks, including provisioning, backup and recovery, monitoring, and deployment processes.
Collaborate closely with cross-functional teams to gather requirements, understand business needs, and propose effective database solutions that align with the overall technical architecture in the Azure cloud environment.
Utilize tools like Liquibase or similar to manage database schema changes and version control.
Implement effective monitoring and logging solutions using tools like Splunk and Datadog to ensure database performance, availability, and security.
Perform thorough assessments and evaluations of existing on-premises databases, identifying areas for improvement, optimization, and consolidation prior to migration to Azure cloud.
Ensure data integrity, security, and compliance with industry standards and regulations throughout the database modernization and migration process in the Azure cloud environment.
Provide guidance and mentorship to junior database engineers, fostering their technical growth and promoting best practices in database management, automation, and migration in the Azure cloud.
Stay up to date with the latest trends, tools, and technologies in database management, Azure cloud services, and DevOps practices, evaluating their potential impact and relevance to our organization.
Collaborate with vendors and third-party providers to assess and implement new technologies, tools, and services that can enhance our Azure cloud database environment and support our modernization goals.

Qualifications:

Bachelor's degree in computer science, information technology, or a related field.
Minimum of 10 years of hands-on experience in database engineering, with a strong focus on SQL databases and Azure cloud.
Extensive experience in modernizing and automating on-premises SQL databases, including performance optimization, scalability improvements, and process automation.
Proven expertise in successfully migrating on-premises databases to Azure cloud, ensuring data integrity and minimal disruption to operations.
Strong proficiency in SQL scripting, database performance tuning, backup and recovery, and database security practices in the Azure cloud environment.
Experience with database migration tools and technologies, such as Azure Database Migration Service, Azure Data Factory, or similar tools.
Solid understanding of Azure cloud database services, including Azure SQL Database, Azure SQL Managed Instance, and Azure Cosmos DB.
Familiarity with automation and configuration management tools like Python, PowerShell, and Azure DevOps (ADO) for streamlining database management and deployment processes in Azure.
Experience in using tools like Liquibase or similar for managing database schema changes and version control.
Knowledge of monitoring and logging solutions like Splunk and Datadog for database performance and security monitoring in Azure cloud.
Strong analytical and problem-solving skills, with the ability to troubleshoot complex database issues and propose effective solutions in the Azure cloud environment.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.

Tech Stack:

Databases: On-premises SQL databases (e.g., Oracle, MySQL, PostgreSQL)
Cloud Platform: Microsoft Azure (Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB)
Scripting and Automation: Python, PowerShell
Version Control: Git
CI/CD: Azure DevOps (ADO)
Database Migration: Azure Database Migration Service, Azure Data Factory
Database Schema Management: Liquibase or similar tools
Monitoring and Logging: Splunk, Datadog

Thanks and Regards
Prasad Mamidela | Kairos Technologies Inc.
Direct Number: 972.777.9484 | Mobile: 201.613.3664
433 E Las Colinas Blvd, # 1240, Irving, TX 75039 USA
http://www.kairostech.com​
LinkedIn: https://www.linkedin.com/in/prasadmamidela
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:

11+ years

Schedule:

Monday to Friday

Ability to commute/relocate:

Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)

Experience:

Azure: 4 years (Required)
SQL: 6 years (Required)
On-premises: 2 years (Required)
Total IT: 10 years (Required)
Azure DevOps: 2 years (Required)

Work Location: In person"
Data Engineer,KEA,"Rockwall, TX 75087",Posted 15 days ago,Full-time,https://www.indeed.com/rc/clk?jk=ab2b09cdc1120e91&fccid=7f54ef3ce9a55e38&vjs=3,"We have an immediate need for a Data Engineer whose primary role will be compiling comprehensive data and information on energy and commercial real estate data, and managing the back-end framework while helping the team utilize the data to create front-end visuals. This also involves architecting the back-end system to be scalable in use with web applications. Use multiple software platforms and develop and create reports and graphics to show trends and analysis for internal and external reports. The role is based in Rockwall, 10 miles east of Dallas, Texas. We are a team-first culture that involves leaning on the expertise around you, so any experience in this regard is helpful.


 Analyze, automate, and prepare data for filings in certain key markets
Track, maintain and disseminate detailed data for major CRE markets, including: Leasing activity and comparables, Building and land sales activity and comparables, Ownership analysis
Utilize shape files and platforms such as Mapbox and ArcGIS and tie back to big data
Prepare and develop a monthly market report detailing sales transactions at the submarket level in major markets
Analyze and integrate large amounts of oil & gas data into our system
Work with the business development team to prepare external market reports as thought leadership in the marketplace
Prepare and create multiple template reports out of multiple software systems




Bachelor's degree in economics, real estate, computer science, engineering, or finance
Usage and full understanding of shape files and a platform such as Mapbox or ArcGIS
Understanding of Python, C#, and R
Web application building a plus
3+ years of work experience
Previous usage of Tableau or Alteryx
Strong data and analysis skills including Excel and Microsoft Office
Strong communication skills, with the proven ability to coherently ideas and opinions using the data and communicate them




Full health benefits
1-2 days at home per week
Full dental benefits
401(k) 4% automatic contribution
Competitive compensation
9/80 opportunities"
Data Engineer,Varant Inc,"Hybrid remote in Coppell, TX 75019",PostedToday,"$90,000 - $95,000 a year",https://www.indeed.com/company/varant-inc/jobs/Data-Engineer-9f09de0ba05fbd58?fccid=c7b656059b9fa04c&vjs=3,"Responsibilities:-Analyze and organize raw data
-Build data systems and pipelines
-Evaluate business needs and objectives
-Interpret trends and patterns
-Conduct complex data analysis and report on results
-Prepare data for prescriptive and predictive modeling
-Build algorithms and prototypes
-Combine raw information from different sources
-Explore ways to enhance data quality and reliability
-Identify opportunities for data acquisition
-Develop analytical tools and programs
-Collaborate with data scientists and architects on several projects
Requirements:
-Previous experience as a data engineer or in a similar role
-Technical expertise with data models, data mining, and segmentation techniques
-Knowledge of programming languages (e.g. Java and Python)
-Hands-on experience with SQL database design
-Great numerical and analytical skills
-Degree in Computer Science, IT, or similar field; a Master’s is a plus
-Data engineering certification (e.g IBM Certified Data Engineer) is a plus
If you are a highly motivated individual with a passion for Data engineering and want to be part of a dynamic team working on cutting-edge technologies, we encourage you to apply. We offer competitive compensation packages, opportunities for professional growth, and a collaborative work environment.
Job Types: Full-time, Permanent
Pay: $90,000.00 - $95,000.00 per year
Benefits:

401(k)
Dental insurance
Health insurance

Experience level:

8 years

Schedule:

Monday to Friday

Ability to Commute:

Coppell, TX 75019 (Required)

Work Location: Hybrid remote in Coppell, TX 75019"
Data Engineer,Loopback Analytics,"Dallas, TX 75254 (Far North area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=40c74898dc1dbaa4&fccid=6e48fc883bfc6b37&vjs=3,"This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.

 Come join our Real World Data team at Loopback Analytics, the Loopback platform assembles clinical, pharmacy, enterprise and social data for insight and action across the specialty pharmacy and life sciences value chain. The ideal candidate would be an experienced Data Engineer who will be responsible for building and maintaining data pipelines. The Data Engineer will facilitate deeper analysis and reporting across complex data sets to support customers.


 Job Duties to Include

Assemble and manage large, complex sets of data to meet functional business and analytical requirements
Build required infrastructure, documentation and roadmap for optimal extraction, transformation and loading of data from various data sources
Design infrastructure for greater scalability, optimizing data delivery and automating manual processes 
Plan, coordinate and implement security measures to safeguard data 
Work with stakeholders including data, product and executive teams and assist with data-related technical issues
Develop and maintain processes for data profiling, data documentation, and data quality measurement leveraging both manual and automated data quality testing



 Requirements


 Technical Experience: 3-5 years of experience to include:

Implementing and designing data infrastructure to support data curation and data analysis
Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP)
Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.)
Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders
Documenting and testing of designed solutions 
Writing code that runs in a production system or experience in machine learning



 Required Education:

Bachelors, masters, or Ph.D. in computer science, software engineering or a related field or equivalent experience



 Personal Characteristics:

Complex problem solver
Excellent program/task organizational skills 
Detail and results oriented
Excellent communication skills



 Travel:

Minimal



 All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin. For immediate full-time consideration, please forward your resume to Loopback Analytics via email at careers@loopbackanalytics.com.


 About Loopback


 Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ. Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading academic medical centers, health systems, and life sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com."
Data Engineer,Ad hoc Research,"Hybrid remote in Plano, TX 75074",EmployerActive 2 days ago,Full-time,https://www.indeed.com/company/Ad-hoc-Research-Associates/jobs/Data-Engineer-8432a06788146f15?fccid=cf83219912ebbcdb&vjs=3,"We are seeking a highly skilled and motivated Natural Language Processing (NLP) Data Engineer to join our team. The NLP Data Engineer will be responsible for developing and maintaining data pipelines and infrastructure to support NLP-related projects, specifically focused on call center (helpdesk) data, call routing, and call/chat summaries. The ideal candidate will have experience working with NLP technologies and tools, and will be able to apply that knowledge to solve complex data engineering problems.
Responsibilities:

Develop and maintain data pipelines and infrastructure to support NLP-related projects
Build, test, and deploy NLP models and algorithms for call center data
Collaborate with data scientists and software engineers to integrate NLP models into production systems
Design and implement solutions for data storage, retrieval, and analysis
Develop and maintain ETL processes to ingest data from various sources
Monitor and optimize the performance of data pipelines and NLP models
Ensure the security, availability, and reliability of NLP-related data and infrastructure
Understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests.
Understanding of Data Science toolkits, such as R, NumPy, Matlab, SQL. Data Visualization tools.

Requirements:

Bachelor's or Master's degree in Computer Science, Data Science, or related field
3+ years of experience as a Data Engineer
Proficient in Python and SQL
Familiarity with NLP tools and techniques, such as spaCy, NLTK, or Gensim
Experience with distributed computing systems, such as Apache Hadoop and Spark
Strong understanding of data modeling and database design principles
Excellent problem-solving and analytical skills
Ability to work independently and as part of a team
Strong communication and collaboration skills
Ability to support core hours from 7 to 5 Central Time.

ABOUT AD-HOC RESEARCH
Ad-Hoc Research specializes in providing the full spectrum of Systems Engineering services to major DOD acquisition programs and Research & Development projects. Our company believes in inducing innovations through focused research. We are an army veteran owned 8a company with defense contracts in many states. We are also launching a cyber range platform to provide customer support to our army and defense clients. Our data science team supports a major telecom company to develop and operationalize AI/ML models. We have a lot of exciting opportunities to grow and be successful in our fast growing company!
Ad-Hoc Research is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or national origin.
Job Type: Full-time
Benefits:

401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance

Experience level:

3 years

Schedule:

8 hour shift

Ability to commute/relocate:

Plano, TX 75074: Reliably commute or planning to relocate before starting work (Required)

Education:

Bachelor's (Preferred)

Work Location: Hybrid remote in Plano, TX 75074"
Senior Data Engineer,Copart,"Dallas, TX",Posted 21 days ago,Full-time,https://www.indeed.com/rc/clk?jk=6bb3587cb8bccf0a&fccid=12f1fd5dba253774&vjs=3,"The Senior Data Engineer will be part of the Data Services Team. The Data Services team works very closely with all aspects of applications and data pipelines. We are looking for a Senior Data Engineer to design, develop, and optimize the flow of data throughout the organization, enabling end user to provide valuable insights of data across Copart. In this role, your work will broadly influence the company's data consumers, executives and analysts. 


Key Responsibilities:


Design and build the next generation data platform
Develop and automate data processing systems to deliver data insights at an enterprise scale.
Develop logging, metrics, and alerts that enable active monitoring of designed processes.
Collaborate with Product Managers and Application teams to develop data models and schemas that help provide easy access to complex data sets.
Assist in maintaining data integrity in production systems
Ability to balance and prioritize multiple conflicting requirements with high attention to detail.

Qualifications Requirements:


Bachelor's degree or higher in computer science, Engineering or similar
5+ years of experience designing, developing, testing and implementing scalable, high-performing data warehouse and BI solutions
Good hands-on Experience on real-time data pipelines including Kinesis and Kafka, understanding of Database architecture including MPP and ad-hoc analysis using BI/Analytical tools like Tableau, Pentaho, OBIEE or Power BI
Proven ability to analyze complex business problems using data and translate them into actionable insights stemming from data analysis
Experience with ETL and Data Blending and Transformation tools such as Pentaho Data Integrator, Talend Data Integration, Informatica
Enterprise development knowledge and/or experience with databases such as SQL Server, Oracle, MySQL and Columnar databases like Vertica, MemSQL , Netezza , Redshift
Good understanding of technology and industry and able to make decisions on the best technology for integrated solutions
Proven ability to communicate with business and technical audiences at all levels, including demonstrated success influencing senior leaders and decision makers
Technical Skill Required: SQL, Python. BI/Analytical Tool Preferred: Tableau, Pentaho, PowerBI
 For 40 years, Copart has led its industry in innovation and customer service, enabling it to grow profitably in markets across the globe. Our success is the direct result of the skills and efforts of our talented and diverse employees. Our mindset? It's never just a ""job"" when your coworkers are like family - it's like coming home."
Lead Data Engineer,Rubistone Technologies LLC,"Plano, TX 75023",EmployerActive 3 days ago,$45 - $60 an hour,https://www.indeed.com/company/Rubistone-Technologies-LLC/jobs/Data-Engineer-0c833c3daf18efd5?fccid=f165ed06e3907c24&vjs=3,"Lead Data Engineer role (W2 only)Contract Plano, TX ( onsite)
Job Summary :
Understand the business requirement or the downstream process requirement
Provide impact analysis whenever there are any changes and production events happen
Should be strong in Azure, Airflow, CICD, Kubernetes, Spark Scala, Spark SQL, Scripting (Unix Shell, SQL), Python
Monitor throughout the project delivery life cycle in order to manage the scope, schedule, and cost for IT projects
Ensure development guide and best practice for developing PowerCenter solutions
Required Skills : AWS Machine Learning ,Big Data ,Apache Spark
Roles & Responsibilities :This position will be responsible for designing, developing, implementing, and testing conceptual and logical database models in response to an external or internal need.The candidate is a self-motivated individual who can collaborate with a team and across the organization.The candidate is data-centric and should have a track record of handling large scale data projects involving Azure cloud services, ETL, data modeling, data security, and performance tuning.
Job Type: Contract
Salary: $45.00 - $60.00 per hour
Benefits:

Referral program

Experience level:

8 years

Schedule:

8 hour shift
Monday to Friday

Ability to commute/relocate:

Plano, TX 75023: Reliably commute or planning to relocate before starting work (Required)

Work Location: In person"
Senior Data Engineer,Stellar Inc.,"Plano, TX 75024",EmployerActive 2 days ago,$75 an hour,https://www.indeed.com/company/Stellar-Inc./jobs/Data-Engineer-01fddb43f0dc39d7?fccid=8971d13e5511b843&vjs=3,"***This is a hybrid job at below locations : Plano TX, Richmond VA, Mclean VA, SF CA, NY NY***
MUST HAVE SKILLS (Most Important):
9+ years of Experience in designing, building, and deploying enterprise-level large data streaming event based application using Spark, Kafka, Snowflake, Java, HBase 
7+ years of programming experience in java 
Strong experience with AWS and scaled agile based product development methodology is preferred
3+ years of proven experience in collaborating with data architects, data scientists and enterprise platform team in building, deploying and managing models in production
Knowledge on Jenkins, Jira, Git etc.
Responsibilities:

Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.


Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.


Write applications to solve analytical problems.


Code, test, release, and support Big Data.


Help in the design and build of the data platform over Big Data technologies.


Solve big data engineering problems.


Analyze, recommend, and implement data technologies for the platform.


Involved in case studies about Big Data.


Responsible for efficient deliveries.


Work with others to propose the best technical solutions.


Help design the best backend data warehouse platform to support the capacity and performance.


Participate in the proof of concept application

Job Type: Contract
Pay: $75.00 per hour
Schedule:

8 hour shift

Work Location: In person"
Sr Data engineer,MANDO TECHNOLOGIES INC,"Dallas-Fort Worth, TX 75261",Posted 30+ days ago,Contract,https://www.indeed.com/rc/clk?jk=391a252658da941e&fccid=e083889a5e194a39&vjs=3,"Mando Technologies is specializes in helping organizations make the most of their information assets. From acquiring, organizing, analyzing, and delivering data to closing the loop by integrating intelligence into the operations of the enterprise, Mando Technologies covers the full spectrum of Business Intelligence. 
 
Our data engineers are powering the capability to make decisions using data to improve operations and our customer and employee experience.
This role is a part of the Data Engineering and Analytics team with our Client Technology group. You'll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective.

What you'll do:

Be a part of the data privacy and governance team. Work closely with data privacy office, data application teams and product owners to design, implement and support data privacy, governance and analytics solutions that provide insights to make better decisions.
Implement data privacy and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools.
Perform multiple aspects involved in the development lifecycle – design, cloud engineering (Infrastructure, network, security, and administration)Provide technical leadership and collaborate within a team environment as well as work independently.
Implement batch and streaming data pipelines using cloud technologies.
Leads development of coding standards, best practices and privacy and security guidelines.
Mentors' others on technical and domain skills to create multi-functional teams.
Develop and support data privacy and governance related frameworks and help other team implement them for compliance
Experience working in an Agile environment and with Agile teams

Preferred Qualifications:

Minimum Qualifications- Bachelor's degree in Computer Science, Computer Engineering, Technology or related technical discipline
6 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions
7 years data analytics experience using SQL
4 years of cloud development and Data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI


Tools/platforms required:

Programming/Scripting: Python, Spark, Unix, SQL, APIs, Java
Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake
Azure Cloud Technologies: Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions
CI/CD: GitHub, Jenkins, Azure DevOps, Terraform
BI Analytics Tool Stack - Cognos, Tableau, Power BI, and Grafana
Tools: DataStage, Informatica EDC/Axon, BigID"
Data Engineer (Remote),Ad Hoc Team,"Remote in Dallas, TX",Posted 26 days ago,"$101,570 - $136,994 a year",https://www.indeed.com/rc/clk?jk=77ae0e5a3484c2ee&fccid=7707621c92754acd&vjs=3,"This is a fully remote position.
 Work on things that matter Ad Hoc is a digital services company that helps the federal government better serve people. Our teams use modern, agile methods to design and engineer government systems that connect Veterans with services, bring affordable health care to millions of people, and support important programs like Head Start. And as we work to make critical government services intuitive, accessible, and human-centered, we're also changing how the government thinks about and uses technology. If you thrive on change, want to help close the gap between consumer expectations and government services, and can see the possibilities in ambiguity, then we want you here with us.
 What matters most Ad Hoc operates according to our commitment to inclusivity, acceptance, accountability, and humility. We aren't heroes. We believe in missions larger than our individual selves and leave our egos at the door, learn from our mistakes, and iterate in order to better serve the people in our country. We prioritize building teams that represent the diversity of the people our government serves. We love the challenge of government-size projects. We want to bring skills to federal agencies, help them better meet the needs of their users, and close the gap between consumer expectations and government.
 Built for a remote life Ad Hoc is remote-first and remote-always. We've designed our culture, communications, and tools to support a nationwide distributed team since the beginning. Being remote by design allows Ad Hoc to be thoughtful and intentional about creating diverse teams and supporting them with a work environment that fits their lives. With a generous PTO policy and Slack channels for every interest (from bird watching to space nerds to parenting) our culture embraces the things happening in your life. Maybe you need to adjust your schedule to care for your family or take a bike ride. At Ad Hoc, that's embraced.
 What you'll do
 Data Engineers are responsible for working with the systems and infrastructure that enable data storage, processing, and analysis. They work closely with data scientists and analysts to ensure that data is properly collected, organized, enriched, refined, and available for analysis. They are the critical connectors between the teams that maintain existing legacy systems, and the data analysts and data scientists that will use aggregated data for analysis, reporting, and predictive analytics.

Shipping software that impacts the lives of millions of people
Using modern programming languages and frameworks to build scalable services that gracefully integrate with legacy systems
Building and working with APIs to support both the digital services we deliver as well as third-party usage
Helping us continuously, iteratively improve

What we hope you'll bring
 

A minimum of four (4) years of professional software development experience
AWS experience
Understanding of ETL/ELT processes and tooling
Understanding of database technologies - setup/ maintenance / data loads / etc. (not data modeling)
Redshift experience preferred
Understand system security
API design and implementation
GIT and DevOps release process
Python or Scala, Python is preferred for ETL
Some experience with older file systems / file based processes such as MOVEit
Some experience with Mulesoft
Experience with agile software development practices emphasizing agility, flexibility, and iterative development


More than that, our ideal candidate wants to contribute to work that is bigger than themselves and wants to make a difference collaborating with their team. They care deeply about building better products, better relationships, and better trust in each interaction people have with their government. They believe in intuitive, easy-to-use government services. They collaborate well with designers, stakeholders, and other teams. They mentor and guide more junior engineers. They're human-centered.
 And if you don't check every box on the list? That doesn't mean you can't help us in our mission to deliver critical government services. Talk to us!
 Some basic requirements

All work must be conducted within the U.S., excluding U.S. territories. Some federal contracts require U.S. citizenship to be eligible for employment.
You must be legally authorized to work in the U.S now and in the future without sponsorship.
As a government contractor, you may be required to obtain a public trust security clearance.
Bachelor's Degree in a technical field is preferred
4 years of professional software development
Our technical screening involves completing a homework assignment that is then graded blind to remove bias. We do not do tricky, unreliable whiteboarding tests. You can read more about our homework here.

Learn more about engineering at Ad Hoc.
 Benefits

Company-subsidized Health, Dental, and Vision Insurance
Use What You Need Vacation Policy
401K with employer match
Paid parental leave after one year of service

Ad Hoc LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, sex, sexual orientation, gender identity or expression, religion, age, pregnancy, disability, work-related injury, covered veteran status, political ideology, marital status, or any other factor that the law protects from employment discrimination.
 In support of the Colorado Equal Pay Transparency Act, and others like it across the country, Ad Hoc job descriptions feature the starting range we reasonably expect to pay to candidates who would join our team with little to no need for training on the responsibilities we've outlined above. Actual compensation is influenced by a wide range of factors including but not limited to skill set, level of experience, and responsibility. The range of starting pay for this role is $101,570 - $136,994 and information on benefits offered is here. Our recruiters will be happy to answer any questions you may have, and we look forward to learning more about your salary requirements.
 job reference: 2015"
Sr. Data Engineer,Cyber Infotech,"Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=09649a951a1d3567&fccid=2150b2a4695db8ee&vjs=3,"Responsibilities

Serve as a Data Engineer and support data engineering requirements and priorities.
Support tasking, metrics, and updates for the Data Engineering team
Develop code using various programming and scripting languages to automate data ingestion and improve data management processes
Develop automated ingestion pipelines
Ensure Data Engineering Team Standard Operating Procedures are appropriately implemented across the team
Ensure technical correctness, timeliness and quality of deliverables
Demonstrate excellent oral and written communication skills to all levels of management and the customer.

Please forward your resume to contact@cyberinfotek.com"
Data Platform Engineer,Copart,"Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=427362acb2fb26b9&fccid=12f1fd5dba253774&vjs=3,"Copart is looking for a talented Data Platform Engineer to join our DevOps team to support our data, search and messaging platforms and other related technologies, as well as partner with the product team to ensure data integrity in the entire system. An ideal candidate brings curiosity, passion for data, and a deep understanding of the technologies behind data pipelines, big data, analytics, and deep understanding of the software development life cycle. 

 Responsibilities 



Deploy and manage and monitor data platforms and data movement solutions.
Build scalability, fault-tolerance, security, and performance into our data platforms to meet the growing needs of our customers
Partner with agile data development teams and use DevOps methodologies to create automated, efficient CI/CD processes to reduce the time to promote, test and deploy code
Partner with Information Security professionals to ensure data is secure both at-rest and in-flight
Develop, test, deploy and maintain efficient reusable patterns of streaming and batch data ingestion pipeline architectures for our data engineers to use
Document and maintain key architecture and coding standards for supported platforms
Act as first line support for data platforms to maintain peak operating efficiency and ensure maximum uptime
Ensure data platforms remain current to take advantage of the latest features and support
Communicate status of assigned work to management and follow agile practices and procedures covered by precedents, standards, or policies
Seek guidance when direction is needed and speak up about technology risks identified and own addressing them with the team
Influence peers on the team with questions and insight
Be open to learn, train and develop as a team
Experience required minimum 8 years
Extensive Experience with Kafka and related technologies (Hive, Hadoop, Spark, Storm, Zookeeper)
Experience with Solr and related technologies (Solr, Elastic Search, Lucene API, etc.)
Experience with deploying, configuring, scaling, and troubleshooting data and search infrastructure
Experience with alerting, monitoring and auto-remediation in a large-scale distributed environment
Programming experience in Java, Python, or similar languages
Experience in support of stream processing solutions in Big Data and Kafka, Kinesis
Strong Knowledge of messaging/events architecture Concepts and PUB/SUB Pattern
Solid knowledge of Java SE, Java EE, XML, XML Schema, XSD, XSLT/XPath and JSON technologies
Experience in building and deploying Micro Services on containers such as Pivotal Foundry Cloud, Docker, or Kubernetes etc.
Experience with Source control/Bug Tracking/Automated Build tools Jira, Jenkins and Git
Experience with http web servers and load balancers
Experience with Reporting and ETL platforms
 For 40 years, Copart has led its industry in innovation and customer service, enabling it to grow profitably in markets across the globe. Our success is the direct result of the skills and efforts of our talented and diverse employees. Our mindset? It's never just a ""job"" when your coworkers are like family - it's like coming home."
Sr. Data Engineer,laiba Techologies,"Dallas, TX 75201 (Downtown area)",Posted 1 day ago,"$75,664 - $125,306 a year",https://www.indeed.com/company/laiba-Techologies/jobs/Data-Engineer-38b29a9c7a7a25d8?fccid=2a1dc54782681ebb&vjs=3,"Position: Data EngineerLocation: Dallas, TX (Hybrid - 3 Days Onsite, 2 Days Remote)
Duration: Long-term Contract (W2 Contract)
Experience: 8+ Years
Job Description:
Role & Responsibilities:

Data Engineers will be responsible for design, build and maintain data pipelines ensuring data quality, efficient processing, and timely delivery of accurate and trusted data.
The ability to design, implement and optimize large-scale data and analytics solutions on Databricks, Spark, Snowflake Cloud Data Warehouse is essential.
Ensure performance, security, and availability of the data warehouse.
Establish ongoing end-to-end monitoring for the data pipelines.
Strong understanding of full CI/CD lifecycle.

Must Haves:

2+ years of recent experience with Databricks / Spark / Snowflake and a total of 6+ years in data engineering role.
Designing and implementing highly performant data ingestion pipelines from multiple sources using spark and databricks.
Extensive working knowledge of Spark and Databricks
Demonstrable experience designing and implementing modern data warehouse/data lake solutions with an understanding of best practices.
Hands-on development experience with Snowflake data platform including Snowpipes, SnowSQL,tasks, stored procedures, streams, resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, cloning, time travel, data sharing and understanding how to use these features.
Advanced proficiency in writing complex SQL statements and manipulating large structured and semi-structured datasets.
Data loading/Unloading and Data sharing
Strong hands-on Experience on SNOWSQL queries, script preparation and stored procedures and performance tunning
Knowledge of SnowPipe implementation
Create Spark jobs for data transformation and aggregation
Produce unit tests for Spark transformations and helper methods
Security design and implementation on Databricks
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and scalable 'big data' data stores.

Job Type: Full-time
Salary: $75,663.85 - $125,305.74 per year
Experience level:

10 years
8 years
9 years

Schedule:

8 hour shift
Monday to Friday

Ability to commute/relocate:

Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)

Experience:

Python: 1 year (Preferred)
Spark: 1 year (Preferred)
Data warehouse: 1 year (Preferred)

Work Location: In person"
Data Engineer,BuzzClan Private Limited,"Dallas, TX 75240 (Far North area)",Posted 30+ days ago,"Up to $80,000 a year",https://www.indeed.com/company/BuzzClan-Private-Limited/jobs/Data-Engineer-e185e49f149ae1b0?fccid=c37823a15cfb63b6&vjs=3,"-Job Title: Data Engineer /Data AnalystLocation: Dallas, TX
Job OverviewShall apply specialized knowledge in writing high-performance SQLqueries and data engineering solutions that moves, cleans, and loads data to differing systems. Theperson will interact with our clients and differing roles within IT to gain understanding of the businessenvironment, technical context, and strategic direction. This person will also be responsible forgenerating documentation as needed; conform to security and quality standards; and stay current onemerging trends. Teams supported include: Application Engineering, Data Platform Engineering, DataEngineering, Decision Science, and Business AnalyticsA successful candidate for this position, you must:? Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions? Proactively identify and assist in solving recurring data quality or data availability issues.? Have experience in monitoring, troubleshooting, and resolving ETL issues.? Be able to develop high performance data queries, stored procedures and/or functional code for data related ad-hoc reporting and/or ETL batch triage reasons.? Understand how to profile code, queries, programming objects and optimize performance? Aspire to be efficient, thorough, and proactive
Responsibilities and Duties? Monitors, supports, triages data pipelines and ETL tasks that ingest, move, transform, and integrate data in a secure and performant manner.? Prepares necessary SQL scripts to perform data manipulation in key systems to address data related application and reporting needs.? Explores new technologies and data processing methods to increase efficiency, performance, and flexibility to proactively address recurring data related issues.? Document requirements and translate into proper system requirements specifications using high-maturity methods, processes, and tools.? Designs, prepares, and executes unit tests.? Participates in cross-functional teams.? Represents team to clients.? Demonstrates technical leadership and exerts influence outside of immediate team.? Develops innovative team solutions to complex problems.? Contributes to strategic direction for teams.? Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. SSIS, Azure ADF).? Integrates technical expertise and business understanding to create superior solutions for clients.? Consults with team members and other organizations, clients and vendors on complex issues.? Work on Special projects as requested? Performs other duties as assigned
Qualifications? Bachelor’s degree in IT preferred. Equivalent combination of education and experience may be substituted in lieu of degree.? 2 to 4 years of Microsoft SSIS package development experience including experience with Microsoft Visual Studio? 1+ years of experience working with enterprise data warehouses? 5+ years of experience in SQL and be able to write complex logic using SQL as part of ETL, and use SQL effectively to perform complex data analysis and discovery? 1+ years of experience building reports with SSRS.? Exposure to Azure and Azure Data Factory? Exposure to an Enterprise Data Lake? Demonstrate strong organization skills and detail-oriented? Experience with CMD shell and PowerShell? Experience with large-scale, complex data environments? Ability to self-motivate and meet deadlines? Intense desire to learn? Ability to express complex technical concepts effectively, both verbally and in writing? Ability to multi-task in a fast-paced, changing environment? Ability to maintain confidentiality
Job Types: Full-time, Contract, Temporary
Salary: Up to $80,000.00 per year
Benefits:

401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Tuition reimbursement
Vision insurance

Experience level:

4 years

Schedule:

8 hour shift

Ability to commute/relocate:

Dallas, TX 75240: Reliably commute or planning to relocate before starting work (Required)

Experience:

Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)

Work Location: In person"
Test Data Management Engineer,Health Care Service Corporation,"Richardson, TX 75080",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=128135fe075a9a3b&fccid=89564d5901dd5bfb&vjs=3,"This position is responsible for demonstrating a capable understanding of Test Data Management related to data security, data masking, synthetic data creation and test data strategy planning; working with teams to provide test data; creating synthetic data, perform data conditioning, macro creations for data generation, data masking, provisioning for data and mask data as needed; multi-tasking in an environment of changing priorities.


Required Job Qualifications:

Bachelor's degree required or combination of education and 2 years’ experience OR 4 years Information Technology experience


 
Preferred Job Qualifications:

Data Engineering Certification
Experience with Selenium Automation – novice
Python experience – capable
Experience with SQL database queries and programming – capable
Understanding of data masking concepts, with proven implementation experience. Familiarity with data quality, cleaning, and masking techniques -novice
Ability to diagnose diverse technical issues in a complex enterprise environment – capable
Organizational and time management skills, with the ability to manage workload and projects to navigate peaks, prioritize competing commitments, and complete assignments in accordance with established deadlines – capable
Attention to detail, follow-through, and customer focused orientation – capable
Analytical, decision making and problem-solving abilities, with demonstrated troubleshooting and debugging skills – capable
Ability to work independently, as well as collaboratively in a team environment – capable
Verbal and written communication, presentation, and interpersonal skills – capable
TDM functions including Data provisioning, sub setting, profiling, Data mining
Abilities with TDM masking tools
Macro generation for synthetic data creation
Scripting experience with Shell, Perl, .bat, VB Script, or any other scripting language experience

 BCBSTX complies with applicable Federal civil rights laws and does not discriminate on the basis of race, color, national origin, age, disability, or sex.
  XJ6"
Data Engineer Lead,Webyops Inc,"Irving, TX 75038 (Cottonwood area)",EmployerActive Today,$40 - $50 an hour,https://www.indeed.com/company/Webyops-Inc/jobs/Data-Engineer-94c2211aa7a5ea40?fccid=85163f7c94ac6220&vjs=3,"Job Profile: Data Engineer Lead
Local only
VISA: USC/GC/GC EAD/ H4 EAD/TN/ H1B
Responsibilities

Lead the Azure Data Ingestion and Integration activities from onshore for the on-off team of around 8 people
Collaborate with the Client’s Data Architects and Lead Data Engineers in data pipeline design
Co-ordinate across the data engineering workstreams happening in parallel including reviews of work from the team
Help PM/Scrum Master with technical effort estimations

Skills

3+ years’ experience with Azure data services including Azure Data Factory, Azure Synapse Analytics, Azure Data Lake Storage
6+ years of SQL scripting and database experience
5+ years of ETL experience with any ETL tool
Good understanding of data warehouse design principles
Agile DevOps working knowledge desired

Job Types: Full-time, Part-time, Contract
Pay: $40.00 - $50.00 per hour
Schedule:

8 hour shift

Experience:

Data Engineer: 2 years (Preferred)

Work Location: In person"
CRM data Analytics Engineer,NLB Technology Services,"Dallas, TX",Posted 30+ days ago,$65 - $75 an hour,https://www.indeed.com/company/NLB-Technology-Services/jobs/Data-Engineer-c39cf01a93b98fd8?fccid=b3fc027da382dafc&vjs=3,"MUST HAVE:
o Tableau CRMA & Einstein Discovery Certification
o Experience building Dashboards using CRMA (aka Tableau CRM and Einstein Analytics)
o Design, build, and support datasets, data recipes, dataflows, and dashboards using Salesforce and non-Salesforce data sources
o Troubleshoot functional, data, and performance issues with CRMA Dashboards
o Skilled in SAQL, JSON
o Develop and deploy AI/ML solutions using Einstein Discovery and deploy AI/ML Models built outside Salesforce
o Experience working with large data (1 B+ rows) and supporting large user base (10,000+)
o Mentor and coordinate the development efforts of other CRMA DevelopersAT&T
Job Type: Contract
Salary: $65.00 - $75.00 per hour
Benefits:

401(k)
Dental insurance
Health insurance
Vision insurance

Experience level:

8 years

Schedule:

8 hour shift

Ability to commute/relocate:

Dallas, TX: Reliably commute or planning to relocate before starting work (Required)

Experience:

CRM ANalytics: 8 years (Required)
einstein: 1 year (Required)
Salesforce: 1 year (Required)

Work Location: In person"
Big Data Engineer -AWS,CEDENT,"Plano, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=281a0358b74cd423&fccid=2ecf4575019bf07a&vjs=3,"Title: Big Data Engineer@ Plano, TX
  Terms of Hire: Full Time.
  Salary: $Open
 

Job description
:


Use modern tools, techniques, and methods to develop, modify, or update applications used by business units or infrastructure units. Lead, or play lead technical role in development teams' efforts to determine unit needs and business processes that are automated by the application. Assess high level design issues relating to platform, enterprise software, and interactions with other systems.


Key Job Functions
:


•Support the team in the writing of deployment scripts and place strong emphasis in automated deployment, infrastructure automation solutions, and continuous delivery process.


•Work with product owners and other development team members to determine new features and user stories needed in large/complex development projects


•Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.


•Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.


•Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.


•Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies.


•Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives


•Mentor or provide technical guidance to less experienced staff; may use high end development tools to assist or facilitate development process.


•Leverage Client DevOps tool stack to build, inspect, deploy, test and promote new or updated features.


•May serve as technical lead, architect, project lead or principle developer in course of large or complex project.


•Expert proficiency in unit testing as well as coding in 1-2 languages (e.g. Java, etc).


•Expert proficiency in Object Oriented Design (OOD) and analysis.


•Expert proficiency in application of analysis/design engineering functions.


•Expert proficiency in application of non-functional software qualities such as resiliency, maintainability, etc.


•Expert proficiency in advanced behavior-driven testing techniques.


•Provide expertise for teams in all matters related to deployment, building and release process.


Education
:


Bachelor’s degree / Master or Other Advanced Degree


Certifications Preferred :


Specialized training in specific platforms, enterprise or development tools


Minimum Experience 
:


• 8-10 years of related experience; Highly experienced with Agile practices/methodologies (e.g. Scrum, TDD, BDD, etc).


Specialized Knowledge and Skills
:


•Highly experienced in the use continuous integration tools (e.g. Jenkins, Hudson, etc) and infrastructure automation (VM Ware, Puppet, Chef, Vagrant, Docker, etc).


•Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.


•Strong analytic skills related to working with unstructured datasets


•A successful history of manipulating, processing and extracting value from large disconnected datasets


•Build the infrastructure required to process data from a variety of data sources using SQL.


•Create data tools for analytics and data scientists to optimize data


•Experience working with either a Map Reduce or an MPP system on any size/scale


•Experience with big data tools: Hadoop, Spark, Kafka, etc.


•5+ years of Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, et


What are the 3-4 non-negotiable requirements on this position?
:


1. Python/Java 2. Big Data Tools (Hadoop, Spark, Kafka) 3. AWS

 Preferred Skills: 
 

• AWS cloud services: EC2, EMR, RDS, Redshift


•SQL experience and No-SQL experience is a plus

You Will Enjoy: 
 

An opportunity to be a part of a great culture, an awesome team, a challenging work environment, and some fun along the way!


Apply today to learn more and be part of our Growth story.

All applications will be kept strictly confidential and once shortlisted, our team will be in touch with you for further discussions."
Sr Cloud Data Engineer,"IT Engagements,Inc.","Dallas, TX",EmployerActive 28 days ago,From $65 an hour,https://www.indeed.com/company/IT-Engagements/jobs/Cloud-Engineer-a9b452b4337134bd?fccid=0a97ba15ee10e561&vjs=3,"Greeting from IT Engagements…!
IT Engagements is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. We have an immediate opening for the below position with our premium clients.
Job:- Sr Cloud Data Engineer
Location:- McLean VA or Dallas, TX (Onsite)Duration:- 6+ Months Contract to hire
Exp: 8 years+
Job Description
Must Haves:

Building data lake platform in AWS (Glue, DynamoDB S3, EMR)
Python
Scala
Understand data engineering, data governance, data lineage
Linux Shell Scripting

Really Nice to Have:

Snowflake
Data warehousing
Kubernetes
Hadoop infrastructure

Requirements:

Experience with Big Data technologies such as Hadoop/Hive/Spark specific to AWS related services is a plus.
Expertise in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.
Sound knowledge of distributed systems and data architecture - design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.
5+ years of work experience with building Data Pipelines, Data Processing, Data Modeling, and Data Architecture.
Experience operating very large data warehouses or data lakes. (Snowflake, etc)
Excellent skills in writing and optimizing SQL.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Knowledge of IT, service-oriented architectures, software development life cycles, or information security platforms and applications.
Minimum 5+ years of experience in software development.
3+ years of related industry experience in an enterprise environment.
5+ years of data engineering experience.
Scala / Python, pySpark(Boto, Boto3, etc.)/ Spark experience.
Delta lake, delta table and lakehouse architecture
Datewarehouse Experience (Snowflake)
Experience with lambda, EMR, SQS, DynamoDB, Glue, Stepfunctions, etc.
Linux and shell scripting.

Knowledge of:

Kubernetes.
Formal design patterns and industry best-practices.
2+ years of experience with requirements, design, implementation, integration, and testing for data and analytics integration.
2+ years of experience across a variety of technologies such databases, directory services, application servers, network infrastructures, Linux operating systems, and an understanding of fundamental security and data flows within these components.
Excellent verbal and written communication skills.
Self-motivated, driven, and creative individual.
Scaling systems and microservices.
Familiarity with CI/CD processes
Code coverage analysis / static analysis tools.
Agile programming processes and methodologies such as Scrum.
Scheduling tools like Autosys , ControlM.
Informatica IICS , Talend

Thankyou
Divya Kumari
Technical Recruiter
divya(at)itengagements(dot)com
Job Types: Full-time, Contract
Pay: From $65.00 per hour
Experience level:

8 years

Schedule:

8 hour shift

Experience:

Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)

Work Location: On the road"
IT Data Engineer,Kelly-Moore Paint Company,"Irving, TX 75061 (Bear Creek area)",Posted 6 days ago,Full-time,https://www.indeed.com/rc/clk?jk=60bef0415f729d12&fccid=87b27d42aa007773&vjs=3,"Kelly-Moore's mission is to provide high quality, innovative products with exceptional service at fair value. What sets Kelly-Moore Paint Company apart from the competitors, is not just premium products but also our amazing team! 


FULL TIME - Location: Las Colinas, TX 

 OVERVIEW 
Kelly-Moore Paint Company’s Information Technology department focuses on delivering effective solutions for employees, customers, and vendors. IT is comprised of talented individuals who cohesively drive effective solution strategies within a balanced ecosystem of systems and support. The Data & GRC Team is focused on fulfilling the business needs for data owners and consumers. Providing data governance guidance and meeting regulatory requirements. 
PRIMARY RESPONSIBILITIES 


Provide day-to-day data processing development and support of existing data automation.
Manage and support the enterprise master data model, as well as inbound and outbound integrations.
Assist in the development of master data model architecture.
As needed develop, implement, and document new data integrations.
Develop data integrations using APIs and cloud native technologies.
Develop test plans and perform testing and certification of new and/or updated integrations.
Perform and document change management in line with ITIL and SDCL best practices.
Proactively fulfill project tasks with minimal supervision and raise any project gaps discovered.
Work closely IT and business team members to automate processes.
Act as subject matter expert on data integrations and data schema.
Support business intelligence projects, working collaboratively with Tableau team.
Perform all data handling in compliance with applicable regulatory requirements.
Maintain knowledge and users training documents relevant to data access, authorization, automation, and control.
Develop new projects as business need arise and document business requirements.
Perform other duties as assigned.

QUALIFICATIONS:


3+ years of progressive experience data processing and/or data integration. Bachelor’s degree in information technology, data science, or equivalent experience.
Basic level knowledge of scripting languages.
Intermediate level knowledge of transact SQL or equivalent structured query language.
Knowledge of on-premises and cloud-based data processing and database systems.
Azure data lake/factory experience preferred.
Strong English writing and communication skills in a professional setting.
Ability to collaborate effectively with cross functional teams.
Self-motivated with the ability to work autonomously, effectively prioritize and manage multiple concurrent tasks and projects.

REQUIREMENTS:


Experience in database management and business process automation.
Database management or data science certifications or equivalent experience.
CompTIA Data+ or Microsoft Certified: Azure Data Engineer Associate a plus.
ITIL Foundation certification a plus.
Perform duties on site in Las Colinas 3 days per work week.
 Kelly-Moore provides equal opportunity in all terms and conditions of employment. We will not discriminate against qualified applicants or employees with respect to any terms or conditions of employment because of any basis protected by federal, state, local law including race, color, religion, religious dress or grooming practice, national origin, sex, age, marital status, physical and mental disability, medical condition, veteran status, sexual orientation, gender identity, gender expression, and genetic information."
Senior Data Engineer,Rojan USA Incorporated,"Frisco, TX 75034",EmployerActive 3 days ago,"From $140,000 a year",https://www.indeed.com/company/Rojan-USA-Incorporated/jobs/Data-Engineer-ea68f13063dffa39?fccid=a27c63c1b16739fb&vjs=3,"We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, your role will involve monitoring player data and the in-game economy, enabling real-time insights and optimizations. You will play a crucial role in designing, implementing, and maintaining our data infrastructure, ensuring the efficient processing and analysis of large-scale data sets. You will be assisting with the design of scalable and reliable data pipelines, optimizing data models, and staying up-to-date with the latest data engineering technologies.
Responsibilities:
Assisting with the design of scalable and reliable data pipelines and ETL processes to collect, transform, and load data from various sources into our data warehouse.
Continuously monitor and troubleshoot data quality issues, perform data validation, and implement data cleaning techniques.
Responsible for designing schemas to optimize data storage and retrieval, ensuring efficient and effective database structures.
Create and manage indexes to improve query performance and enhance database search capabilities.
Develop and maintain a scalable and robust database architecture, ensuring data integrity, security, and performance.
Qualifications:
Proven experience as a Data Engineer, with at least 5 years of experience in data engineering or a similar role.
Strong proficiency in programming languages such as Python, and experience with Microsoft SQL Server.
Extensive knowledge of data modeling, data warehousing, and ETL techniques.
Solid understanding of data governance, data privacy, and security practices.
Strong problem-solving skills and ability to optimize data processing workflows for performance and scalability.
Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.
Preferred Skills:
Experience with Grafana Dashboards
Experience with Power BI
Lua coding experience
Eligibility:
Due to the nature of our work environments, we can only consider US Green Card Holders or Citizens at this time. Thank you for your understanding.
Job Type: Full-time
Pay: From $140,000.00 per year
Benefits:

401(k)
Paid time off

Compensation package:

Yearly pay

Experience level:

5 years

Schedule:

8 hour shift
Overtime
Weekends as needed

Ability to commute/relocate:

Frisco, TX 75034: Reliably commute or planning to relocate before starting work (Required)

Experience:

Informatica: 5 years (Required)
SQL: 5 years (Required)
Data warehouse: 5 years (Required)
Data Engineer: 5 years (Required)

Work Location: In person"
"Manager, Data Engineer & ETL Processing",Spark Foundry,"Dallas, TX 75201 (Downtown area)",Posted 6 days ago,Full-time,https://www.indeed.com/rc/clk?jk=aa1e5eaa5fd971d2&fccid=65e65a4212c7f0fe&vjs=3,"Company Description
  About Spark Foundry:
 Spark Foundry is a global media agency that exists to bring HEAT – Higher Engagement, Affinity, and Transactions – to brands. By combining flawless media fundamentals with aggressive innovation, Spark inspires consumers to pay more attention, to care more about our clients’ brands, and to buy more products and services from them.
 Balancing the nimble spirit of a startup with the powerhouse soul of Publicis Media, Spark Foundry delivers the best of both worlds to a client roster that spans some of the world’s best and most beloved brands and companies. We combine boutique-caliber insights and service with the buying clout and first-look access of a global leader, bringing the heat to challenger brands that want to act like giants, and to giant brands that want to act like challengers.
 With a bottom-up culture that celebrates diversity and aims for all voices to be heard, Spark has become a magnet for the industry’s best talent, with one of the best retention rates in the industry. And by applying a whole-person approach to professional and personal development, Spark develops a workforce that is well prepared for today’s challenges, and also poised to create meaningful careers in the years to come.
 Because we know that heat arises the intersection of complementary forces, our professionals come from myriad disciplines and backgrounds: data, analytics, and insights, content and creative production, communications and strategy, finance and marketing, and sociology, psychology, and other liberal arts disciplines.



 Job Description
  Overview: 
The Manager, Data Engineering & ETL Development is a key driver to build data platform leveraging best in class ETL practice and is also a strategic thinker and a talented data visualization expert with emphasis on dashboard reporting. The primary responsibility will be developing business intelligence platform to support media and marketing data for our clients. This position will allow you to be a significant contributor as part of the team to support easy access of data and visualizations to fuel stronger insights and media optimizations.
 This position requires strong technical and tactical skill sets with an eye for numbers, intellectual curiosity, proficiency at problem solving, and a critical understanding of online media. The candidate must have a proven track record in managing ETL’s, APIs and business intelligence platforms. Candidate should be a team player. A “roll up the sleeves” approach is mandatory and a “get it done” attitude is a must. Specific responsibilities include coordination between the research, analytics & media teams ensuring high quality data projects are effectively delivered.
 Successful candidates will be multi-dimensional ‘rising-stars’ who are able to employ complex problem solving skills, and are able to communicate these succinctly to a broad client and media stakeholder audience.
 Role Objectives:

 Responsible for loading and validating data into the data warehouse from various source systems
 Analyze, develop, fix, test, review and deploy functionality, and bug fixes in ETL data pipelines
 Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required
 Building Data mappings between Source to Target systems
 Provides support for technical issues and ensuring system availability
 Work with business customers to identify and develop additional data and reporting needs
 Understand how business intelligence platform/data technologies work and offer the ability to explain technical concepts in ordinary terms (be technically savvy - understand the opportunities and limitations)
 Establish and manage data integrations utilizing a taxonomy nomenclature to make recommendations for data visualization to showcase media performance through the use of Datorama or Tableau
 Perform regular quality assurance/quality control checks on assigned client campaigns to ensure the data is processing accurately
 Design and build backend data streams and processes to automate reporting capability with data visualization tools
 Contribute to client status and reporting calls, including presentation of reporting as required
 Develop subject matter expertise in ETL, API development, and Business Intelligence platforms
 Clearly define project deliverables, timelines, and dependencies for junior team members, internal stakeholders and clients
 Collaborate on an inclusive team, where members openly communicate and collectively problem-solve
 Strong ability to evaluate new technologies and present findings to team
 Contribute to knowledge sharing efforts and mentorships
 Complete other duties as assigned.




 Qualifications
 

 Bachelor’s degree or combination of education, and equivalent work experience is preferred in the field of computer science, management information systems or Information Technology
 3+ years’ related experience ETL development and business intelligence platform management
 Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
 3+ plus years’ development experience with database engines including Presto, Mongo db and Hadoop.
 3+ years’ experience in ETL development, Strong Database (Modeling, SQL), SQL Server, Hadoop, AWS Redshift, Qubole.
 Experience managing team of 2 or more associates/analysts preferred
 Strong database modeling and SQL skills. Ability to write complex SQL statement, Procedures and data automation programs
 Knowledge of ETL tools like Airflow, AWS Glue, Alteryx, SSIS, Qubole/Spark is a must
 Knowledge of Python or similar programming languages required
 Proficiency with Datorama, Tableau, or other data visualization tools is preferred
 Experience working with AWS or other cloud technologies
 Advanced user Microsoft Office Suite and reporting tools
 Demonstrated expertise in core MS Excel functions (vlookup, pivot tables, data visualization)
 Experience in designing jobs that can be easily promoted from one (Dev) environment to another (Test or Prod) seamlessly, without modification.
 Strong analytical and problem-solving skills
 Strong verbal/written communication and interpersonal skills is required
 Self-starter with ability to thrive in a fast-paced environment and able to function independently while providing status updates to a team of analysts
 Cooperative, flexible, conscientious, dependable, resourceful, self-motivated, and team-oriented
 Problem solving, time management, and critical thinking skills with a professional and positive attitude
 Ability to work independently and as part of an agile team, participating in daily stand-ups, sprint planning and sprint review


 Character: 
The following qualities help drive success as member of the Spark Data and Analytics team:

 Entrepreneurial, engaging, resourceful, curious, and self-directed spirit
 Willing and easily roll sleeves up or down; love the nitty-gritty and the strategy
 Collaborative approach to building cohesive, strong teams
 Loving and living the intersections between brands, people, media, communications
 Relentlessly passionate and resolute
 Planning and time management excellence.
 Embrace challenges
 Proactive, especially in pushing for new opportunities, approaches, and ideas.
 Keenly focused on action and solutions; thrives with deep critical thinking and analysis.
 Pioneering insight attitude and research in-the-know.
 Resourcefulness, flexibility and adaptability, strong ability to pivot when the need arises.
 Inspired to be part of the insight journey/revolution with a growing, dedicated team

 Additional Information
  All your information will be kept confidential according to EEO guidelines.
 23-2795"
Senior Data Engineer,Unleashed Brands,"Bedford, TX 76022",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=197ae5e4d92eef76&fccid=2ca71b2af21642db&vjs=3,"SENIOR DATA ENGINEER




Role Summary:


   Unleashed Brands is seeking a 
   Senior Data Engineer to join a team using cutting edge data technology to solve business, operations, and analytical challenges. This individual must be highly experienced, detail-oriented, and incredibly organized to gather requirements, architect solutions, implement deliverables while mixing in regular maintenance, troubleshooting, and performance tuning of existing systems. To ensure success, you should be an excellent problem-solver and communicator that is fluent in most data manipulation languages with extensive knowledge, sound technical skills and administrative aptitude.
  


This is a full-time position (not C2C or C2H) based in Bedford, TX



Duties / Responsibilities:


Collaborate in creating data architectures and support mechanisms.
Creating ETL and ELT processes.
Regularly liaising with IT project managers, database programmers, and data scientists.
Designing databases with both front-end and back-end users in mind.
Instituting/maintaining regular database maintenance plans.
Monitoring system performance and identifying problems that arise.
Regular performance tuning.
Responding in a timely manner reported errors.
Protecting the database against threats or unauthorized access.
Ensuring that the database is adequately backed up and able to be recovered in the event of memory loss.
Reporting on metrics regarding usage and performance.
Suggesting changes and improvements for database maintenance or protection.
Performs other duties as assigned.


Job Skills & Qualifications:


A bachelor's degree in Computer Science or related field or equivalent experience
A minimum of 5-years’ experience as a database administrator
Extensive experience with Microsoft SQL Server, Azure SQL, and other Azure technologies
Experience with Azure Data Factory
Experience with Azure Synapse a plus
Proficient with PowerShell and Azure CLI
Advanced certification as an MCDBA is advantageous
Expert T-SQL abilities
Experience with Power BI
Analytical mindset and good problem-solving skills
Excellent verbal and written communication skills
Self-Starter
Highly organized and responsible
Capable of working under pressure
Up-to-date with trends and developments in database administration


Physical Requirements:


Prolonged periods of sitting at a desk and working on a computer.



Perks:


Paid semi-monthly
Company Paid Holidays
Flexible Paid Time Off
Multiple health care insurance plans that cover medical, dental, prescription, vision, and employer HSA contributions
Competitive 401(k) Program with employer matching contributions
Daily dress code of “business casual”
A positive work environment
And much, much more





Company Description


    Unleashed Brands was founded to curate and grow a portfolio of the most innovative and profitable brands that help kids learn, play and grow. Over the last 10 years, the team at Unleashed Brands has built a proven platform and know-how for scaling businesses focused on serving families. Its mission is to impact the lives of every kid by providing fun, engaging and inspiring experiences that help them become who they are destined to be. Unleashed Brands has more than 350 locations open and in development with plans to open more than 100+ new units annually over the next 5 years. For more information, please visit www.unleashedbrands.com. Unleashed Brands is headquartered in the Dallas/Fort Worth Metroplex.
   




Commitment to Equal Opportunity


      Unleashed Brands is committed to diversity in its workforce and is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law.
     



     #UBHQ"
Sr. Data Engineer,AXS,"Dallas, TX 75226 (Deep Ellum area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=63bf256d6bb74824&fccid=7ae5de8f02a3e815&vjs=3,"AXS connects fans with the artists and teams they love. Each year we sell millions of tickets to thousands of incredible events – from concerts and festivals to sports and theater – at some of the most iconic venues in the world. Since our founding in 2011, we've consistently pushed the industry forward and improved experiences for fans, making it easier than ever to discover events, find the perfect seats, and enjoy unforgettable live entertainment, and we continue to lead the evolution of our industry today.


 We're passionate about improving the fan experience and providing game-changing solutions for our clients, and we're always looking for smart, motivated people to help make it happen. Bring your enthusiasm, your big ideas, and your desire to team up with some of the best and brightest in technology and entertainment. Together we keep the world cheering.


 The Role


 AXS is seeking a Senior Data Engineer to join our global data engineering team. The ideal candidate enjoys and thrives in working with a variety of data engineering and analytics tools, systems and architectures while working with our product, leadership, and marketing teams across three continents to build high quality, scalable batch and real time data pipelines integrated with our products.


 What You'll Do

Design, implement and maintain modern ETL pipelines.
Support existing processes and frameworks
Define data models that fulfill business requirements
Process, cleanse, and verify the integrity of data used for analysis
Prepare development estimates and design documents.
Successfully adopt the best practice and industry standards in data engineering and analytics fields



 What to Bring

4-6 years of data engineering experience
3 years' experience with Python and writing SQL
Experience with Streaming data technologies such as Kinesis, Kafka
Experience with AWS, Snowflake, RedShift, Airflow, DBT, and Streamset
Experience in building both relational and dimensional data modeling.
Excellent communication skills to share findings in an understandable and actionable manner
BS. or M.S. in Computer Science or other related fields preferred

Nice to Have

Proven ability to work independently in an Agile environment
Experience working with Agile Scrum and Sprints.
Experience mentoring junior engineers or working with consultants



 What's in it for You?

The opportunity to make a real impact.
Extraordinary People – we're not kidding!
Meaningful Mission - Helping revolutionize an industry and deliver better experiences for fans and clients around the world.
Opportunities for learning and leveling up through training and education reimbursement.



 Curious about the typical interview process for this position? Here's what to expect:

Stage 1: 20-min Recruiter Video interview
Stage 2: Hiring Manager Video Interview
Stage 3: Four 30-min Technical Video Interviews
Stage 4: 30-min Executive Video Interview



 Why AXS? AXS, a subsidiary of AEG, sells millions of tickets every year for 500+ partners across North America, Europe, and Asia, from venues like the O2 in London and the Red Rocks Amphitheatre in Denver to teams like the Los Angeles Clippers, Minnesota Timberwolves, and Houston Rockets. Headquartered in Downtown Los Angeles, California, AXS employs more than 500 professionals in multiple locations worldwide, including Charlotte, Cleveland, Dallas, Denver, Las Vegas, London, Tempe, Stockholm, and Tokyo. At each location you'll find a team of dedicated, diverse employees (we've dubbed ourselves ""Fanatix"") who create groundbreaking products and services in a fun, fast-paced environment.
 To learn more about our culture and values, visit: https://solutions.axs.com/careers/


 More about AEG
 For more than 20 years, AEG has played a pivotal role in transforming sports and live entertainment. Annually, we host more than 160 million guests, promote more than 10,000 shows and present more than 22,000 events around the world. We are committed to innovation, artistry, and community, and leverage the power of our 300+ venues, leading sports franchises, marquee music brands, integrated entertainment districts, premier ticketing platform and global sponsorship activations, to create memorable moments that give the world reason to cheer.  Our business is interwoven with the human mind and heart, and we strive to build a diverse and inclusive company that reflects the artists, athletes, and fans that we host; reach beyond traditional boundaries to support the communities in which we operate; and minimize our impact on the environment by adopting sustainable practices throughout our business operations.


 We're an equal opportunity employer and never discriminate based on gender, age, race, religion, color, national origin, sexual orientation, marital status, veteran status, or disability status. We are dedicated to a diverse, inclusive and authentic workplace, so if you're excited about this role but can't ""check every box"" in the job description, we encourage you to apply anyway. You may be the right candidate for this or other roles."
Senior NetSuite Data Engineer,Soligent,"Remote in Arlington, TX",Posted 2 days ago,,https://www.indeed.com/rc/clk?jk=c22b368ed788392c&fccid=0701ce89311f75bf&vjs=3,"Soligent is the largest pure play solar distributor in the Americas. At Soligent, we envision a world where solar isn't just the cleanest choice for power: it's the obvious choice. Our mission is to empower homeowners and installers with a tool-set that transforms the way the world produces energy. We do this by hiring talented people, empowering our team, and being mission driven. Soligent has been featured by the White House, Forbes, Wall Street Journal, The World Economic Forum, and the Atlantic among other publications. In this role, you will see and participate in the interworking of a high growth entrepreneurial large company in a sector that changes the world.
 Why Soligent?

We are the largest business of its kind in the world. With 5,000 customers in 45 countries, Soligent is an established industry leader in solar with the fast-paced and entrepreneurial spirit of a startup.


You will be part of a vital team at the forefront of transforming how people use energy. Drive positive change in the world while working with the best and brightest. Our executives have built and led successful billion-dollar companies.


We invest in you. You will learn the ins and outs of solar, be exposed to roles across the company, and receive quality training.

Why You?

Are you mission driven? At Soligent, solar is more than a job; it's our future.


Do you see solutions where others see problems? You're not satisfied with the status quo; you constantly think about how to improve processes.


Do you enjoy getting your hands dirty? You understand the big picture but also have an innate ability to drill down and execute with high attention to detail.


Can you execute individually but enjoy working with a team? We support a strong culture of teamwork where it's about the collective gain. Feedback is a big part of that.


Does your word mean everything, you take responsibility for outcomes, and are incredibly persistent? We have a culture of integrity and accountability.


Do you connect the dots easily? You can see how your role fits in with the organization.


Are you fun to be around and able to wear many hats? Soligent is a fast-paced entrepreneurial culture. We believe saving the world should be fun.



 As Soligent, the leading pure-play solar distributor in the US, continues to grow rapidly, we're seeking a highly skilled Senior NetSuite Data Engineer to build the foundational datasets and pipelines for robust financial, supply chain and operations reporting.
 You'll be at the forefront of integrating our product, financial, and business systems to create rock solid processes that will propel us forward. If you're passionate about analytics use cases, data models, and solving complex data problems, then we want you on our team.
 The perfect candidate will bring expertise in SuiteQL, SuiteAnalytics, and the forward-thinking NetSuite2 schema to refine our NetSuite data infrastructure. Significant to this role is experience in crafting robust datasets for Microsoft's Power BI, driving business intelligence. Additionally, the ideal candidate will have hands-on experience with Fivetran and DBT for creating scalable and trustworthy data pipelines. Understanding the software development lifecycle and having a knack for building and leading data teams will be an added advantage. This role requires proficiency with AWS Redshift, and familiarity with Microsoft's Azure Synapse Analytics is considered advantageous. Mastery of SQL, with an intimate understanding of aggregation functions, window functions, UDFs, self-joins, partitioning, and clustering approaches for optimal query performance, is a key requirement for this role.
 Responsibilities:

Innovate, construct, and implement custom objects, and new functionalities in line with dynamic business needs using SuiteQL and SuiteAnalytics within the NetSuite2 schema.
Design and create data structures, schemas, and data models to facilitate efficient data exchange among various business systems.
Develop and maintain datasets for usage with Microsoft's Power BI, delivering strategic insights for business decision-making.
Collaborate proactively with the Business Intelligence team to translate business requirements into technical specifications.
Establish and adhere to quality assurance practices and procedures, performing detailed testing of data pipelines.
Construct scalable and dependable data pipelines using Fivetran to bolster robust reporting across finance, sales, supply chain, and operations.
Regularly analyze our NetSuite environment for potential enhancements and efficiency upgrades.
Spearhead troubleshooting initiatives and determine root causes of issues to ensure their resolution.

Qualifications:

BS or MS in Computer Science, Information Systems, or a similar field.
A minimum of 5 years of experience as a NetSuite Data Engineer or in a similar role.
SQL expert with a deep understanding of aggregation functions, window functions, UDFs, self-joins, partitioning, and clustering for high-performing queries.
Advanced knowledge of NetSuite's SuiteScript, SuiteQL, and SuiteAnalytics.
Comprehensive understanding and experience with the new NetSuite2 schema.
Documented experience with Microsoft's Power BI and business intelligence data modeling.
Hands-on experience with Fivetran for building data pipelines.
Proficiency with DBT is highly desirable.
Must have experience with AWS Redshift.
Familiarity with Microsoft's Azure Synapse Analytics is considered a plus.
In-depth understanding of the software development lifecycle.
Experience building and leading data teams is a significant advantage.
Profound knowledge of ETL processes, data management, and data warehouse principles.
Proven capability to troubleshoot and resolve complex technical issues.Excellent interpersonal skills with the ability to collaborate effectively in cross-functional teams.
Certified NetSuite Developer or equivalent professional experience is a plus.
Perform other duties as assigned



 Benefits

Medical, Dental, Vision, 401K, HSA, FSA, Flexible PTO
 California Privacy Protection Notice"
Data Engineer (Santander),ITKAWA,"Dallas, TX (Downtown area)",EmployerActive 2 days ago,"$50,000 - $105,000 a year",https://www.indeed.com/company/ITKAWA/jobs/Data-Engineer-52ce46b654bc4f19?fccid=290d9295bb6c78a8&vjs=3,"Data Engineer (Santander)
Location : Dallas, TX, USAConditions : Hybrid position - 2 days a week on-site at 1601 Elm Street in Dallas.VISA : Only USC or GC holder will be consideredSalaries : Open to Negotiate , depend on the posicion and level of Seniority could go from 50,000 to 105,000 USA$ year + BenefitsDuration: Possibility of extension or FTEMUST : Previous Experience on finance, banking is a PLUS
Responsibilities:

Establish Data Engineering architecture strategy, best practices, standards, and roadmap
Experience developing ETL Pipeline using Python, Snowflake and IDMC.
Experience with loading batch data and streaming data via Kafka
Build Data Flows mapping Source systems and Process flows.
Assemble large, complex data sets that meet non-functional and functional business requirements
Mentor team members on best practices, efficient implementations and delivering high quality data products
Lead onshore and offshore teams
Perform code reviews and assist developers in optimization and troubleshooting.
Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features
Knowledge in AWS and management technologies such as S3

Must Haves:

5-10 years of experience within data engineering
Python experience
Snowflake experience (developing ETL pipelines)
IDMC (intelligent data management cloud) experience
Knowledge of AWS

Plusses: Kafka experience (loading batch and streaming data)
Location: Candidates must be local to the Dallas area and open to going on-site - not open to seeing non-local and/or remote candidates.
Interview Process: 2 interviews to hire
Job Type: Full-time
Salary: $50,000.00 - $105,000.00 per year
Benefits:

Health insurance

Compensation package:

Bonus pay

Experience level:

5 years

Schedule:

Monday to Friday

Experience:

ETL: 6 years (Preferred)
IDMC (intelligent data management cloud): 5 years (Preferred)
AWS: 5 years (Preferred)
Python: 6 years (Preferred)
Snowflake: 6 years (Preferred)

Work Location: In person"
Big Data Engineer,Alt Shift USA,"Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=a7e7b7752cea9bcc&fccid=35a5820b212db4cc&vjs=3,"Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL
Position 1: Big Data Engineer
Location: Dallas TX
Experience Required - 6+ years

Hadoop/HDFS.

Spark is a must. Scala preferred but Java is ok too.
Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.

Position 2: Sr. Big Data Engineer
Experience Required - 8-15 years
Location: Tampa, FL
Job Description

The client is looking for a senior engineer who can drive things rather than being managed by the client.
 Hadoop/Hive and Kafka.

Spark streaming using Java is a must.

Position 3: Lead Big Data Engineer
Location: Dallas TX
Experience Required - 8-15 years
Job Description

Primary requirement - Spark, Scala developer with knowledge of Kafka.
 Good to have exposure to other NoSQL databases like Casandra.
 AWS experience will be a definite plus."
Senior Data Engineer (Databricks),Apixio,"Hybrid remote in Dallas, TX",Posted 30+ days ago,"$120,000 - $140,000 a year",https://www.indeed.com/rc/clk?jk=5506ce64b8f95c07&fccid=d709010bf50f2482&vjs=3,"Who We Are:
 Apixio is advancing healthcare with data-driven intelligence and analytics. Our Artificial Intelligence platform gives organizations across the healthcare spectrum the power to mine clinical information at scale, creating novel insights that will change the way healthcare is measured, care is delivered, and discoveries are made.



Who you are:

 Experience designing and developing data engineering solutions.


 Support analytics, data science and data platform teams and understand their unique needs and challenges.


 Design, implement and manage data flows that integrate information from various sources into a common pool implementing data pipelines based on ELT and ETL models.


 Build dashboards and query tools for cross-functional teams in the organization. 


 Develop datasets and models to meet the requirements of analysts and key stakeholders; optimizing query compute and data storage patterns for cost and performance


 Work closely with the engineering teams to troubleshoot issues with API's and data exchanges between multiple systems.

 What Knowledge, Skills & Abilities you bring to the table::

 5+ years Data Engineering experience including 


 3+ years hands-on experience working with large, structured/unstructured datasets using partitioned cloud storage architecture using query engines such as Spark, Delta Lake.


 3+ yearsExperience designing, developing, deploying and testing in Databricks. 


 3+ years of hands-on experience in Python/Pyspark/SparkSQL.


 2+ years experience on Big data pipelines/DAG tools like Airflow, dbt is required.


 2+ years of SQL experience, specifically to write complex, highly optimized queries across large volumes of data.


 Experience in the AWS computing environment and storage services such as s3/glacier is required.


 Experience with conceptual, logical and/or physical database designs is required.


 Good knowledge in Linux and shell scripting is highly desired.


 Past experience in healthcare data extraction, transformation and normalization is highly desired.


 Hands-on experience with Kafka or other live streaming technology is highly desired.


 Experience with Data Visualization tools like Looker, Tableau is desired.


 Strong communication skills to relay complex data integration requirements to team members.

 After 3 months you will have accomplished: 

 Develop a very good understanding of Apixio's products and existing DI tools and processes.


 Work closely with the R&D team to implement the building blocks of the next generation data integration platform.

 After 1 year you will have accomplished: 

 Instill excellence into the processes, methodologies, standards, and technology choices embraced by the team.


 Serve as a SME in migrating the legacy data Integration pipelines to the next generation Integration platform.



    The salary range below is for Base Salary. Total compensation also includes benefits and variable compensation. Compensation will be determined based on several factors including, but not limited to, skill set, years of experience, and the employee’s geographic location.
  
 Base Compensation

    $120,000—$140,000 USD
  


 We recognize that people come with experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Your skills and background may be more translatable to this role than you initially thought. Allow us the opportunity to get to know you. Please let us know if you require accommodations during the interview process.
 What Apixio can offer you:

Meaningful work to improve healthcare
Competitive compensation
Exceptional benefits, including medical, dental and vision, FSA
401k with company matching
Generous vacation policy
A hybrid work schedule (2 days in office & 3 days work from home) (Note: If the position is designated as REMOTE it will stay REMOTE)
Modern open office in beautiful San Mateo, CA; Los Angeles, CA; San Diego, CA; Austin, TX and Dallas, TX
Subsidized gym membership
Catered, free lunches
Parties, picnics, and wine-downs
Free parking

We take your privacy very seriously. Please review our privacy policy to see exactly how we protect your information here
 We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.
 Apixio will consider for employment qualified applicants with criminal histories pursuant to the San Francisco Fair Chance Ordinance subject to the requirements of all state and federal laws and regulations.
 If you are a recruiter or placement agency, please do not submit resumes to any person or email address at Apixio prior to having a signed agreement from Talent Acquisition. Apixio is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Furthermore, any resumes sent to us without an agreement in place will be considered your company's gift to Apixio and may be forwarded to our recruiters for their attention and no fee will be paid.


 LI-RB1"
Big Data Engineer - PySpark,Logic20/20 Inc.,"Dallas, TX",Posted 30+ days ago,"$130,000 - $155,000 a year",https://www.indeed.com/rc/clk?jk=3267f0cf0ccd342c&fccid=2e9e942085461a66&vjs=3,"Company Description
  We’re a seven-time “Best Company to Work For,” where intelligent, talented people come together to do outstanding work—and have a lot of fun while they’re at it. Because we’re a full-service consulting firm with a diverse client base, you can count on a steady stream of opportunities to work with cutting-edge technologies on projects that make a real difference.
 Logic20/20's Global Delivery Model creates a connected experience for Logicians across geographies. You'll have access to projects in different locations, the technology to support Connected Teams, and in-person and online culture events in our Connected Hub cities.



 Job Description
  As a Data Engineer, you'll be joining a team at one of the nation's largest utilities companies in California to build out an ""All Hazards"" portal, which is a web-based platform designed to provide information and resources related to various types of hazards and emergencies that may impact service areas. The portal is intended to help its customers, stakeholders, and the general public to access information and resources related to safety, preparedness, and response during emergencies or hazardous events, such as wildfires, earthquakes, storms, and other natural disasters.
 Hear more about these efforts as Jeff Lovington shares his experience working in Data Science and Machine Learning for the Energy & Utilities sector.
 About the team
 The Logic20/20 Advanced Analytics team is where skilled professionals in data engineering, data science, and visual analytics join forces to build simple solutions for complex data problems. We make it look like magic, but for us, it’s all in a day’s work. As part of our team, you’ll collaborate on projects that help clients spin their data into a high-performance asset, all while enjoying the company of kindred spirits who are as committed to your success as you are. And when you’re ready to level up in your career, you’ll have access to the training, the project opportunities, and the mentorship to get you where you want to go.
 “We build an environment where we really operate as one team, building up each other’s careers and capabilities.” – Adam Cornille, Director, Advanced Analytics



 Qualifications
  Must have:

 3+ years of implementation experience using PySpark
 5+ years of data engineering experience
 Strong understanding of high-performance ETL development with Python
 Experience with Big Data Technologies (Hadoop, Spark, MongoDB)
 Experience designing and developing cloud ELT and date pipeline with various technologies such as Python, Spark, PySpark, SparkSQL, Airflow, Talend, Matillion, DBT, and/or Fivetran
 Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule

 Preferred:

 Experience with Palantir’s Foundry

 Additional Information
  All your information will be kept confidential according to EEO guidelines.
 Compensation range: $130,000 - $155,000 annually

 About Logic20/20
 To learn more about Logic20/20, please visit: https://www.logic2020.com/careers/life-at-logic
 Core Values
 At Logic20/20, we are guided by three core values: Drive toward Excellence, Act with Integrity & Foster a Culture of We. These values were generated and agreed upon by our employees—and they help us pursue our goal of being one of the best companies to work for and to work with. Learn more at https://www.logic2020.com/company/our-values.
 Logic20/20 Benefits
 Why Logic20/20? It’s our goal to be one of the best companies to work for. One piece of the puzzle is an evolving set of benefits that extend past medical, dental, and 401(k).
 You will have

 Career Development – A built-in program from day 1, providing a mentor and individually-directed training opportunities, plus access to leaders across the company
 PTO, Paid Holidays, & Voluntary Leave – Worry-free time off to recharge and pursue your personal goals
 Community & Committees – As part of our “Culture of We,” Logic20/20 invests in providing many social, interest, and learning opportunities


 Recognition – From peer recognition, swag, and the chance to win a once-in-a-lifetime type of award, we make your Logic20/20 journey stand out
 Referral Programs & Bonuses – Employee, project, and sales referral programs with paid incentives

 Equal Opportunity Statement
 We believe that people should be celebrated: for their talents, ideas, and skills, but most of all, for what makes them unique. We prohibit harassment and/or discrimination based on age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status, or any other basis as protected by federal, state, or local law.
 To learn more about our DE&I initiatives, please visit: https://www.logic2020.com/company/diversity-equity-inclusion
 Privacy Policy
 During the recruitment and hiring process, we gather, process, and store some of your personal data. We consider data privacy a priority. For further information, please view our company privacy policy."
GCP IBM Streams Data Engineer || Hybrid role,Cat software service,"Irving, TX",EmployerActive 7 days ago,$55 - $60 an hour,https://www.indeed.com/company/Cat-software-service/jobs/Data-Engineer-c5cf2ad420147f27?fccid=e497ab2d4dc4f4d9&vjs=3,"Job Title: GCP IBM Streams Data Engineer
Location: Irving, TX | Basking Ridge, NJ | Temple Terrace, FL | Alpharetta, GA | Piscataway, NJ | Colorado Springs, CO | Ashburn, VA | Lone Tree, CO
Duration : 12 months Contract
Skillset:
IBM Streams, Apache Flink, Google Cloud Platform, Linux, Apache Airflow, Google Compute Engine, GitLab, BigQuery, Python, Pyspark, Spark & Kafka
Job Description :

Must have 5 + years of experience as GCP Data Engineer.
The Data Engineer will be responsible for developing and supporting database applications to drive automated data collection, storage, visualization and transformation as per business needs.
Understand the existing IBM streams pipelines
Able to write the Custom templates in Apache Data flow (Apache Beam)
Migrate the IBM Stream pipelines to Data Flow
Able to handle Billions of records / day volume in real-time streaming systems and write the pipelines in Flink / Dataflow over GCP
Able to perform the Quality check on the data (Data validation)
Work experience over the CICD like jenkins and construct them over the cloud Database design, Data Modelling and Mining.
Consolidate data across multiple sources and databases to make it easier to locate and access.
Implement automated data collection and data storage systems.
Write complex SQL queries and stored procedures.
Work with multiple data systems and large relational databases.

Best Regards,
Sam wilson| IT Recruiter
CAT Software Services INC.
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience level:

8 years

Schedule:

No weekends

Experience:

IBM Streaming: 2 years (Required)
Apache: 3 years (Required)
Google Cloud Platform: 1 year (Required)

Work Location: On the road"
Senior Data Engineer,Camden Homes,"Dallas, TX 75243 (North Dallas area)",EmployerActive 29 days ago,,https://www.indeed.com/rc/clk?jk=3ff6fae6e8732b36&fccid=17ad8c79a686f6af&vjs=3,"The Position:
 The purpose of the position is to review, optimize, and maintain our business information systems to progress the development and efficient aggregation of all our data so that inter-department tasks and operations across applications support data integrity. Once data integrity is established, implement a system that will provide analysis from our data. warehouse. In addition to a database administrator, we're looking for someone who is will fill the gap between a database admin and an analyst. Someone who has the education background with technical knowledge to address specific technology related limitations, consider broader business information systems, and communicate with non-technical personnel to help leadership make better choices and approvals to processes, etc. 
Main Responsibilities:

 Data Warehouse Architecture

 Deep understanding of the sources, operations, and applications that generate the data to better architect our data warehouse.
 
Technical Support

 Aid with managing technical support requests from other members of the company and elevating cases to our IT vendor
 
Information Systems Support

 Work with our team on creating processes and learning best practices resulting in more efficient aggregation of data from our various sources to create better reporting
 

Authority:

 Correct inconsistencies concerning data integrity.
 Monitor and correct automations between different applications.
 Submit ideas or suggestions on better integrations and products.
 Submit ideas or suggestions on process improvement.

 Competencies:

 Microsoft Excel experience, experience in writing SQL Queries.
 Having additional knowledge in creating dashboards in Microsoft Power BI or Tableau is a plus.
 Experience in writing validation rules, creating flows, Process builders and Workflows within Salesforce.
 Experience in Salesforce architecture, reporting, and dashboards is a plus.
 Experience in managing IP phone system.
 Experience in writing API’s for Salesforce, experience in developing RPA is a plus.
 Minimum 7 years experience working as a data engineer.
 Masters Degree in Computer Science or related-field .

 What we provide:

 Competitive compensation
 Health Insurance, Vision, Dental, Life Insurance
 Short-Term Disability, Long-Term Disability
 PTO
 Gym
 Nice work environment



 After finishing up the Application Form, please proceed to complete the Candidate Survey to continue on with the hiring process. You must complete the Candidate Survey in order to move on to the next hiring step.


 Who We Are:
 Camden Homes is a vertically integrated privately-owned company that is in the business of providing housing solutions to the workforce of America. For more than 20 years, we have been achieving the goal of changing people's lives one house at a time. In order to help fulfill the American dream of becoming a homeowner, we build quality homes and sell them at an affordable price. We believe our people play a major role in our success, and to continue this our company follows and believes in the 6 Core Values. Teamwork makes the dream work, Go all-in, Always do the right thing, Sweat the small stuff, Create happy energy, Deliver ""wow"".
 We are proud to be an equal opportunity employer. Camden Homes highly respects and welcomes diversity and believes it to enhance the community we live and work in. Our applicants will not be considered for the positions based on their race, ethnicity, national origin, sex, sexual orientation, gender identity, age, disability, religion, or any other characteristic that is protected by the law."
Principal Analytics Data Engineer,CarOffer,"Addison, TX 75001",EmployerActive 2 days ago,"Up to $130,000 a year",https://www.indeed.com/company/CarOffer/jobs/Principal-Software-Engineer-c01159936de915e9?fccid=bc531b7769dde0ea&vjs=3,"We are an innovative and customer-centric company that revolutionizes car selling and buying. With a transparent and trustworthy approach, we simplify the process, providing fair valuations and quick offers for sellers, and an extensive, thoroughly inspected inventory for buyers. Join us in driving towards a sustainable automotive future, where the car trading experience becomes seamless and enjoyable for all.
Role overview
As a Principal Analytics Data Engineer at our organization, you will be at the forefront of revolutionizing the way we leverage data to drive insights and decision-making. Your primary focus will be designing, developing, and maintaining a robust and scalable data analytics infrastructure, enabling efficient data storage, retrieval, and analysis. Collaborating closely with data analysts, scientists, and cross-functional teams, you will craft interactive visualizations and dashboards to empower stakeholders with actionable insights. With a keen eye for data governance and performance optimization, you will ensure that our analytics ecosystem operates transparently, delivering a seamless and rewarding experience for all users. Your expertise and leadership will be instrumental in shaping our data-driven future and guiding our organization towards data excellence.
What you'll do

Data Analytics Infrastructure: Lead the design, development, and maintenance of a cutting-edge data analytics infrastructure. You will build robust data models, warehouses, and pipelines to enable efficient data storage, retrieval, and processing, ensuring the seamless flow of data throughout the organization.
Actionable Insights: Collaborate closely with data analysts, scientists, and business stakeholders to understand data requirements and translate them into actionable insights. You will develop interactive visualizations and dashboards that empower teams to make informed decisions and drive business growth.
Data Governance and Performance Optimization: Ensure adherence to data governance principles, data security, and privacy standards while continuously optimizing data pipelines and analytics queries for top-notch performance. Your focus on efficiency will enhance the speed and accuracy of data-driven processes.
Innovation and Best Practices: Stay at the forefront of data engineering and analytics technologies, evaluating and implementing emerging tools and best practices. Your expertise will shape our data strategy, maximizing the value derived from our data assets.
Leadership and Mentorship: Act as a subject matter expert and mentor for data engineering teams, providing guidance, support, and technical leadership. Your collaborative approach will foster a culture of innovation and continuous improvement within the organization.
Cross-Functional Collaboration: Work closely with cross-functional teams, including product managers, developers, and data scientists, to align data engineering efforts with business objectives. Your ability to communicate complex technical concepts to non-technical stakeholders will be invaluable in driving successful projects.
Scalable Solutions: Architect data solutions that can scale to accommodate growing data volumes and evolving business needs, ensuring our analytics ecosystem remains agile and adaptable.

What you'll bring

The successful candidate will be required to perform their duties onsite in the CarOffer's Addison, Tx office.
Bachelor's degree in Computer Science, Data Science, Statistics, or a related field required. Master's preferred or equivalent practical experience and relevant certifications will also be considered.
4+ years of experience as a software engineer, including 3+ years as a Data Engineer
Professional experience developing and executing on a data warehouse and/or data platform strategy using a modern data stack
Confident and experienced looking at data platform architecture as a whole and helping to drive strategic roadmap decisions
Ability to move quickly in a dynamic and cross-functional team environment
Expertise in SQL, Python, and system architecture – with the ability to understand the context of a business need and build a system that addresses it

Job Type: Full-time
Pay: Up to $130,000.00 per year
Benefits:

401(k) matching
Health savings account
Life insurance
Paid time off
Referral program
Retirement plan
Vision insurance

Compensation package:

Bonus pay

Experience level:

5 years

Schedule:

Monday to Friday

Ability to commute/relocate:

Addison, TX 75001: Reliably commute or planning to relocate before starting work (Required)

Experience:

SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)

Work Location: In person"
Sr. Data Engineer,"Evergreen Residential Holdings, LLC","Hybrid remote in Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=2a3fe808051fe586&fccid=dd616958bd9ddc12&vjs=3,"We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!
 We offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.

 This is not a consulting position. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time for this position.
 This position will be (Full-time/Permanent employment) based in our downtown Dallas offices, and we will consider a hybrid work schedule.
 As a technical/engineering expert, you also pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs. You will design and support data warehouse systems, perform data extraction and ensure data accuracy, enabling real-time insights from both internal and market data that will drive revenue growth and capital efficiency. This position plays a critical role in working with our analytics and reporting specialists to help Evergreen Residential make the best investment decisions.
 The Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.


 The role includes, but is not limited to, the following responsibilities:

Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository
Developing and maintaining data processing and storage infrastructure
Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes
Collaborating with data analysts to ensure that data is readily available for analysis and modeling
Optimizing database performance and troubleshooting issues as they arise
Implementing data security and access controls to protect sensitive data
Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization
Staying up-to-date with emerging trends and technologies in data engineering
Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance
Ensure scientific method and research are key drivers of the product roadmap

 What You Will Bring to the Table:

Bachelor's Degree in a relevant field required
Min 5 years of experience in data engineering or a related field
Proficiency in one or more programming languages such as Python, Java, or Scala
Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases
Experience in real estate investment and/or rental sector highly desirable
Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry
Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function
Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level
A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk
Collaborative team player, you are truly a ""do-er"", happy to be a hands-on problem-solver to move the data program forward
Excellent communication skills – verbal and written


 About Evergreen Residential
 Founded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.
 The leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.
 Beyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.
 Our cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.


 Equal Opportunities and Other Employment Statements
 We are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law."
Data Engineer - Need on w2 only,Vidu Solutions LLC,"Dallas, TX",EmployerActive 2 days ago,"$66,357 - $146,846 a year",https://www.indeed.com/company/Vidu-Solutions-LLC/jobs/Data-Engineer-040fb07f4898d185?fccid=324290a0b0d0068d&vjs=3,"Need 3 Data Engineers
internal positions. Min 2 to 3 years of experience is fine.
Must be W2 only.
Good to be onsite. Immediate interview.
Job Type: Contract
Salary: $66,356.86 - $146,845.57 per year
Schedule:

8 hour shift

Work Location: In person"
"Big Data Engineer - Hadoop data engineer at Dallas, TX",CapB InfoteK,"Dallas, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=3ce719a2d31bb050&fccid=5033463af3c58699&vjs=3,"The ETL Hadoop Data Engineer will be responsible for analyzing the business requirements design develop and implement highly efficient highly scalable ETL processes 
Candidate is required to perform daily project functions with a focus on meeting the business objectives on time in rapidly changing work environment and should be able to lead and drive globally located team to achieve business objectives 


Required Skills:
 3 8 years of hands on experience working with ETL Hadoop Knowledge of various components of Hadoop ecosystem and experience in applying them to practical problems Hive Impala Spark Strong knowledge of working with relational databases like Teradata DB2 Oracle Sql server 
Hands on experience in writing shell scripts on Unix platform Experience in data warehousing ETL tools MPP database systems 
Understanding of Data Models Conceptual Logical and Physical Dimensional Relational Data Model Design Analyze functional specifications and assist in designing potential technical solutions Identifies data sources and works with source system team and data analyst to define data extraction methodologies 
Good knowledge in writing complex queries in Teradata DB2 Oracle PL SQL Maintain batch processing jobs and respond to critical production issues communicate well with stakeholders on his her proposal recommendations 
Knowledge status risks regarding delivering solution on time 
Strong experience with Data Analysis Data Profiling Root Cause Analysis 
Should able to understand Banking system processes and data flow 
Can work independently lead and mentor the team."
Data Engineer,RumbleOn,"Irving, TX 75038",EmployerActive 2 days ago,"$115,000 - $125,000 a year",https://www.indeed.com/rc/clk?jk=55c451d0f50bfab6&fccid=bd9b67a2bfe2ef7e&vjs=3,"As a Senior Data Engineer, you will be responsible for architecting, building, and maintaining scalable data infrastructure to support our data-driven initiatives. You will collaborate closely with cross-functional teams, including analysts, stakeholders, and software engineers, to design and implement data solutions that meet business requirements. Your expertise will be essential in ensuring data integrity, performance, and security while optimizing data processes for efficiency and scalability.
 Responsibilities:

Design, develop, and maintain data infrastructure, including data pipelines, ETL processes, and data warehouses, to support data-intensive applications, analytics, and business intelligence.
Collaborate with stakeholders to gather data requirements and translate them into scalable data solutions.
Identify, evaluate, and implement appropriate tools and technologies for data ingestion, storage, processing, and analysis.
Ensure the reliability, availability, and performance of data platforms through monitoring, troubleshooting, and optimization.
Develop and enforce data governance policies, including data quality standards, privacy, and security protocols.
Lead and mentor junior data engineers, providing technical guidance and fostering a culture of continuous learning and improvement.
Collaborate with cross-functional teams to understand data needs and design efficient data models, schemas, and data transformation processes.
Stay up-to-date with industry trends, emerging technologies, and best practices in data engineering and recommend enhancements to existing systems and processes.
Participate in code reviews, perform unit testing, and maintain documentation to ensure code quality, maintainability, and knowledge sharing.
Proactively identify and resolve performance bottlenecks, data issues, and technical challenges related to data processing and storage.

Requirements

Bachelor's degree in Computer Science, Engineering, or a related field. A Master's degree is a plus.
Proven experience (5+ years) as a Data Engineer or similar role, with a strong focus on building scalable data infrastructure.
Strong proficiency in data modeling, data warehousing concepts, and SQL.
Expertise in designing and implementing data pipelines, ETL processes, and workflow orchestration using tools such as Apache Airflow, AWS Glue or similar.
Proficiency in programming languages such as Python, Java, or Scala for data processing and scripting.
Solid understanding of distributed systems, cloud platforms (e.g., AWS, Azure, GCP), and big data technologies (e.g., Hadoop, Spark, Kafka).
Experience with data integration and streaming technologies, such as AWS Kinesis Data Streams/Firehose, Apache Nifi, or similar.
Strong knowledge of database technologies (e.g., SQL databases, NoSQL databases, columnar databases).
Familiarity with containerization (e.g., Docker, Kubernetes) and infrastructure-as-code tools (e.g., Terraform) is a plus.
Excellent problem-solving skills, attention to detail, and the ability to work in a fast-paced, dynamic environment.

Benefits
 What RumbleOn Offers You:
 A fun, relaxed, and casual work environment with awesome people by your side working as a team to ensure the entire group's success! Plus...

Healthcare, Dental, & Vision Insurance (RumbleOn pays a generous portion of medical premium!)
PTO Plan & Public Holidays Off
Close knit, open, inviting environment where you can make your mark and where your ideas are heard!
Employee discounts on purchases
Extremely competitive compensation packages commensurate with experience and skillset
Fully stocked kitchen with drinks and snacks all day
Fun company events
The opportunity for growth and a solid long term career...we promote from within!!
Casual Dress code
Training and full support while you learn
And more…

*All applicants must pass pre-employment testing to include: background checks, MVR, and drug testing in order to qualify for employment*·"
Data Engineer - (ETL & Pyspark),Maveric NXT INC,"Irving, TX 75039 (Freeport/Hackberry area)",EmployerActive 1 day ago,"$75,000 - $90,000 a year",https://www.indeed.com/company/Maveric-NXT-Inc/jobs/Data-Engineer-25befd8855b33277?fccid=71c90b19b09e328e&vjs=3,"Role: Data Engineer ( ETL & Pyspark)
Location: Irving, TX
Mode of hire: Fulltime
Nature of Industry / End Client: Banking Domain.
Mode of work: currently Hybrid Monday & Wednesday ( but should be ready to work as per employer work policy).
Start date: at the earliest since this is a backfill.
Interview: 2 rounds of discussion internal + customer round of discussion.
Must to have lead experience
Must to have good communication & articulation skills.
* We prefer candidates who are local to Irving TX or ready to relocate this Job location.
Responsibilities:
Title: Lead ETL Automation Tester.
Primary Skill: Pyspark along with Abinitio & Informatica.

7-8 years of overall experience in ETL & Automation Testing using PySpark and other tools like Ab Initio and Informatica (preferably Ab Initio).
Must possess strong technical skills in PySpark(2-3 Years).
Strong Unix Shell scripting skills.
Excellent communication skills.
Experience working in client-facing roles.
Strong analytical skills.
Strong functional domain knowledge – Banks.
Knowledge of mainframe systems is a plus.
Ability to drive automation initiatives.

Responsibilities:

Develop, implement, and maintain ETL processes using PySpark and Ab Initio or Informatica.
Create and execute automation test scripts for ETL workflows.
Collaborate with cross-functional teams to design efficient ETL solutions.
Troubleshoot and resolve issues related to ETL processes and testing frameworks.
Serve as the primary contact for clients, providing technical expertise and support.
Communicate any risks to both technical and non-technical stakeholders proactively.
Analyze data to identify patterns, trends, and potential issues.
Proactively identify and implement automation initiatives.
Collaborate with offshore teams for efficient knowledge sharing and provide the required guidance for project specific needs.

Job Type: Full-time
Pay: $75,000.00 - $90,000.00 per year
Benefits:

Health insurance
Paid time off

Experience level:

10 years
6 years
7 years
8 years
9 years

Schedule:

8 hour shift

Ability to commute/relocate:

Irving, TX 75039: Reliably commute or planning to relocate before starting work (Required)

Experience:

Pyspark along with ETL: 2 years (Required)

Work Location: In person"
Data Engineer - Mulesoft,American National Bank of Texas,"Plano, TX 75074",EmployerActive 7 days ago,,https://www.indeed.com/rc/clk?jk=419a48250e2f8b02&fccid=83f67c59739e4e6c&vjs=3,"The Data Engineer (Mulesoft) is responsible for the design, build, and optimization of systems for data collection, storage, access and analytics at scale. The position is responsible for creating pipelines, building algorithms for accessing raw data, and providing data that is accurate, congruent, reliable, and easily accessible. Additionally, the position is responsible for the full life cycle development, implementation, production support, and performance tuning of the Enterprise Data Warehouse, Data Marts, and Business Intelligence Reporting environments. The role supports the integration of those systems with enterprise application databases and real time processing

Designs, builds and optimizes systems for data collection, storage, access, and analytics AT SCALE (i.e., data that is collected, stored, secured, etc. across the entirety of the bank's technology systems)
Builds out new API integrations to support continuing increase in data volume and complexity
Utilizes the latest technology and information management methodologies to meet requirements for effective logical data modeling, metadata management and warehouse domains
Assists with the development of architectural strategies for data modeling, design and implementation for metadata management, operational data shores and Extract Transform Load (ETL) environments
Creates and test data models for a variety of business data, applications, database structures to meet operational or project goals
Performs data analysis required to troubleshoot data related issues and assist to resolve issues
May require work on physical bank premises

Qualifications:

5+ years IT experience preferably in the financial industry
5 years experience developing dashboards and reports with a leading business intelligence and analytics platform like DOMO, Tableau or QLIK
5 years experience with a low code solution, Mulesoft experience
5 years experience experience developing and/or supporting SQL databases preferably MS-SQL
5 years experience working with integration platforms or middleware
5 years user requirements gathering/user case development experience; experience converting legacy reporting platforms to dashboards; experience transforming from a manual process centric environment to a fully automated environment; experience with API integration development; experience developing and/or support data warehouse architecture, preferably Snowflake

Skills:

Advanced proficiency in the use of Microsoft Excel, Word, Access, SQL Database, modeling, data manipulation; basic keyboarding and calculator skills; must be able to perform advanced math and carry out complex written instructions
Travel to a variety of branch and vendor locations to perform work and/or attend meeting as required
Work occasionally requires more than 40 hours per week to perform the essential functions of the position
Lifting in an office setting may be required up to 30 lbs.

ANBTX strongly encourages candidates that are fluent in English and Spanish to apply. Jobs that specifically require candidates to be bilingual will be posted as a requirement."
Data Engineer II,"AmTrust Financial Services, Inc.","Hybrid remote in Dallas, TX 75244",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=400dc68ef5de7147&fccid=e6e094c308c82aee&vjs=3,"Overview: 
 
   Responsible for design, development, testing, and maintenance of data architectures including large scale databases, data pipeline processes, and data delivery solutions. Maintains a solid understanding of AmTrust’s mission, vision, and values. Upholds the standards of the AmTrust organization.
  Responsibilities: 
 
Assemble large, complex sets of data that meet the reporting and analytics needs of the enterprise.
 Analyze raw data sources and develop, test, and implement optimized data pipeline solutions and controls.
 Design, develop, test, and implement queries, reports, cubes, and dashboards in support of business analytic needs.
 Continuous focus on improving data quality and efficiency.
 Interface with business analysts to discuss timelines and clarify requirements as it pertains to new projects, enhancements and bug fixes. 
Adherence to DevOps, SDLC, and Change Management practices 
Keeps current with market trends and demands.
 Performs other functionally related duties as assigned.
 Qualifications: 
 
Advanced knowledge of SQL, T-SQL and/or PL/SQL; Ability to write complex, highly optimized queries across large volumes of data.
 A thorough understanding of data pipeline construction and ETL processes. 
Proficiency with SSIS | SSAS | SSRS
 Expertise in database design methodologies.
 Excellent analytic skills.
 5+ years of data focused technical experience


 Preferred:

 Excellent oral and written communication skills.
 Ability to communicate complicated/ technical information to non-technical audiences in an efficient and simple method.
 Curiosity and passion for data, visualization and solving problems
 Highly creative in order to determine the best solutions for real-world problems with quantitative data.
 Enjoy collaborating with others in a team atmosphere.
 Eagerness to learn in a fast-paced environment.
 B.S. degree in Computer Science, Math, Statistics, or related technical field, or equivalent professional experience.
 Property & Casualty insurance experience a plus.



 This job description is designed to provide a general overview of the requirements of the job and does not entail a comprehensive listing of all activities, duties, or responsibilities that will be required in this position. AmTrust has the right to revise this job description at any time.
 


 #LI-GD1
 

   #LI-HYBRID
  What We Offer: 
 
   AmTrust Financial Services offers a competitive compensation package and excellent career advancement opportunities. Our benefits include: Medical & Dental Plans, Life Insurance, including eligible spouses & children, Health Care Flexible Spending, Dependent Care, 401k Savings Plans, Paid Time Off.
 


 AmTrust strives to create a diverse and inclusive culture where thoughts and ideas of all employees are appreciated and respected. This concept encompasses but is not limited to human differences with regard to race, ethnicity, gender, sexual orientation, culture, religion or disabilities.
 


 AmTrust values excellence and recognizes that by embracing the diverse backgrounds, skills, and perspectives of its workforce, it will sustain a competitive advantage and remain an employer of choice. Diversity is a business imperative, enabling us to attract, retain and develop the best talent available. We see diversity as more than just policies and practices. It is an integral part of who we are as a company, how we operate and how we see our future."
Associate Systems Engineer - Audio Visual and Data Networks,ELB US Inc.,"Hybrid remote in Dallas, TX 75238",EmployerActive 2 days ago,"$60,000 - $80,000 a year",https://www.indeed.com/company/ELB-US-Inc./jobs/Audio-Visual-Engineer-0437cf5d8c35f7af?fccid=8e80bab487fe40c1&vjs=3,"ELB US Inc. is a world class integrated solutions provider, specializing in visual collaboration and unified communication services and solutions. We offer a full suite of value-added services, including needs assessment, solution design, installation, training, maintenance and support, to ensure integration success. We strive to create high quality integrated solutions for all our enterprise, government and education customers. At ELB we create experiences, we communicate ideas, and we collaborate across the world!
Our company is growing and we are looking to on-board an Associate Systems Engineer to join our team.
Key responsibilities include, but are not limited to:

Consult with clients and colleagues as necessary regarding audio visual requirements.
Assess briefs provided by customers, Account Manager, or Systems Engineers and identify options for potential solutions.
Assist with project design and/or build costing and project bidding activities.
Participate in the development of logical and innovative AV solutions to complex problems, ensuring that AV system designs meet customer business requirements.
Create, edit, and track Bill of Materials for pre, and post-sales support and for product release/ordering.
Create labor estimate for pre-sales support.
Assist in the development of detailed design documentation once a contract for a system has been awarded.
Assist Project Managers, where necessary, with planning the installation of systems
Provide technical writing services when requested.
Verify installation completion and completeness.
Some equipment installation and rack build as required.
Commission AudioVisual systems in the shop and in the field as required. System commissioning may include, but is not limited to: DSP configuration and audio system tuning; Video display and processor configuration and calibration; Loading and testing completed control system code; Testing and troubleshooting the system through satisfactory completion.

Qualifications:

Possess a four-year degree, or equivalent combination of education and related work experience in the Professional Audio, Professional/Broadcast Video, Audio Visual, Security, or Data Integration fields.
Excellent written and oral communication skills with an ability to define and articulate “the vision”.
Ability to create and evaluate specifications and present recommendations and solutions to customers in a clear and articulate way.
In depth knowledge of AV theory, technology and product.
Possess general work knowledge of electronics and/or construction.
Minimum CTS Certification or equivalent qualification.
Personal management of attitude, perseverance, and time.

Job Type: Full-time
Pay: $60,000.00 - $80,000.00 per year
Benefits:

Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance

Experience level:

3 years

Schedule:

8 hour shift
Monday to Friday

Ability to commute/relocate:

Dallas, TX 75238: Reliably commute or planning to relocate before starting work (Preferred)

Education:

Bachelor's (Preferred)

License/Certification:

CTS certification (Preferred)

Work Location: Hybrid remote in Dallas, TX 75238"
Big Data Engineer,Docyt,"Remote in Dallas, TX",Posted 6 days ago,Full-time,https://www.indeed.com/rc/clk?jk=85cecf8c67600f6d&fccid=d37438b8a6c14975&vjs=3,"Docyt, a fast-growing FinTech startup based in Silicon Valley, is seeking a highly motivated Big Data Engineer to join our team. The ideal candidate will be responsible for maintaining our data processing infrastructure and optimizing our data architecture, as well as contributing to the development and implementation of new data-driven solutions. At Docyt, we are passionate about empowering businesses to take control of their financial data using an AI-driven super app, and we're looking for a skilled engineer to help us continue to innovate in this exciting space.
 Responsibilities

Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse
Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines
Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques
Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities
Build and maintain data models and ensure data accuracy and consistency
Implement and manage data security measures, including backups and access controls
Participate in code reviews, providing constructive feedback to ensure code quality

Requirements

Bachelor's degree in Computer Science, Engineering, or related field
At least 3 years of experience in big data engineering or related field
Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling
Proficient in at least one programming language, such as Python or Java
Experience with SQL and NoSQL databases
Familiarity with distributed computing frameworks, such as Hadoop or Spark
Understanding of data security and access control best practices
Strong problem-solving skills and ability to work independently and in a team environment
Excellent verbal and written communication skills
Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field
Experience with AWS and Docker is a plus.

Benefits

Great growth potential at a fast-growing startup, we want you to grow with us!
Company-provided laptop and necessary hardware to ensure your setup for success.
Comprehensive health, dental and vision coverage.
Company-sponsored 401(k)
Inclusive and motivating work culture that values team collaboration.

About Us
 Docyt, pronounced “docket”, is a FinTech startup headquartered in Silicon Valley, that is passionately focused on giving businesses control of their financial data. While great strides have been made in sending and receiving payments, businesses still struggle to aggregate all their financial data, understand it, and use it to make well-informed, timely decisions. Docyt brings order to data chaos.
 Docyt is a super app that applies AI (artificial intelligence) across the entire accounting tech stack. Docyt digitizes financial data, automates both income and expense workflows, continuously reconciles QuickBooks®, and generates real-time financial statements. That explains what we do, but here’s why it’s important. A complete, accurate, real-time financial picture empowers businesses to make timely and smart decisions so their business can thrive."
Sr. Data Engineer,Experfy Inc,"Dallas, TX 75201 (Downtown area)",Posted 30+ days ago,Contract,https://www.indeed.com/rc/clk?jk=5ada3bf94db180e8&fccid=19327ae379f6e1a2&vjs=3,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.

 Responsibilities
 Position Responsibilities

Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions

 Requirements

8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise – Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services – Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)

 Tools Required

Amazon Kinesis
AWS
AWS Glue
Java
Kafka
Microsoft SQL Server
Neo4J
PySpark
Python
Scala
Snowflake
Spark
Sybase ETL"
Sr. Data Engineer || NO C2C || ONLY W2,Amazee Global Ventures Inc,"Irving, TX",EmployerActive 2 days ago,Up to $60 an hour,https://www.indeed.com/company/Amazee-Global-Ventures-Inc/jobs/Data-Engineer-39772e076a550449?fccid=6f3859631af87af7&vjs=3,"Title- Sr. Data Engineer
Location- Irving, TX (Onsite Role)
Type – Contract – 6 Months contract to hire
Rate - $60/hour on w2 || no c2c
Job Descriptions:-
As a member of the Data and Analytics team, the Sr. Data Engineer role will contribute to the success in both the daily operations and long-term strategy of the organization's data assets. The optimal candidate brings a business-centric mindset to data architecture efforts and has a track record of being a growth enabler.
Job Responsibilities:

Design, build & maintain scalable data pipelines in Azure to ensure their compliance with up-time, security, and privacy.
Actively contribute to both data & application governance initiatives by designing and implementing policies, procedures, and best practices
Implement API-based integrations to support continuing increases in data volume and complexity
Collaborate with report developers/analysts and business teams to improve data models that feed BI tools and increase data accessibility.
Build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python and SQL
Assist in the design of processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
Troubleshoot data pipeline related issues and oversee their resolution.
Identify ways to improve data reliability, efficiency, and quality.
Collaborate with all business units and development teams to develop strategy for long term data platform architecture.
Familiarity with one or more DevOps tools (Git, Jenkins, CI/CD, Jira)
Identify and integrate external data sources from various source formats.
Assist and oversee other projects as assigned

Qualifications/Skills :

Mastery in converting raw data into valuable datasets
Excellent prioritization skills
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service

Education/Experience:

BS or MS degree in Computer Science or a related field
Microsoft DP-203 certification preferred
5+ years of database architecture and dimensional data modeling
2+ years assisting in Master Data Management efforts, including metadata management

Preferred Experience:

5-7+ years of data management experience
5+ years of data migration activities
5+ years of Python and SQL experience
3+ years working in an Agile development environment pairing DevOps with CI/CD pipelines
3+ years with data processing systems in Azure (Data Lake storage, Data Bricks, Data Factory, Event Hubs, etc

This is a full-time W2 opportunity with Amazee Global Ventures Inc. We welcome all qualified applicants to apply.
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Contract
Salary: Up to $60.00 per hour
Compensation package:

Hourly pay

Experience level:

10 years
11+ years

Schedule:

Monday to Friday

Ability to commute/relocate:

Irving, TX: Reliably commute or planning to relocate before starting work (Required)

Experience:

Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)

Work Location: In person"
Data Engineer Clinical Analytics,Mitchell Martin,"Hybrid remote in Addison, TX",Posted 30+ days ago,"$125,000 - $150,000 a year",https://www.indeed.com/rc/clk?jk=208012cafe38d5fd&fccid=56dffd077bee5a25&vjs=3,"Our client, A national health care establishment, is seeking a Data Engineer Clinical Analytics
  

Location: Addison, TX / Hybrid
  
Position Type: IT Full Time
  

Job Summary:

 The Data Engineer- Clinical Analytics is primarily focused on analytical processes with ability to implement database solutions and best practices in the realm of data science and machine learning projects. Essential software engineering skills with strong foundational knowledge on data movement and orchestration both on-premises and cloud environment. The Data Engineer supports and aligns with business decisions within client by analyzing raw data, constructing, and maintaining data systems, and improving data quality and efficiency. Implements programming languages to develop and test architectures that enable data operations for predictive (i.e., machine learning/AI) or prescriptive modeling.
  

Responsibilities:


Analyze, develop, combine raw information, and maintain various data sources
Improve data quality and efficiency to build data systems and pipelines
Identify opportunities for data acquisition and collaborate with Application owners and Subject Matter Experts (SME) to document data domain knowledge
Implement ETL methods to prepare both structured and unstructured data for predictive and prescriptive modeling
Leverage data serialization techniques to meet project needs for use in various reporting platforms
Collaborate with Business Intelligence (BI) ETL Developers/Data Architect, Data Scientists, Reporting Analysts, and Subject Matter Experts (SME) to understand business goals
Understand enterprise project life cycle and prepare for integration and user acceptance testing methods.
Produce technical documentation by following enterprise standards and guidelines
Participate in relevant information-sharing activities
Serve as escalation point for application support and troubleshooting
Proactive identification of issues and opportunities that will have an impact on the business use of reports and ensure managerial awareness
Daily review outstanding issues to assure that troubleshooting and resolutions are current
Ensure all changes comply with change management policies and procedures
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Job-Related Skills/Competencies:

Client Core Competencies of Service Mentality, Attention to Detail, Sense of Urgency, Initiative and Flexibility
Ability to make decisions or solve problems by using logic to identify key facts, explore alternatives, and propose quality solutions
Outstanding customer service skills as well as the ability to deal with people in a manner which shows tact and professionalism
The ability to properly handle sensitive and confidential information (including HIPAA and PHI) in accordance with federal and state laws and company policies
Strong SQL development and performance tuning skills
Competencies with Oracle, SQL Server, SSIS, Sybase, NoSQL, Python, Azure, Docker, Git, and Visual Studio
Experience with Data Lakes, Lake Houses, and ELT is preferred
Experience with Azure Client, Azure Data Factory, Azure Data Bricks, Azure Data Flow, and Azure Functions is preferred
Highly organized
Communication skills to be able to effectively speak and write in a clear and professional manner
Skilled at listening and providing feedback

Customarily has at least the following experience:

Customarily has at least three or more years in Software development / Data-Centric pipelines / Model-Centric pipelines
Relational Database experience
Documentation and publication

Salary Range: $125,000 - $150,000"
Data Engineer (Azure),Tiger Analytics,"Remote in Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=a11a934648a4bbcb&fccid=cba01270e96bb012&vjs=3,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
 Responsibilities 

Design, implement, configure, and maintain Azure SaaS/PaaS/IaaS(Data bricks/ADF/Synapse/VMs/AKS etc.) Cloud Services for hosting wide range of complex projects.
Analyze business decisions related to hosting and make recommendations/estimates accordingly
Resolve issues and provide technical assistance on the Azure cloud-hosted environment
Create and maintain Cloud security strategies, policies, procedures, and documentation
Configure VMs and Web Applications
Coordinate and deploy software application releases to UAT and Production
Document issue resolutions and changes in cloud or system configurations
Perform and evaluate cost analyses and vendor comparisons of software/hardware systems to ensure cost-effective and efficiency and measures feasibility of various approaches and makes recommendations
Needs to have a continual improvement mindset while working within a team centric environment.
Apply advanced methods, theories and research techniques

 Requirements

Bachelor’s degree in Computer Science or similar field 
4+ years of experience in traditional and modern Big Data technologies (HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Apache Spark, hBase, Oozie, No SQL databases, PostgreSQL, GIT, Python, REST API, Snowflake, etc.)
Experience in Java/Python/Scala 
Experience extracting/querying/joining large data sets at scale 
Experience building data platforms using Azure stack 
Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data 
Strong knowledge on Azure Storage schematics such as Gen1 and Gen2
Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks 
Knowledge of Azure networking, security, key vaults, etc. 
Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred 
Experience utilizing Snowflake to build data marts with the data residing in Azure storage is a plus 
Knowledge of SAS, Teradata, Oracle, or other databases a plus
Exposure with R and ML technologies a plus 
Strong communication and organizational skills

Benefits
 This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility."
Principal Data Engineer - AWS,Tiger Analytics,"Remote in Dallas, TX",EmployerActive 12 days ago,Full-time,https://www.indeed.com/rc/clk?jk=ca638abf8c5b4ad0&fccid=cba01270e96bb012&vjs=3,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
 The Big Data Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. The right candidate will have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business challenges. We are looking for someone who can bring their vision to the table and implement positive change in taking the company's data analytics to the next level.
 Requirements

Bachelor’s degree in Computer Science or similar field
5+ years of experience in a Data Engineer role.
Experience with relational SQL and NoSQL databases like MySQL, Postgres.
Strong analytical skills and advanced SQL knowledge.
At least 4 plus years of hands on experience with AWS cloud services: EC2, EMR, Athena.
Atleast 4 plus years of hands on experience in Python.
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Experience extracting/querying/joining large data sets at scale.
A desire to work in a collaborative, intellectually curious environment.
Strong communication and organizational skills.

Benefits
 This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility."
Senior Data Engineer - Kafka,Republic Finance,"Plano, TX 75024",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=8330a270961f647d&fccid=a707bde58b3deaa0&vjs=3,"Overview and Responsibilities: 
 
   At Republic Finance, we are building a leading consumer lending company. In business for more than 70 years, we are proud to be a trusted lender in over 250 communities across the Central and Southeastern United States! We specialize in providing flexible lending solutions and incomparable customer service. Driven by our shared mission to pursue excellence for our team, our customers, and our performance, Republic Finance is continuously improving our online customer offerings in tandem with opening more branches in new states each year. We also regularly promote from within and enjoy giving back to the communities we serve. In a nutshell, our company culture is about helping customers, investing in our employee's future, and ensuring that our overall performance makes an impact on our community.
 


 The Sr. Data Engineer plays vital within Data Management & Analytics group. Sr. Data Engineer works closely with Data Analyst & Architect, multi-functional teams and develop data streaming solutions that support our organization's objectives and goals from inception to closure. This role will be responsible for the overall use and implementation of Kafka components including the platform, observability, governance, standard processes, and solution development. An excellent understanding of Kafka principles and enterprise integration patterns is required. Job responsibilities are as follows:
 


 Primary responsibilities are focused on the Analysis, Design, Development, testing of features, and implement solutions using Confluent Kafka.
 Apply Confluent Kafka API lifecycle development and management.
 Administer and improve Kafka use throughout our organization including Kafka Producers, Kafka Consumers, Kafka Connect, KsqlDB, KStreams, and custom implementations.
 Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
 Understand and apply event-driven architecture patterns and Kafka standard processes. Enable development teams to do the same.
 Define & Deploy API Governance & Monitoring processes.
 Unit test to confirm that solutions meet requirements outlined in the design documentation.
 Maintain and enhance Confluent Kafka architecture, Confluent Kafka design principles, and CI/CD Deployment procedures.
 Model system behaviors using standard process methods for communicating architecture and design.
 Ensure unit, system, integration, stress, and user acceptance testing are in alignment with architecture.
 Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
 Continuous learning to be a Confluent/Kafka domain expert.
 Work with Kafka API's (e.g., metadata, metrics, admin) to provide pro-active insights and automation.
 Perform regular reviews of performance data to ensure efficiency and resiliency.
 Contribute regularly to event-driven patterns, standard processes, and guidance.
 Review feature release and change logs for Kafka, and other related components to ensure best use of these systems across the organization.
 Work with leads to ensure all teams are aware of technology changes and impact.
 Acquire a deep understanding of source and sink connector technical details for a variety of platforms including Snowflake, Sales Force, MongoDB, PostgreSQL, MS SQL Server, Azure, and others as required.
 Manage a team of data engineers and provide leadership, guidance, and support to ensure that projects are delivered on time and within budget.
 Requirements: 
 
10+ years proven experience in data engineering.
 6+ years demonstrated ability with end-to-end design, development, and administration of services for Kafka.
 Proficient understanding of Kafka Architecture
 Solid understanding of Kafka (Consumption, publishing, and streaming) architecture and integrations is required.
 Experience in Spark with Scala and processing of streamed data from Kafka.
 Strong understanding and experience with security implementations (e.g., SSL/mutual SSL, Kerberos).
 Familiar with 3rd Party Confluent Kafka Connectors, Kafka Connect and its connectors for integrating with external systems.
 Development experience using Confluent Kafka producers, consumers, and streams.
 Experience with building streaming applications with Confluent Kafka
 Experience with Java coding, CI/CD processes, microservices is required.
 Good exposure to various Azure cloud platforms to play a key role in Application Modernization
 Knowledge of programming languages such as Java, Scala, and Python, as well as experience with the Kafka API for these languages.
 Significant experience in containers, micro services life cycle, Spring modules, maven build scripts etc.
 Experience in CI/CD, DevOps tool chain, GIT, docker, Jira, and a test-driven approach to agile delivery.
 Ability to participate in and contribute to code management in GitHub including actively collaborating in peer-reviews, feature branches, and resolving impediments and commits.
 Understanding of message queuing and messaging patterns.
 Knowledge of modern architectures based on micro services, REST APIs, NoSQL stores (e.g., Cassandra), and event-based architecture is preferred.
 Ability to adapt to a fast-paced, constantly evolving environment, meet targets and attention to detail.
 Ability to work collaboratively with multi-functional teams.
 Experience with Agile methodologies and project management.
 Experience working with Java, Python, SQL, and others.
 Knowledge of relevant technologies and tools, such as SQL, Hadoop, NoSQL, and cloud computing platforms
 Demonstrated strong attention to detail, quality, time management and user focus.
 Good communicator with problem solving approach and focus on process improvement.
 Outstanding communication, facilitation, mediation, and coaching skills.
 Consistently demonstrates clear and concise written and verbal communication skills.
 Good interpersonal skills, ability to interact with Senior Management
 Highly self-motivated with a strong sense of initiative
 Benefits: 
  We offer a competitive compensation and benefits package including:

 Health/Dental/Vision
 Paid Time Off (PTO)
 401 (K) and employer match
 Company provided Life Insurance & Long Term Disability

 Additional benefits with Republic Finance include:

 Employee of the Month Program
 Philanthropic support for charities such as Juvenile Diabetes Research Foundation and the American Cancer Society
 Professional offices with a friendly team environment



 Republic Finance, LLC is an Equal Opportunity Employer and does not discriminate on the basis of race, sex, color, religion, national origin, age disability or veteran status in employment opportunities and benefits. Republic Finance, LLC maintains a Drug-Free Workplace."
Senior Data Platform Engineer (Hybrid),Apixio,"Hybrid remote in Dallas, TX",Posted 14 days ago,"$120,000 - $185,000 a year",https://www.indeed.com/rc/clk?jk=eb67c36d43083536&fccid=d709010bf50f2482&vjs=3,"Who We Are:
 Apixio is advancing healthcare with data-driven intelligence and analytics. Our Artificial Intelligence platform gives organizations across the healthcare spectrum the power to mine clinical information at scale, creating novel insights that will change the way healthcare is measured, care is delivered, and discoveries are made.



The Opportunity at Apixio:
 Apixio is a healthcare analytics company that leverages artificial intelligence and big data to improve healthcare outcomes. We are seeking a talented Senior Data Platform Engineer to join our team and help build and maintain our data platform. As a Senior Data Platform Engineer at Apixio, you will work on a fast-paced team of talented individuals who are dedicated to improving healthcare. You will have the opportunity to lead and mentor other data platform engineers and work closely with data analysts and data scientists to enable both analytics and AI models.
 Who You Are:
 You are an experienced data platform engineer with multiple years of ETL experience and coding expertise in Scala, Java, and Python. You have experience working with one or more of Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks. You are familiar with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture. You are a problem solver who enjoys collaborating with others to build innovative solutions. You have excellent communication and collaboration skills.
 What You Will Own:
 As a Senior Data Platform Engineer at Apixio, you will own the maintenance, design, and implementation of our data platform. This will include data pipelines, ETL workflows, OCR, developing and optimizing our internal data lakehouse, and ensuring data security and compliance with regulatory requirements. You will work closely with the application teams, data analysts, and data scientists to provide data access and enable analytics. You will automate processes and develop tools to improve the efficiency of the data platform. You will lead and mentor other data platform engineers, stay current with emerging trends and technologies in data engineering, and work to incorporate them into the Apixio data platform.
 In addition to the above, you will also be responsible for:

Maintaining, designing, and implementing scalable, reliable, and high-performance data architectures
Developing and implementing best practices for data integration, data quality, and data governance
Ensuring the scalability, reliability, and security of the data platform
Collaborating with other teams at Apixio to understand their data needs and develop solutions to meet those needs
Managing and optimizing cloud-based infrastructure for the data platform
Keeping up-to-date with emerging technologies and trends in data engineering and healthcare tech

What You Bring to the Table:

Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
5+ years of experience in data platform engineering with coding expertise in Scala, Java, and Python
Experience with technologies such as Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks
Experience with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture
Experience leading and mentoring other engineers
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Nice to have: healthcare experience and familiarity with healthcare tech standards like x12 EDI, HL7 FHIR, CCDA, and V2 messaging



   The salary range below is for Base Salary. Total compensation also includes benefits and variable compensation. Compensation will be determined based on several factors including, but not limited to, skill set, years of experience, and the employee’s geographic location.
  
 Base Compensation

    $120,000—$185,000 USD
  


 We recognize that people come with experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Your skills and background may be more translatable to this role than you initially thought. Allow us the opportunity to get to know you. Please let us know if you require accommodations during the interview process.
 What Apixio can offer you:

Meaningful work to improve healthcare
Competitive compensation
Exceptional benefits, including medical, dental and vision, FSA
401k with company matching
Generous vacation policy
A hybrid work schedule (2 days in office & 3 days work from home) (Note: If the position is designated as REMOTE it will stay REMOTE)
Modern open office in beautiful San Mateo, CA; Los Angeles, CA; San Diego, CA; Austin, TX and Dallas, TX
Subsidized gym membership
Catered, free lunches
Parties, picnics, and wine-downs
Free parking

We take your privacy very seriously. Please review our privacy policy to see exactly how we protect your information here
 We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.
 Apixio will consider for employment qualified applicants with criminal histories pursuant to the San Francisco Fair Chance Ordinance subject to the requirements of all state and federal laws and regulations.
 If you are a recruiter or placement agency, please do not submit resumes to any person or email address at Apixio prior to having a signed agreement from Talent Acquisition. Apixio is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Furthermore, any resumes sent to us without an agreement in place will be considered your company's gift to Apixio and may be forwarded to our recruiters for their attention and no fee will be paid.


 LI-RB1"
Data Center Design Mechanical Engineer,Garver,"Dallas, TX 75202 (City Center District area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=c6d2ab416cf22669&fccid=6abbc3f4cb2d5388&vjs=3,"As part of the Buildings Group for Garver, this 
  Mechanical Engineer will be responsible for the delivery of various Data center/Mission Critical projects. Specifically, this responsibility will include a variety of projects related to investigating, planning, design, and commissioning of:
 

Air distribution, hydronic and automated temperature controls systems
Chiller plants
Alternative energy solutions


  In addition, this role will include coordination with other support disciplines, and coordination with regional offices. Assisting project managers in management of projects. Following quality assurance and quality control plans. Travel to project locations will be required.
 


Abilities and attributes needed for this position:


Collaborate with multi-disciplinary project teams to develop drawing and specification documents.
Design complex electrical medium voltage and low voltage distribution systems, as well as electrical building systems.
Perform project management activities, including proposal writing, budgeting, and client interactions.
Attend and lead client meetings.
Ability to collaborate with internal and external design, client, and construction team members.


Requirements:


Bachelor’s degree in mechanical engineering or architectural engineering with a focus on mechanical building systems.
Minimum of 5 years of experience in designing mechanical systems for mission-critical/data center buildings.
Registered Professional Engineer (PE) certification.
Strong knowledge of mechanical system and energy codes.
Proficiency in Building Information Modeling using Revit.


Preferred Skills:


Experience with short circuit coordination and arc flash studies.
Background in mission-critical/data center projects.
Additional credentials such as LEED and Uptime ATD are preferred.



Grow With Us



    Garver offers its employees programs such as company-paid professional memberships, company support for industry licenses and continuing education opportunities that foster a progressive atmosphere. Garver provides the tools, resources, and environment to develop leaders, stimulate ideas, and accomplish projects. By offering highly competitive salary packages, attractive benefits, and a comprehensive wellness program; Garver walks the talk when it comes to work-life balance.
   


    Founded in 1919, Garver is an employee-owned multidisciplined engineering, planning, and environmental services firm with more than 1,000 employees across the United States. Offering a wide range of services focused on aviation, buildings, construction, enterprise solutions, federal, survey, transportation, water, and wastewater, Garver sits in the top 100 of the Engineering News-Record's prestigious Top 500 Design Firms list and is consistently recognized as a best firm to work for. Learn more at GarverUSA.com.
   


Garver is committed to providing equal employment opportunities to all applicants and employees. Our employment practices are based upon an individual's capabilities and qualifications without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other category protected by law.




#LI-SM1"
Experienced Civil Engineer - Data Center (Remote),Olsson,"Remote in Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=fbaf4b68f16d4254&fccid=d19644165ec38b40&vjs=3,"Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
 Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.



 Job Description
  Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
 As an engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
 You may travel to job sites for observation and attend client meetings.
 We have multiple openings at varying experience levels due to growth of this team and will consider candidates interested in being located out of any of our Olsson office locations or working remotely.



 Qualifications
  You are passionate about:

 Working collaboratively with others
 Having ownership in the work you do
 Using your talents to positively affect communities
 Solving problems
 Providing excellence in client service

 You bring to the team:

 Strong communication skills
 Ability to contribute and work well on a team
 Bachelor's Degree in civil engineering
 At least 6 years of related civil engineering experience
 Proficient in Civil 3D software
 Must be a registered professional engineer

 Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
 As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:

 Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
 Engage in work that has a positive impact in communities
 Receive an excellent 401(k) match
 Participate in a wellness program promoting balanced lifestyles
 Benefit from a bonus system that rewards performance
 Have the possibility for flexible work arrangements

 Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
 #LI-LA1
 #LI-Remote"
Senior Civil Engineer - Data Center (Remote),Olsson,"Remote in Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=f553a3d181432b15&fccid=d19644165ec38b40&vjs=3,"Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
 Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.



 Job Description
  Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
 As an experienced engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
 You may travel to job sites for observation and attend client meetings.
 We have multiple openings at varying experience levels due to growth of this team and will consider candidates interested in being located out of any of our Olsson office locations or working remotely.



 Qualifications
  You are passionate about:

 Working collaboratively with others
 Having ownership in the work you do
 Using your talents to positively affect communities
 Solving problems
 Providing excellence in client service

 You bring to the team:

 Strong communication skills
 Ability to contribute and work well on a team
 Bachelor's Degree in civil engineering
 At least 6 years of related civil engineering experience
 Proficient in Civil 3D software
 Must be a registered professional engineer

 Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
 As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:

 Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
 Engage in work that has a positive impact in communities
 Receive an excellent 401(k) match
 Participate in a wellness program promoting balanced lifestyles
 Benefit from a bonus system that rewards performance
 Have the possibility for flexible work arrangements

 Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
 #LI-LA1
 #LI-Remote"
Senior Electrical Engineer - Data Center Team,Harley Ellis Devereaux,"Dallas, TX 75201",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=e3d59ea06d1bc9f9&fccid=b8b7adf11cdd6e15&vjs=3,"PRIMARY FUNCTION
 Primary responsibilities include providing for the design and technical systems for large, complex projects from schematics through construction administration.  TYPICAL DUTIES

 Responsible for building system concepts and documentation process from schematics through construction administration phases within the discipline.
 It is expected that the Electrical Engineer in regular coordination with Supervisor in project development and performance.
 Electrical Engineer shall have a proficient knowledge of primary sectors of design including:
      
 Power Distribution
 Lighting and Controls
 Fire Alarm
 Specification writing and editing.
 Energy conservation and sustainability/report technical writing.

 Function as Project Discipline Team Leader on large, complex projects when assigned.
 Responsible for directing and overseeing project team members within their own discipline. Level 4 is responsible for monitoring and executing team compliance with project budget.
 Prepare and monitor project status reports.
 Coordinate with Electrical Engineering Discipline Leader in preparing project Man-hour Estimates.
 Prepare and document required code research for project.
 Participate in the development of the project cost model and design to it.
 Develop and maintain Electrical Basis of Design Journal throughout the life of the project.
 Conduct material and product research as required for project development.
 Attend project meetings as necessary.
 Interface with other Project Discipline Team Leaders.
 Respond to client questions/requests within 24 hours.
 Document interpretation and submittal reviews during bidding and construction administration.
 Monitor and mentor staff to produce quality architectural and engineering services within a project team concept.
 Perform other duties as assigned by the supervisor.

 TYPICAL DUTIES - NON-PROJECT RELATED

 Mentor support engineers to help prepare them for greater project responsibilities.
 Provide regular feedback to the Electrical Engineering Discipline Leader on performance of support engineers.
 Identify opportunities to improve quality and services, and participate in the implementation.
 Participate in in-house training activities.
 Participate in related professional organizations and activities.
 Contribute to department discussions on field feedback, lessons learned, new technology or developments encountered.
 Offer recognition to those staff members observed doing excellent work and/or things beyond their job expectations.

 SKILL, KNOWLEDGE, EDUCATION AND EXPERIENCE

 Proven experience working on mission critical/data center projects.
 Bachelor's degree in Electrical Engineering and PE licensure required.
 LEED Accredited preferred.
 Minimum six years' experience in A/E industry preferred.
 Extensive knowledge of all aspects of professional services from schematic design through project close-out, including design and technical expertise.
 A comprehensive understanding of the coordination aspects and related requirements of all design disciplines.
 Proficiency in short circuit and selective coordination calculation. Utilization of EDSA, SKM or other analytical modeling software preferred.
 Excellent written/ verbal communication and strong organizational skills.
 Possess the ability to motivate and mentor staff, and delegate work assignments.
 Computer and CADD/Revit literate.
 Very detail-oriented, self-motivated, enthusiastic and flexible.
 Ability to work well with others under deadline situations.

 PHYSICAL REQUIREMENTS

 Capable of traveling to and from project sites for attending client, project and construction meetings.
 Ability to access existing and new project sites for observation, investigation and evaluation purposes.
 Ability to use equipment for communication and documentation purposes.
 Visual acuity to perform responsibilities.

 #LI-JK1"
Lead Engineer- SAP BODS/Azure Data Engineer,Builders FirstSource,"Remote in Dallas, TX",Posted 12 days ago,"$110,025 - $150,000 a year",https://www.indeed.com/rc/clk?jk=63ff747ddaa573e0&fccid=eb0325fdd6d51307&vjs=3,"This role will focus first and foremost on implementing data migration solutions using SAP BODS and is responsible for the timely delivery of high-quality, reliable and cost-effective integration solutions from a diverse set of sources into SAP S/4 Hana among other targets including Azure Cloud. This includes defining design guidelines using best practices related to SAP BODS. The engineer will primarily be responsible for coding/building pipelines, the complexity of which is driven by business need. As the maturity of the environment increases, this role will be responsible for helping to develop our cloud data platform. As such the role will require cloud architecture and data engineering expertise. 
ESSENTIAL DUTIES AND RESPONSIBILITIES 


Leads complex projects from end to end.
Drives analyses from technical design through completion; guides others on working in a dev ops structure/ environment.
Acts as point of escalation/ deep SME resource to manage admin failure and messaging events.
Develops complex data extracts, applications, and ad-hoc queries as requested by internal and external customers using ETL toolsets.
Conducts complex ETL processes and troubleshooting.
Defines requirements for ETL solutions, ensure data quality and match-back to core systems transaction results.
Identifies and documents process for building data models; ensures data is clean and usable through validation, testing and troubleshooting.
Leads data architecture and pipeline development to store and process high-volume data sets.
Identifies immediate and larger scope problems through thorough code reviews.
Tests and reviews code to optimize scale, velocity, and reliability of deliverables.
Collaborates with Data Scientists to ensure architectural and project specification needs are being met.
Designs solid, robust, and extensible data warehouse system, and prepares data to support key business flows.
Defines standardization and reuse opportunities, abstractions and patterns across classes and components to streamline design and development of data integration solutions.
Determines data mapping tools and techniques, reviews, and highlights process improvement opportunities.
Devises new methods and procedures for collecting data; performs complex data analyses.
Designs and creates reporting and visualization for communicating data insights, underlying principle, and linkage of data to business outcomes; presents findings to senior stakeholders.
 SUPERVISORY RESPONSIBILITIES 
This job has no supervisory responsibilities. 
MINIMUM REQUIREMENTS 
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. 


Bachelor's degree from an accredited college/university.
7+ years relevant work experience.
Or an equivalent combination of education and experience.
 COMPETENCIES 


Evaluates Problems: Evaluates and analyzes different types of information objectively to identify appropriate solutions; writes fluently, establishing the key facts clearly and interprets numerical data effectively.
Technical Communication/ Presentation: Communicates with clarity and precision, presenting complex information in a concise format that is audience appropriate.
Adjusting and Driving Change: Takes a positive approach to tackling work and embraces change; invites feedback relating to performance and deals constructively with criticism. Identifies the need for and drives change when required to achieve objectives.
Focuses on Customers: Understands and anticipates customer needs and takes action to provide high-quality products and services to exceed expectations.
Demonstrates Business Acumen: Demonstrates working knowledge of market, economic, legal, and regulatory environments and how they impact the business.
Agile Best Practices: Understands how agility is leveraged in IT ways of working. Adopts agile best practices as appropriate throughout the assigned work lifecycle. Responds to feedback quickly based on comments of internal and external customers and needs of the market.
Bias for Action: Takes initiative and identifies what needs to be done and acts without waiting to be asked. Executes work in a timely manner. Suggests improvements to current ways of working.
 BFS COMPETENCIES 


Business and Financial Acumen
Demonstrates depth of understanding for the P&L and financial analysis
Teaches business and financial acumen to others.
Understands KPIs and how BFS makes money.
Knows the different business segments and how they relate to one another.
Understands customer sales and engagement.
Demonstrates functional and/or technical expertise.
Understands complex issues and demonstrates problem solving skills.
Understands how to maximize business results regardless of industry cycle.
Results Driven
Holds self and others accountable.
Communicates and sets clear goals with plans to deliver.
Manages competing priorities effectively.
Demonstrates appropriate urgency.
Drives to exceed expectations in alignment with our BFS SPICE values.
Embraces and follows best practices.
Demonstrates self-starter, can-do attitude.
Strategic Thinking and Decision Making
Leverages resources and teams around them to solve problems and create mutually beneficial outcomes.
Demonstrates willingness and courage to make tough decisions in a timely manner.
Balances short-and-long term priorities
Demonstrates proactive versus reactive thinking.
Asks questions to identify root cause and analyze situations more accurately.
Servant Leadership
Demonstrates humility by putting others first.
Builds trust-based relationships.
Leads by example with kindness and respect.
Collaborates well across all areas of the business.
Advocates for others
Actively listens to understand the meaning and intent of what the other person is communicating.
Demonstrates authenticity and encourages others to do the same.
Emotional Intelligence
Demonstrates situational awareness – knows when and how to adjust leadership style in different situations.
Demonstrates self-awareness – understands strengths and weaknesses.
Demonstrates empathy – puts themselves in other’s shoes.
Assumes positive intent.
Develops and Leads Others
Drives alignment through clear communication of vision, goals, and expectations.
Invests time on a regular basis in performance feedback and developmental conversations.
Fosters a respectful and inclusive environment.
Empowers, motivates, and inspires others.
Coaches and mentor others for their development.
Guides and persuades others to deliver positive outcomes.
Growth Mindset
Demonstrates a growth mindset; takes appropriate risks, fails fast and forward, learns from mistakes.
Perseveres and champions growth, even in the face of resistance, ambiguity, or possible failure.
Thinks like an owner with an entrepreneurial spirit.
Demonstrates and encourages intellectual curiosity.
Continuous learner; seeks opportunities and knowledge for personal and professional growth.
Sees possibilities over problems – actively seeks solutions.
Innovation
Encourages out-of-the box thinking to create new ways of doing things.
Continuously seeks to improve and simplify pain points in the business.
Anticipates, embraces, and leads change.
Develops and executes breakthrough strategies.
Integrity
Does the right thing even under challenging circumstances?
Communicates with honesty.
Consistently treats others fairly and equitably.
Demonstrates reliability and does what they say they will do.
Conducts tough conversations and delivers difficult messages with kindness and respect.
 WORK ENVIRONMENT / PHYSICAL ACTIVITY 
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 


Subject to both typical office environment and outside locations with temperature and weather variations.
Must be able to lift and carry up to 25 pounds.
Occasional travel may be required.
 In addition to the base wage listed, this position is also eligible to earn an annual bonus subject to changes in plan design and documents and in accordance with applicable law. Eligibility and the amount of the bonus varies based on overall company success, thresholds met and other terms and conditions of the Company’s active bonus policy for the respective year. 

 Full-Time Team Members are eligible for company benefits, including • Three Medical plan options • Dental & Vision • Critical Care • Accident & Hospital insurance 


Flexible Spending Accounts for Health & Dependent Care • Health Savings Account • 401(k) with company match • Vacation & Sick Time • Paid company holidays
Company Paid Life & AD&D • Supplemental Life • Short & Long Term Disability • Bereavement • Paid Parental Leave • Team Member Assistance Program
 All benefits are subject to change pursuant to state and federal guidelines. 

 Builders FirstSource is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status or status as an individual with a disability. 

 In compliance with the ADA Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position with Builders FirstSource, please call (214) 765-3990 or email: ADA.Accommodation@bldr.com. Please do not send resumes to this email address - it is intended only to be used to request an accommodation in submitting an application for a job opening. 

 https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm 

 EEO THE LAW - English/Spanish 
EEO IS THE LAW - SUPPLEMENT - English/Spanish 
Pay Transparency Provision - English/Spanish"
Licensed Electrical Engineer - Arc Flash - Data Center (Remote),Olsson,"Remote in Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=5769a88639e3d6e9&fccid=d19644165ec38b40&vjs=3,"Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
 Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.



 Job Description
  As an Electrical Engineer, you will work directly with some of the world’s largest technology companies and other mission-critical clients. You will serve as an electrical engineer on projects, design calculations, write technical reports, and prepare documents. Experience in performing short circuit analysis and producing arc flash studies is required. You will also coordinate with other Olsson teams, professional staff, technical staff, and clients. You may travel to job sites for observation and attend client meetings.
 We currently have one opening and will consider candidates interested in being located in most locations across the United States.



 Qualifications
  You are passionate about:

 Working collaboratively with others
 Having ownership in the work you do
 Using your talents to positively affect communities
 Electrical Engineering knowledge

 You bring to the team:

 Strong communication skills
 Ability to contribute and work well on a team
 Ability to be a self-starter to take on a variety of tasks to best serve the client and their project work
 Investigation and troubleshooting of problems to find solutions
 Ability to contribute and work well on a team
 Bachelor's Degree in electrical engineering
 5+ years or related electrical engineering experience
 Registered professional engineer (PE) required
 SKM and ETAP software experience is preferred

 Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
 As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:

 Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
 Engage in work that has a positive impact in communities
 Receive an excellent 401(k) match
 Participate in a wellness program promoting balanced lifestyles
 Benefit from a bonus system that rewards performance
 Have the possibility for flexible work arrangements

 Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
 #LI-MP1
 #LI-REMOTE"
Senior Electrical Engineer - Arc Flash - Data Center (Remote),Olsson,"Remote in Dallas, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=edba3ee290ff1e04&fccid=d19644165ec38b40&vjs=3,"Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
 Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.



 Job Description
  As an Electrical Engineer, you will work directly with some of the world’s largest technology companies and other mission-critical clients. You will serve as an electrical engineer on projects, design calculations, write technical reports, and prepare documents. Experience in performing short circuit analysis and producing arc flash studies is required. You will also coordinate with other Olsson teams, professional staff, technical staff, and clients. You may travel to job sites for observation and attend client meetings.
 We currently have one opening and will consider candidates interested in being located in most locations across the United States.



 Qualifications
  You are passionate about:

 Working collaboratively with others
 Having ownership in the work you do
 Using your talents to positively affect communities
 Electrical Engineering knowledge

 You bring to the team:

 Strong communication skills
 Ability to contribute and work well on a team
 Ability to be a self-starter to take on a variety of tasks to best serve the client and their project work
 Investigation and troubleshooting of problems to find solutions
 Ability to contribute and work well on a team
 Bachelor's Degree in electrical engineering
 8+ years or related electrical engineering experience
 Registered professional engineer (PE) required
 SKM and ETAP software experience is preferred

 Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
 As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:

 Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
 Engage in work that has a positive impact in communities
 Receive an excellent 401(k) match
 Participate in a wellness program promoting balanced lifestyles
 Benefit from a bonus system that rewards performance
 Have the possibility for flexible work arrangements

 Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
 #LI-MP1
 #LI-REMOTE"
