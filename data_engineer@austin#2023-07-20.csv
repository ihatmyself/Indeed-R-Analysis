job_title,company,job_location,post_date,salary,job_url,job_description
Data Engineer II,2K,"Austin, TX",,,https://www.indeed.com/rc/clk?jk=ce0e3a617afe0145&fccid=824f64d0ff3c8c8f&vjs=3,"Who We Are
 2K is headquartered in Novato, California and is a wholly owned label of Take-Two Interactive Software, Inc. (NASDAQ: TTWO). Founded in 2005, 2K Games is a global video game company, publishing titles developed by some of the most influential game development studios in the world. Our studios responsible for developing 2K's portfolio of world-class games across multiple platforms, include Visual Concepts, Firaxis, Hangar 13, CatDaddy, Cloud Chamber, 31st Union, and HB Studios. Our portfolio of titles is expanding due to our global strategic plan, building and acquiring exciting studios whose content continues to inspire all of us! 2K publishes titles in today's most popular gaming genres, including sports, shooters, action, role-playing, strategy, casual, and family entertainment.
 Our team of engineers, marketers, artists, writers, data scientists, producers, thinkers and doers, are the professional publishing stewards of our growing library of critically-acclaimed franchises such as NBA 2K, 2K PGA, Battleborn, BioShock, Borderlands, The Quarry, The Darkness, Mafia, Sid Meier's Civilization, Marvel's Midnight Suns, WWE 2K, and XCOM.
 At 2K, we pride ourselves on creating an inclusive work environment, which means encouraging our teams to Come as You Are and do your best work! We encourage ALL applicants to explore our global positions, even if they don't meet every requirement for the role. If you're interested in the job and think you have what it takes to work at 2K, we encourage you to apply!
 What We Need
 Data is the lifeblood of everything we do. We are a data-driven organization providing data products and insights throughout the 2K organization and related companies. We work directly with a wide variety of partners, from executive management to studio heads to analysts. We also partner in providing data to our players in the form of campaigns, promotions, and machine learning-based data products and recommendations.
 What You Will Do

You will work in the capacity of a Data Engineer focusing on the data deliverables for Business and Studio Ingesting third-party data into our data lake and warehouse.
You will work with different vendor APIs and services. Working with vendor partners to understand the data they are providing, and document the data sets, lineage, and flows.
Designing and building data models to support Data Science, Business Intelligence, and downstream data sets. Building APIs and data products to better integrate data throughout our systems and processes. Monitoring the data pipelines and communicating any issues to leadership and partners.
Working closely with business partners and developers to ensure proper requirements are documented and agreed to for our different initiatives. Partnering with developers and leadership to coordinate cross-function and cross-team efforts

What Will Make You a Great Fit

3+ years of professional experience as a Data Engineer in a fast-paced environment. An expert developer using SQL and SQL-like query languages
Have a strong understanding of different data modeling methodologies (Kimball, Inmon, Data Vault). You understand when to apply the correct techniques to solve different problems on different technologies in our stack.
Have deep expertise in Python and have experience organizing Python-based projects. You've built software services and APIs using Python frameworks such as Flask.
Experience with both proprietary and open-source big data technologies and platforms (Snowflake, Vertica, Hive, Spark, Presto, Airflow).
Worked in a cloud environment (AWS, Azure, GCP).
Experience defining and crafting automated unit and integration testing frameworks for data projects.
Functional understanding of different privacy and compliance practices around data (GDPR, CCPA).
Excellent collaborator by tailoring your communication for different audiences and ensuring effective communication between developers, partners, and leadership.

Bonus Points

Video game and/or entertainment and media industries

As an equal opportunity employer, we are committed to ensuring that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform their essential job functions, and to receive other benefits and privileges of employment. Please contact us if you need reasonable accommodation.
 Please note that 2K Games and its studios never uses instant messaging apps or personal email accounts to contact prospective employees or conduct interviews and when emailing, only use 2K.com accounts. 
#LI-Onsite #LI-BN1"
Data Engineer,Informa Tech,"Austin, TX",,,https://www.indeed.com/rc/clk?jk=4e38cab1a91432ef&fccid=723cd490904b231f&vjs=3,"Company Description
  Informa Tech informs, educates and connects the technology community through world-class research, training, events and media. A vibrant community of over 1,000 colleagues across 19 locations, we’re dedicated to inspiring the tech community to design better, to build better.
 Which is why we champion you, the individual, to make an impact through change and innovation. Life at Informa means more freedom and fewer barriers. Our entrepreneurial spirit runs deep, encouraging our team to stick their hands up to spark innovation, or get stuck in to deliver amazing results. It’s time to feel the impact of your work and enjoy making a difference. Are you ready to start your future, today?



 Job Description
  Purpose of Role
 This Data Engineer is a member of the Data & Analytics Community at Informa Tech and is a unique role among a group of Data Analysts in the Central Data & Analytics team within Informa Tech’s Strategy department. They will develop and maintain solutions to support the collection, storage, enrichment, and delivery of data from across our varied businesses practices. The successful candidate will work with cross-functional teams to help design, build, test, deploy, maintain, and enhance pipelines and tools to improve business data accessibility, quality, transparency, and understandability.
 Key Responsibilities

 Developing and maintaining tools, processes, and pipelines needed to successfully (reliably, on time, and at high quality) deliver critical data products from most business data domains (audience development, event registration and sponsorship, client and product—e.g. SalesForce, HR/people, and more) into internal user-facing platforms.
 Supporting all phases of the software development life cycle, including plan, design, implement, test/integrate/deploy, and maintain
 Evaluating requests for new datasets by assessing their feasibility, development effort, and value
 Managing relationships with technical (e.g. data analysts, data product managers, marketing platform admins, other data/technology teams) and business user (e.g. marketing, sales, events, content) stakeholders of all levels, including identifying and resolving risks/issues, prioritizing work, and organizing/attending stakeholder meetings and other communications
 Acting as an interface between the Informa Tech Data Analytics Community and other data engineering colleagues/teams within Informa Tech and Informa Group.
 Implementing best practices for data tooling, testing, and implementation; training internal users, coordinating with peers throughout Informa Tech and Informa Group to share knowledge and best practices; participate in relevant data governance processes
 Regularly soliciting, evaluating, and prioritizing feedback to continuously improve data products and processes
 Staying up to date on current and emerging industry best practices, tools, and techniques




 Qualifications
  Experience & Qualifications

 Familiarity with the entire data life cycle
 Demonstrable success building data pipelines (ingestion, cleansing, and ETL/ELT) with cloud platforms (AWS, Power BI Service—Power Query and data flows/data sets/data marts—any Microsoft Fabric experience a plus, Alteryx, Apache Airflow, etc.)
 Successful track record in building and maintaining data tools in SQL
 Experience in scripting/programming languages (e.g., Python, R, VBA, VB.Net, C#, etc.)
 Familiarity with reporting and visualization tools such as Power BI and Tableau
 Experience in documenting processes, procedures, and standards
 Prior experience collaborating with Business Analyst and Product Owner roles to effectively develop products (especially data products) a plus
 Prior experience working under Agile frameworks (Kanban, Scrum) and familiarity with related tools (Jira, Rally/CA Agile Central, Smartsheet, etc.) a plus
 Experience and comfort interacting with stakeholders and consumers across the business
 Process improvement qualification or experience (e.g., Lean/Six Sigma) a plus
 Academic or professional experience related field(s) (e.g., Data Analytics, Statistics, Software Engineering, Computer Science, Information Technology, Mathematics, Engineering) a plus

 Knowledge & Skills

 A genuine affinity for problem solving - having an inquisitive and positive mindset to overcome complex challenges
 Entrepreneurial spirit – able to constructively question and criticize existing processes, identify potential for new processes, advocate for identified enhancements to convince stakeholders, and proactively pursue execution (including self-organizing with related colleagues)
 Product-centric mindset (as opposed to project-centric) to deliver value early and continuously, including understanding and embracing the product life cycle
 Strong attention to detail and proactive in highlighting and resolving errors/issues
 Communication and presentation skills and ability to collaborate effectively across groups and time zones

 Additional Information
  Just as no two days are the same, at Informa Tech we recognise that no two people are the same, putting diversity and inclusivity at the heart of what we do. This doesn’t happen by chance. We actively work to create a shared culture. It’s a place where individuals bring their own experience and insights to discover new opportunities and build a varied career. We want you to thrive as part of a fantastic community. We champion you."
Software Engineer - Data,SpyCloud,"Remote in Austin, TX",,,https://www.indeed.com/rc/clk?jk=8a2afee93e316671&fccid=f1381f0c25abcd2f&vjs=3,N/A
Data Engineer,OneSource Regulatory,"Austin, TX 73301 (St Edwards area)",,,https://www.indeed.com/rc/clk?jk=1b88651c1c70c2ec&fccid=13c11dac3d323b9a&vjs=3,"Company Introduction
 OneSource Regulatory Technology hosts a number of innovative solutions to enhance job performance in the Pharmaceutical space. OSR Technology is looking for an experienced and dedicated data engineer to join our product solutions team!

 
Job Description
 OneSource Regulatory is trying to identify a full-time contractor with at least 4+ years of experience to assist us with ongoing R&D projects.
 We are looking for a data engineer to pull data from various sources and do all the necessary steps to clean, normalize, possibly annotate, and finally load the data into databases. The candidate should be able to develop and implement a strategy for testing the data integrity of the collected data. This role requires extreme attention to detail to ensure data quality is top priority.

 Responsibilities

Well versed in parsing and synthesizing of XML and/or JSON documents.
Curating of data that can involve some intermediate to advanced web scraping. (data may need to be fetched via SFTP, FTP, Wget, Curl, REST APIs, GraphQL queries from spots on the Internet)
Proficiency with Linux command line and various simple tools, such as grep, wc, sed, awk, find, ls, cat, piped commands and possibly some very light Bash shell scripting, setting up crontab schedules and programs
Must have basic knowledge of SQL with the following databases: PostGres, MySQL, Google BigQuery
Must have basic knowledge of No-SQL database knowledge such as MongoDB or similar
Familiarity with basic Cloud technology such as storage buckets, cloud serverless functions
Must have experience extracting text and images from PDF files
Knowledge of Puppeteer or other automatable web client technologies
Understanding JavaScript, HTML/CSS and HTTP methods (for understanding page structure for web scraping)


Skills

Solid experience with Python and Python Libraries such as Pandas, requests, etc
Skill set should match up with required responsibilities listed above
Strong English skills (e.g. grammatical analysis and rhetorical structure)
Team Player
Great communication skills



 Bonus Skills

Experience within the Pharmaceutical Space
Ability to expose data via C# NETCore and/or GraphQL
Google Cloud Platform (Cloud Buckets, Google Cloud Functions (.NET, Python, Node.JS))
Ability to parallelize data manipulation and scraping via Python multi-threading, etc.
Python BeautifulSoup
Scrapy
Docker (setting up Kubernetes style processing if warranted for data scraping/data ingestion/normalization)
Multithreading concepts"
Product Data Engineer,uStudio,"Austin, TX",,,https://www.indeed.com/rc/clk?jk=66fa55b8ae7bcd4e&fccid=1072196173bf7264&vjs=3,"We are looking for an experienced Product Data Engineer.
 uStudio seeks an energetic and creative data engineer to join our product and platform team and help us as we build a rich and intuitive suite of media products atop a proven enterprise media platform.




Send résumé to jobs@ustudio.com












As a company, our goal is straightforward: evolve enterprise communication beyond traditionally text-based solutions through our secure media applications including podcasting, live streaming, and media distribution. As a development team, we support this goal by building a family of web and mobile applications and intuitive APIs. We employ test-driven development and pair-programming to ensure our code is robust and iteration-friendly while collaborating to identify and refine solutions to our customers’ problems.
 Reporting is a fundamental part of our value to our customers. Video and audio experiences offer a deeper understanding of viewer engagement than any other medium, and we need our products’ analytics tools to demonstrate that clarity and power.
 We:

Provide competitive salaries with great benefits
Work in small pair-programming teams with continuous, agile releases
Value test-driven development
Strive to provide the best tools for our developers
Prioritize API development and extensible solutions
Enjoy exploring new technologies without blindly following trends
Extensively utilize (and contribute to) open source projects
Believe that diversity of experience, perspectives, and background leads to a better workplace and better products

You:

Desire to build out modern, intuitive, and comprehensive reporting products
Think beyond pure numeric representation and consider complete product experience when designing data solutions
Have experience in one or more modern language(s). (Python, Javascript, and SQL are a plus.)
Have experience with a variety of databases
Have experience with one or more modern data / ML frameworks
Eagerly ask questions and dive into codebases to get the necessary context for improving and aggregating metrics
Learn new technologies rapidly (and love doing so)
Are comfortable building clients for various API standards (REST, RPC, etc.)
Deploy your own code (or are excited to start)
Have a proven track record delivering strong, stable software"
Data Engineer,"Phunware, Inc","Austin, TX 78757 (Allandale area)",,,https://www.indeed.com/rc/clk?jk=cc1f9a67bd5bb5fe&fccid=7dd797e7bdfd7d63&vjs=3,"Since our founding in 2009, we’ve relentlessly worked toward a vision of a future powered by Phunware. We spend our days obsessing over how best to design, build, launch, promote and support branded apps that engage, compel and delight the world’s most discerning audiences. For over a decade, we’ve helped Fortune 5000 businesses throughout the mobile app lifecycle with data-backed decisions at every step.
Everything You Need to Succeed on Mobile — Transforming Digital Human Experience
Phunware, Inc. (NASDAQ: PHUN), is the pioneer of Multiscreen-as-a-Service (MaaS), an award-winning, fully integrated enterprise cloud platform for mobile that provides companies the products, solutions, data, and services necessary to engage, manage and monetize their mobile application portfolios and audiences globally at scale. Phunware’s Software Development Kits (SDKs) include location-based services, mobile engagement, content management, messaging, advertising, loyalty (PhunCoin & Phun) and analytics, as well as a mobile application framework of pre-integrated iOS and Android software modules for building in-house or channel-based mobile application and vertical solutions. Phunware helps the world’s most respected brands create category-defining mobile experiences, with more than one billion active devices touching its platform each month.
If you share our passion for innovative mobile app experiences and dream of a world empowered by seamless, one-to-one interactions, we want to hear from you. Get in touch with us today—our Phamily always has room for one more!
Job Summary:
Phunware is seeking a Data Engineer with hands-on experience creating, deploying and optimizing large-scale data systems.
The ideal candidate will bring strong technical skills and be proactive, responsive and very comfortable dealing with ambiguity. He or she will also bring good experience with Big Data systems/technologies and have a strong track record of deployment, maintenance, and optimization of production code.
The ideal candidate is someone who combines an understanding of business processes with knowledge of both client and server-side technical requirements in mobile software projects. They will put the customer first, quickly build strong relationships, learn rapidly, and enjoy autonomy and problem-solving. They must be a gifted leader with a genuine passion for working with high-performance teams, extraordinarily organized., and have a strong work ethic. Additionally, the position may require travel both domestically and internationally.
What You’ll Do:

Create robust, high-volume production systems/architectures, and develop prototypes quickly
Work with development teams to design maintenance and support strategies
Create optimized workflows using relevant technologies (Spark, Elastic Search, Kafka, Oozie, Hadoop)
Create architectural workflows, diagrams, and specification documents to help define platform features/functionality
Perform experiments and analyze results to improve the performance and quality of algorithms
Work with product management and executive stakeholders to take detailed requirements and implement them using Agile Test Driven techniques
Work in an organized team-oriented environment with shared responsibilities


What You’ll Bring:

Bachelor’s Degree or higher in Computer Science or Computer Engineering; Master’s Degree preferred
Have previously worked in Big Data technologies and deployed in production environment
Strong experience in building highly scalable, available and responsive systems using open-source software tools and technologies
5-10 years of professional software development
5-8 years strong Java development experience
Good experience with REST API frameworks
Strong SQL skills
1+ years of professional software development experience with some of the big data technologies including: Spark, Map Reduce, Hive, HBase, Hadoop, Kafka, Impala, Cassandra
Experience in Elastic Search is highly desirable
Some experience with one or more of the following will be an added advantage: statistical analysis, machine learning, natural language processing, predictive modeling
Domain experience in one or more of the following:
Outstanding skills for interacting with people
Responsible, organized and hardworking with excellent communication skills
Must be living in the Irvine, CA or Austin, TX area or be able to immediately relocate


Desirable:

NoSQL or similar DB design/implementation experience with large number of records (i.e. 1 Billion+)
Experience with information retrieval, network programming and/or developing large software systems
Experience with cloud delivery platforms, ideally Amazon
Experience doing Test Driven Development (TDD), Continuous Integration (CI) and test automation
Open-source software contributions
Track record of success in a start-up or high-growth environment


Compensation and Benefits:

Fun, casual, fast-paced work environment filled with talented colleagues
Flexible paid time off
Competitive salary
Restricted Stock Units
Full range of benefits, including 401(k), medical, dental and vision coverage 


Candidates for this position must be authorized to work in the United States and not require work authorization sponsorship by our company for this position now or in the future."
Data Engineer,Purple Drive Technologies,"Austin, TX",,"$81,233 - $110,061 a year",https://www.indeed.com/company/Purple-Drive-Technologies/jobs/Data-Engineer-be8aefe516837053?fccid=b50885016c495d25&vjs=3,"Position: Data Engineer
Location: Austin, TX (Onsite)
Duration: Full Time
Job Qualifications / Requirements:

Deep knowledge and hands-on experience working with cloud data platform technologies in AWS, and scripting language Python


Strong experience in Spark


Strong experience in Kafka

Job Type: Full-time
Salary: $81,233.39 - $110,060.94 per year
Schedule:

8 hour shift

Ability to commute/relocate:

Austin, TX 78652: Reliably commute or planning to relocate before starting work (Required)

Experience:

Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)

Work Location: In person"
Data Engineer,Acrisure Technology Group,"Hybrid remote in Austin, TX",,,https://www.indeed.com/rc/clk?jk=dc3f6bcbc33c198e&fccid=5756f855d0a2475d&vjs=3,"Data Engineer
 Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
 Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
 Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
 At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
 We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
 The Role
 The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
 The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
 You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
 Here are some of the ways in which you'll achieve impact

Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams

You may be fit for this role if you have

Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes

Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
 Location: Austin, TX or Grand Rapids, MI
 We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.
 #LI-Hybrid"
"Software Engineer, Data",Acrisure Technology Group,"Hybrid remote in Austin, TX",,,https://www.indeed.com/rc/clk?jk=6791f9f7256ede81&fccid=5756f855d0a2475d&vjs=3,"Software Engineer, Data
 Hybrid Position (3 days per week average in Downtown Austin, TX office)
 About us:
 Acrisure Technology Group (ATG) is a fast paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
 At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
 We are a small team of extremely high caliber engineers with a diverse background across industries and technologies. Our engineers have worked at large companies like Google and Amazon, high frequency trading companies like Two Sigma and Jump Trading, and a variety of smaller startups, including successful startup founders.
 The Role:
 As a Software Engineer focused on Data Engineering, you'll be an essential part of the team building world-class software to transform the insurance industry. Working closely with engineers, researchers, product and design talent, and domain experts, you will design and implement new data processing systems that enable both cutting edge research, and high quality user experiences. As a successful candidate, you will take full advantage of state-of-the-art tools, conceiving of new ones when the right solution does not yet exist. You are driven by a passion for improving the world through technology and delighting users. Help us to turn vision into reality.
 Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
 Here are some of the ways in which you'll achieve impact:

Build efficient and reliable technology with a customer-first mindset.
Assist in designing and maintaining our tech stack, building when it makes sense, inventing when necessary, and upgrading as tools evolve.
Identify, adopt, and evangelize best practices.
Advocate for and identify creative implementations to optimize business impact.
Measure the effectiveness of new features, find and address performance issues, and drive continuous improvement.
Work collegially and effectively as we grow a world-class, diverse engineering team.

You may be a fit for this role if you have:

4+ years of experience in a Data Engineering or similar role using modern data processing techniques.
Exceptional knowledge of computer science fundamentals and software engineering principles, as well as expertise with distributed data processing.
Experience writing recurring data ingestion and validation pipelines across multiple data sources.
Strong knowledge of data architecture and modeling best practices.
Expert level SQL experience.
A product- and customer-focus, with a passion for delighting the end user using data.
An excitement about the opportunity to use data and AI to transform the insurance industry.
An entrepreneurial spirit and are action-biased; self-directed and excited to build something from scratch in a fast-paced, experimentation-driven environment.
Strong communication skills that allow you to be effective in a cross-functional team, as well as deliver data driven insights.
Ability to stamp out unnecessary complexity, harness necessary complexity, and make complex topics clear and accessible to others.
Empathy, kindness towards others, a positive attitude, and self-awareness.
A unique, non-traditional perspective to enhance our team's problem solving abilities.

Academics: Bachelor's degree in Computer Science or a related field, or equivalent experience.
 Location: Must be willing and able to work in the Austin,TX office an average of 3 days a week or as business needs permit.
 #LI-Hybrid #BI-Hybrid"
GCP Data Engineer,Sintesys,"Austin, TX",,"$70,000 - $120,000 a year",https://www.indeed.com/company/SINTESYS/jobs/Data-Engineer-33dfde94d71ff8c6?fccid=8521deabffa13eec&vjs=3,"Job Description - GCP Data Engineer
The Role: Emids are looking for an experienced Data Engineer who can help move on-prem Hadoop workloads to Google Cloud (GCP).
Responsibilities:
Export data from the Hadoop ecosystem to ORC or Parquet file
Build scripts to move data from on-prem to GCP
Build Python/PySpark pipelines
Transform the data as per the outlined data model
Proactively improve pipeline performance and efficiency ‘Must Have’ Experience: 4+ years of Data Engineering work experience
2+ years of building Python/PySpark pipelines
2+ years working with Hadoop/Hive 4+ years of experience with SQL Any cloud experience – AWS, Azure, GCP Experience with Data Warehousing & Data Lake
Understanding of Data Modeling
Understanding of data files format like ORC, Parquet, Avro
General requirements:
*Open to relocation (very important)
- Excellent communication skills
- Professional english
- Good attitude
- Committed
- Flexible
Job Type: Full-time
Salary: $70,000.00 - $120,000.00 per year
Benefits:

Health insurance
Life insurance

Experience level:

6 years
7 years

Schedule:

8 hour shift

Application Question(s):

Are you a US citizen or have a green card?

Education:

Bachelor's (Required)

Experience:

Google Cloud Platform: 6 years (Required)

Work Location: In person"
Data Engineer II (Data Analytics Specialist),Match Inc,"Austin, TX 78727",,$51 - $56 an hour,https://www.indeed.com/company/Match-Inc/jobs/Data-Engineer-2d156525189b38ee?fccid=9dbaeb5de04679f7&vjs=3,"Job Title : Data Engineer II (Data Analytics Specialist)
Location : Austin ,TX-78727
Duration : 12 Months
Summary:
AMP Video QC & Metadata Operations teams are looking for a highly
motivated and dedicated DataDev Specialist to support AMP Video QC & Compliance and
UMC (Universal Media Content) Metadata teams’ data and analytics needs. In this role, a
primary focus is on Apple’s growing Video Operations through content-centric data analytics
that provides insights into operational effectiveness, efficiencies, and excellence.
Key Qualifications :

Excellent interpersonal and communication skills


Strong experience and knowledge of data analytics with proficient SQL (Oracle,

Teradata, PostgreSQL, and SparkSQL)

Experience in Data visualization tools (Tableau or Splunk), if added languages

(Javascript, Python) or libraries (e.g., D3, Graphene) a plus

Experience in building robust and scalable data pipelines and ETL jobs


Experience or working knowledge with Big Data technologies such as Hadoop, Hive,

HDFS, Parquet, PySpark, and Spark desired

Experience in a typical Big Data programming language such as Python, Scala, or

similar languages

Experience in querying data through API (RESTful or GraphQL), using JSON,

ProtocolBuffers, or XML desired; if with API development experience a plus

Working knowledge of Kubernetes, AWS, or CI/CD development (e.g., Airflow, Jenkins)

a plus

Exposure or interest in the UX/UI frontend development (e.g., React, Typescript) or

Machine Learning (e.g., Scikit-Learn, Tensor Flow) a plus

An independent learner, insatiably curious, and a creative thinker


Ability to work both independently and within a team environment


Passion for movies, television, sports, fitness, or other video content

Description:
Develop dashboards, data pipelines/ETL jobs, and reporting to analyze and present data
related to video contents, asset reviews, metadata curations, and operational supports.
Closely partner with the internal teams within the AMP Video QC & Metadata Operations
organization to define metrics, KPIs, and automation strategy while meeting the teams’ data
and reporting needs.
Automate and optimize existing data processing workloads by recognizing complex data
structures and technology usage patterns and implementing solutions.
Focus on scale and efficiency — build and implement innovative data solutions and establish
best practices with a start-to-end workflow in mind.
Education & Experience:
Bachelor or Master's degree in a related field, such as Data Science, Computer Science,
Statistics, Mathematics, Business Analytics, Business Administration, or meaningful industry
experience prefe
Job Type: Contract
Salary: $51.00 - $56.00 per hour
Schedule:

8 hour shift

Ability to commute/relocate:

Austin, TX 78727: Reliably commute or planning to relocate before starting work (Required)

Experience:

Data analytics: 2 years (Required)
SQL: 2 years (Required)
Big data: 2 years (Required)

Work Location: In person"
Sr. Data Engineer (with ETL testing experience),Intellibee,"Austin, TX",,,https://www.indeed.com/rc/clk?jk=a1c9e29e0e55d7ab&fccid=d99d618a0bcbc8ee&vjs=3,"Experience in Cloud and building data flow/pipelines in mandatory – AWS is preferred but any cloud is fine.
 Should know how to program distributed systems
 Expected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.
 Knowledge of Test Automation preferred"
Data Engineer,Apple,"Austin, TX",,,https://www.indeed.com/rc/clk?jk=ab9c7a3ca7469fd1&fccid=c1099851e9794854&vjs=3,"Summary


      Posted: Mar 27, 2023
     


       Role Number:
       200471689



      Make a difference. The Operations team at Apple is looking for a dynamic and motivated candidate for the role of a Data Engineer. The role is an opportunity for a self-driven individual to utilize their business acumen, acquire process knowledge and apply analytical skills to deliver creative, value-added solutions to the Operations team at Apple. The engineer needs to be adept in the use of data forensics, statistical tools and techniques to be able to identify process gaps, root cause issues and recommend optimal solutions. Knowledge and hands on experience of quantitative analysis in the supply chain domain is expected. Ability to use statistical tools and techniques to identify process gaps, root cause issues and drive data driven decision making is desirable.
     









Key Qualifications




5+ years experience in quantitative analysis within the supply chain domain is expected.
Computational analysis using Snowflake, mySQL, Teradata, Python, Tableau, Business Objects, JMP, R, Matlab, SPSS and working knowledge of SAP/S4 data structures.
Experience with Snowflake, Teradata, and Tableau. Expert level fluency in SQL and Python required.
Applied Machine Learning experience, ie. regression analysis, time series, probabilistic models, bayesian statistics, etc.
Proven ability to perform effectively under dynamic conditions such as directional changes, tight deadlines and limited resources; organizational skills and the ability to multi-task are essential.
Commitment to keeping up to date with industry leading technologies, techniques, tools and best practices, including experience with SQL, Teradata, Python and Tableau.
Advanced understanding of data sources & relationships, reporting tools and systems knowledge, including experience in measuring and managing data quality.
Ability to translate technical content for non-technical audiences and vice-versa, and excellent verbal and written communication & presentation skills using data visualization applications is desired.










Description



       As a Data Engineer within Americas Business Process Re-Engineering team, you will: - Think strategically while executing on operational strategies in order to make definitive and measurable improvements for the operations teams. - Use statistical tools and techniques to identify process gaps, root cause issues, and drive data driven decision making; articulate findings to senior leadership. - Think out of the box and influence peers and management with data driven models using advanced analytics. - Leverage quantitative skills, select appropriate tools and techniques, create frameworks to drive policy changes, and implement corrective actions to improve the customer experience. - Apply knowledge of data concepts including data warehousing, data engineering and experience in managing a self-serve model. - Create and maintain reports and data models, and leverage data across complex hierarchies using multiple data sources. - Influence the architecture of core data models that fuel the Operations teams, guiding other data engineers to incorporate best practices in data engineering. - Perform testing to support system implementations and upgrades, and drive process improvement techniques in data quality.
      








Education & Experience



       BS/MS in Data Mining, Statistics, Machine Learning, Computer Science, Operations Research or related field required.
      








Additional Requirements"
Data Engineer II,Belcan,"Austin, TX 78758 (The Domain area)",,,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DXzDzZ1Oulz9LSjzVbF8otUHEujJfFPwzVdyJWZPnyGP21i8g1idx-A-BThzGW7o8YTukT4JHiY6Tb-lK8SJhlYqPjixWgfQKrtmFZibfdOCswg9osVsXAMMVVT1zNBLfZUVarJHeCAkbty3ThUYSSuigF52Srt1BFKDaMJbofbhpfnLEZ_h3w-3E598KS3C353luk6T2Lc7rJy6QLRKN_PDQk8pwy2uhZUiHsaQkb_xgVZhNh_2B3_PiB-1vUwZ3gy7dE8zQ48yFlQXX1VPBMwcDfceyO56RKhsqWwO8qqjxx4EeQTyo4LvFYN8q8hH9_F_ZGnlQmB9-xxHb5Oe270h1ybVpSErWAsJ2xfQDiKfxmnZip6iOAvELrgKwkpRuePB5wNrB4RKCgTJNb1asjQ_uIJxjxDlbXtU-kHaxamOxmc-TW-jEdPUuoSTHUbd84K9Br2faOJH3O9r_D_7OBgygdcHzodMJ8knsZSjgdlWs60QNhzYA_L0jnLTLnROxgWU1V3RKiz2-XoMT4rFIXHK7CGrxe-R1kv13TXVzEgHX06jRUcdwkC-fGM0n8OuzeFVu48zvBGV55eUd18L2i4XPYHftke49J5XUr8o7B2ui3ICSxidmydgEqT3bEJ7dB8FU6fo5o7kkXoj9BXBByZwppWV1Fk1TY8PPj4x8jEPnZevWFQt6BIqzERhV-Yk0f4JtzLmj4vQYwbiUweFnVhXfkkkuh5PR5KI3yg7q1ZVWqasYgPtc7bS0BDeTJZQX8ahaK8-ma9zlpAxbT-OUvbvykR9O0ZspW6fqi7LGF7wgNEqx2NI2cHGPMx41LXxkt9HibJT8-9w==&xkcb=SoDn-_M3ML34ie1NR50EbzkdCdPP&p=13&fvj=0&vjs=3,"#NowHiring #DataEngineer 

Job Title: Data Engineer II 

Contract: 6 Months, potential to convert to a permanent position. If offered a permanent position, immediate eligibility for retirement benefits, health insurance, and pay increase. 

 Belcan is a leading provider of professional IT, Engineering, Workforce Solutions and staffing in the United States, Canada, UK, Europe and India. 

 A Data Engineer II Job in Austin, TX is currently available through Belcan. In this role you will be working in one of the world's largest and most complex data warehouse environments. If you are interested in this role, Apply Today! 


Job Description:
 Job Description 
Client is looking for a Data Engineer to join the GFP Analytics team. The GFP Analytics Data & Infrastructure team consists of experienced engineers and manages a suite of core data services that ingests and processes all data related to Client""s rapidly growing delivery fleet of vans, trucks, electric vehicles, and more. The tech stack designed, built, and operated by the team uses an event-driven architectural paradigm enabled through creative use of AWS and Client-internal data services. 

 Responsibilities 


As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments.
You will be developing and supporting the analytic technologies that give our customers timely, flexible and structured access to their data.
You will be responsible for designing and implementing a platform using third-party and in-house reporting tools, modeling metadata, building reports and dashboards in Oracle BI Enterprise Edition (OBIEE).
You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs.
 Required Skills & Experience 


3+ years of relevant work experience in Big Data engineering, ETL pipeline development, Data Modeling, and Data Architecture
3+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets
Experience with coding languages like Python/Java/Scala
Excellent written and spoken communication skills
A good candidate has strong analytical skills and enjoys working with large complex data sets
 Preferred 
Proficiency in the DevOps style of software deployment (infrastructure-as-code) 
Proficiency with AWS database/ETL tools including Lambda, Glue, Redshift, DynamoDB 
Proficiency with AWS technologies including SNS, SQS, SES, Route 53, Cloudwatch, VPC 

 2 Years of AWS experience 
The candidate will have to do Lots of data modeling. 


Work Schedule: 9-80 (M-F 7:30AM-5:00PM) 
On site only 


Location: Austin, TX 

Zip Code: 78758 

Keyword""s: #Austinjobs; #DataEngineerjobs; 

Start Date: Right Away 

 #ZR 

 Belcan is a global supplier of engineering, technical recruiting, and IT services to customers in the aerospace, industrial, and government sectors. Belcan engineers"" better outcomes through adaptive and integrated services-from jet engines, airframe, and avionics to heavy vehicles, chemical processing, and cybersecurity. Belcan takes a partnering approach to provide customer-driven solutions that are flexible, scalable, and cost-effective. Our unique capabilities have led to continuous growth and success for 63+ years. We are a team-driven Equal Opportunity Employer committed to workforce diversity."
Senior Data Engineer,MAP,"Austin, TX",,,https://www.indeed.com/rc/clk?jk=cc2e1df15a74a484&fccid=3223156f9ea04632&vjs=3,"Who is Wunderman Thompson MAP
 Wunderman Thompson MAP is the world-leading center of excellence for Marketing Automation, Personalization, Loyalty and CRM at scale. Our mission is to deliver value for our clients by humanizing the relationship between the brand and the consumer. To do this, we help clients make data-driven and personalized experiences that can be scaled with efficiency and operationalized intelligently. We believe that serving consumers personal, mindful content in an omnichannel world is key to transforming experiences. With the brain of a consultancy, the heart of an agency and the power of technology and data, we work with some of the world's most admired brands to help them on their transformation journey to becoming truly customer-centric.
 At Wunderman Thompson MAP, we are always making room for more. We are 800+ technology specialists, data scientists, strategic thinkers, consultants, operations experts, and creative minds from 40+ nationalities who collaborate closely to help our clients inspire and engage consumers on five continents.
 Who we are looking for:
 Are you a Senior Data Engineer with a passion to build, support & maintain digital marketing solutions that offers real value? Are you interested in working with the largest global brands and complex challenges in the media analytics space? Then you might be the Senior Data Engineer we're looking for!
 What will your day look like?
 As our new Senior Data Engineer, you will become part of our growing Data Insights and Science team. Here, you will employ new technologies across multiple cloud platforms to help successful brands reach their next level in 1:1 retargeting, communication and CRM. More specifically, your tasks include:

Identify, collect, and integrate data from various sources by building high quality data pipelines and data models for analytics and business intelligence (BI) purpose.
Develop and optimize code to enable pipelines at minimum cost and ease of maintenance.
Build monitoring procedures & tools to ensure solid ETL flows and data quality.
Design processes and tools to correct ETL incidents.
Collaborate with CRM developers, data scientists and data analysts and product owners to ensure the supplied data supports the business initiatives.
Consult our data analytics teams to ensure best practices on the technical use of data are followed.
Design data architectures and collaborate in data migrations in cloud environments.Who are you going to work with?

You will join a team of highly skilled Architects, Data Scientists and Consultants who are passionate about unlocking insights from data through analytics. You will also get to work closely with experts from other Technology, Creative and Client Teams.
 What do you bring to the table?
 As a person, you have a team player mindset and an open-minded attitude. You can communicate ideas and technical topics honestly and clearly – also to non-experts – while respecting the views of others on the team. You are eager to understand and find solutions, allowing you to quickly adapt to changing situations and come up with new ideas.
 At the same time, you solve problems in an analytical and pragmatic manner. Moreover, you have:

4+ years of experience in data engineering, big data, business intelligence or data science
4+ years of experience in Spark, Python, Scala or similar.
Excellent SQL skills enabling large scale data transformation and analysis.A comprehensive understanding of cloud data warehousing, data pipelines and data transformation (extract, transform and load) processes and supporting technologies such as Google Dataflow, Looker, DBT, EMR, CI/CD Pipelines, Airflow DAGs, and other analytics tool.
Experience with cloud based data infrastructures (Ideally GCP, but AWS or Azure would also suffice)
Programming experience in Javascript, Python or similar.
Expertise in managing databases, including performance tuning, backup and recovery.
Solid knowledge of data quality management best practices, including data profiling, data cleansing, and data validation
Knowledge of strengths and limitations of visualization tool (e.g. PowerBI, Tableau, Looker etc) in terms of data modelling in the visualization tools.
Understanding of versioning control tools like GitHub to changes related to dbt models and transformations, ensuring that changes are tracked, documented, and reviewed by the appropriate stakeholders.
Knowledge of applying data governance principles, policies, and practices that ensure data accuracy, consistency, and security
Knowledge of Kubernetes would be an advantage

Personal skills:

Strong desire to contribute towards keeping data tidy and well organized
Ability to think critically, identifying issues autonomously, and proposing corrective actions.
Build great relationships with your team and stakeholders
A leader in personalised customer experiences

What we offer:

Passionate, driven people | We champion a culture of people that do extraordinary work.
Consciously cultivated culture | We aim to embody the behaviors to build an inclusive community that is in it together, bringing both positivity and active listening into the workplace as we simultaneously strive to empower creative bravery.
Competitive benefits | What we offer full time hires ranges from the full spectrum of group health coverage options (medical, dental, vision) to a generous 401k match (100% dollar-for-dollar match, up to 5% of salary contribution), and a variety of paid time off offerings that reflect our investment in all aspects of your overall life balance and wellness.
Growth-minded opportunities | We aim to nurture a culture of real-time feedback, growth-oriented mindset, and plenty of training opportunities through Wunderman Thompson and WPP, so you can continue to grow personally and professionally.

Wunderman Thompson is an equal opportunity employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability, or other protected group status. We believe in creating a dynamic work environment that values diversity and inclusion and strives to recruit a diverse slate of candidates to help us achieve that goal.
 #LI-JK1


    The base salary range for this position at the time of this posting is indicated below. Individual compensation varies based on job-related factors, including location, business needs, level of responsibility, experience, and qualifications. We offer a competitive benefits package, click WPP Benefits for more details.
  
 _

    $80,000—$140,000 USD"
Senior Data Engineer,"Double Line, Inc.","Austin, TX 78758 (North Austin area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=47ba162fca6eddc9&fccid=a3af736f7a482061&vjs=3,"(This is a remote position open to candidates residing near Austin, TX, Raleigh, NC, or Nashville, TN. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

 Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Development team with an experienced and innovative Senior Data Engineer with impressive analytical skills. Sound interesting?

 If so, we're looking for a motivated and driven person like you who has:

 Successfully completed multiple projects where you designed and executed ways to solve complex problems for clients around data integration, data quality, data warehousing, and analytics
 Demonstrated proficiency in deciding which ETL and data streaming technologies to use in AWS, Google Cloud, and Azure-based solutions, and propensity to pick something new when you want to push yourself and the team to innovate
 Mastery of T-SQL and experience with postgreSQL or other forms of SQL
 Experience in an Agile environment with Lean software development principles
 Drive to amplify the skills of teammates through mentoring and training junior and mid-level data engineers
 Mindset of continuous improvement and setting best practices
 Deadline-driven mentality


 Bonus points if you're bringing knowledge of or really want to learn the following:

 A wide variety of data processing tools and approaches, from Python to Google BigQuery to SSIS to AWS Lambda to Azure Data Factory and others
 Performance implications of memory and disk usage at different data volumes
 Business intelligence tools and dashboard design theory


 We do not want you to make the leap without knowing what we need, so here is how we define success for this position:

 Bring a new idea to our team of brilliant data engineers in the first 30 days
 Lead the collaborative design process of a data engineering solution in one of our projects in the first 2 months
 Become a mentor to a data engineer within your first 6 months


 In return, we offer:

 A mission-driven company with a long-term focus on helping the world by untangling the technical knots that challenge state and local governments, particularly in education, healthcare, and similar fields
 A home where your voice matters and you can affect real change
 Direct connection to the Executive team where you can help drive the future of the company
 An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture


 We need to know - can you make this happen? If so, we definitely need to talk to you.

 Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.

 Double Line does not currently offer relocation assistance.
 
 
FcLpeWsMBJ"
Data Engineer,Deloitte,"Austin, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=4382a29546447d7c&fccid=9e215d88a6b33622&vjs=3,"Are you an experienced, passionate pioneer in technology who wants to work in a collaborative environment? As an experienced 
 Data Engineer you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery.
 

Work you'll do/Responsibilities 

 This is an opportunity to join a fast-paced team that plays a key role in the overall success of our client's organization through technology enablement. You'll play a critical part in driving the technology vision forward and ensuring that we execute across multiple initiatives.
 

 You will be responsible for collecting, analyzing, and reporting on customer insights. From this data you will generate insights into how customers interact with our clients' products and use these insights to drive improvements to user-facing features 
Communicate regularly with Engagement Managers (Directors), project team members, and representatives from various functional and / or technical teams, including escalating any matters that require additional attention and consideration from engagement management 
Independently and collaboratively lead client engagement workstreams focused on improvement, optimization, and transformation of processes including implementing leading practice workflows, addressing deficits in quality, and driving operational outcomes 


The Team 

 As a part of the US Strategy & Analytics Offering Portfolio, the AI & Data Operations offering provides managed AI, Intelligent Automation, and Data DevOps services across the advise-implement-operate spectrum.
 

Qualifications 


Required 



Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience 
5+ years of prior experience in a Senior Data Engineering role or as a technical lead on Data Engineering projects 
Advanced experience working with both relational databases (e.g. Teradata, Vertica, etc.) and Big Data platforms (e.g. Hadoop, etc) require 
Advanced hands-on experience with Java and Scala programming languages 
Intermediate understanding of OLAP systems and Data warehousing & Data modelling concepts 
Advanced experience working on Big Data, BI or Analytics related projects as a technical lead and individual contributor 
Advanced knowledge of SQL/Hive/Trino 
Intermediate experience with various performance tuning techniques in Spark/Hive/Teradata 
Advanced experience with job schedulers developing shell scripts, CRON, Airflow jobs to automate data workflows 
Experienced in leading a team of data engineers or individually developing technical solutions to business problems 
Experienced in leading a team of data engineers or individually working on the implementation of data integration requirements and developing the pipeline of data from raw to curation layers including the cleansing, transformation, derivation, and aggregation of data 
Ability to communicate effectively (written and spoken) 
Ability to work with the multi-location development teams and self-manage individual and others work 
Limited immigration sponsorship may be available 
Ability to travel 10%, on average, based on the work you do and the clients and industries/sectors you serve 
Role is remote 


Preferred 



Analytical/ Decision Making Responsibilities 
Analytical ability to manage multiple projects and prioritize tasks into manageable work products 
Can operate independently or with minimum supervision 
Excellent Written and Communication Skills 
Ability to deliver technical demonstrations"
Data Engineer,Tesla,"Austin, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=c88b779349d52e66&fccid=86e9be6ce380173e&vjs=3,"What to Expect
 

    The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. We are looking for someone with strong hands on experience in all layers of data Integration and analytics! We especially need experience in using Python as an ETL tool. The Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
  
 Some of the technology we use:

 Python
 SQL Server and MySQL
 Vertica
 Kafka

 Your Role:


 Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result
 Derive an overall strategy of data management, within an established information architecture that supports the development and secure operation of existing and new information and digital services
 Plan effective data storage, security, sharing and publishing within the organization
 Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks
 Ensures data quality and implements tools and frameworks for automating the identification of data quality issues
 Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings
 Mentor and lead data engineers providing technical guidance and oversight
 Provides ongoing support, monitoring, and maintenance of deployed products

 Qualifications:


 Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus!
 Strong background in data modeling, data access, and data storage techniques
 Experience with design, development, and implementation of highly scalable, high-volume software systems and components, source of truth systems for different business areas, developing and maintaining web services in an agile environment
 Working experience with Kafka Streaming layer
 Experience in Spark Framework on both batch and real-time data processing is a plus
 Experience in Big Data Integration & Analytics is a plus
 Experience in Supply Chain and Logistics data is a plus"
"Senior Software Engineer, Data Solutions-Dallas, Austin, or San Antonio, TX",H-E-B,"Austin, TX 78702 (Holly area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=c7b6409637acea3a&fccid=c629e32155ebd42c&vjs=3,"Overview: 
 
 H-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, 
  H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. 
  H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.
  Responsibilities: 
  Since H-E-B Digital Technology's inception, we've been investing heavily in our customers' digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital-we're using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. If you enjoy taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.  Our Partners thrive The H-E-B Way. In the Senior Software Engineer, Data Solutions job, that means you have a... HEART FOR PEOPLE... you can organize multiple engineers, negotiate solutions, and provide upward communication HEAD FOR BUSINESS... you consistently demonstrate and uphold the standards of coding, infrastructure, and process PASSION FOR RESULTS... you're capable of high-velocity contributions in multiple technical domains

 What You'll Do

 Implement Infrastructure as code, security, and CI/CD for data pipelines.
 Develop solutions to build and continuously improve monitoring and observability for data pipelines and platform infrastructure
 Build data platform components using hybrid cloud services (AWS, GCP, and Azure)
 Implement features to continuously improve data platform performance and security
 Build Real-time data streaming infrastructure
 Create self-service tools and experience for all enterprise data engineering teams
 Build strong relationships with cross functional teams to accomplish impactful results. Work with teams such as Data Science, Front End Teams, and Product Managers.

 Project You Will Impact

 Recommendations systems for customers to provide a more customized shopping experience
 Real time API services that leverage ML Models
 Improve the data quality and consumer experience for 100K+ enterprise data consumers

 Who You Are

 5+ Years of experience with SQL and one or more of the following languages, Python, Java, or Scala.
 A solid understanding of Big Data and Hybrid Cloud infrastructure. 
Proven experience with Kafka, Kubernetes, Spark, Databricks, AWS EMR and Lambda, S3, Data warehouses (Snowflake, Teradata), GCP Cloud services, and NoSQL Solutions
 Experienced in cloud administration and infrastructure as a code (Terraform, Cloud Formation, Ansible, Chef)
 Experienced in DevOps tools such as GitLab CI/CD, and Jenkins.
 Up to date on latest technology developments. should be able to evaluate and propose new tooling/solutions for data platform.
 You have an advanced understanding of development methodologies and processes
 You have a comprehensive knowledge of CS fundamentals: data structures, algorithms, and design patterns
 You have advanced knowledge of system architecture and design patterns
 You can understand architecture, design, and integration landscape of multiple H-E-B systems
 You have experience with common software engineering tools such as Git, JIRA, Confluence, etc.
 You have a high level of comfort in Lean Startup or Agile development methodologies
 You have a related degree or comparable formal training, certification, or work experience
 Excellent written, oral communication and presentation skills.
 Understanding of MLOps and Data Engineering

 Bonus

 DevOps Certifications
 Cloud certifications


   DATA3232"
Data Engineer (Immediate Opening),IDEA Public Schools,"Austin, TX 78704 (St. Edwards area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=8b5a4b01ba85e6f9&fccid=4c58f1f52a144526&vjs=3,"Description 
Role Mission: The Data Engineer will be responsible for improving IDEA's operational processes and supporting critical strategies by assisting in the new development and implementation related to our internal applications systems. The Engineer will deliver the testing, ongoing evaluation, and validation of organizational data structures and identify issues in current processes while providing proven strategies for ongoing database and data warehouse development relating to internal custom applications development and deployment. This position will provide recommendations and insight on IDEA's IT operations and strategy from an applications development standpoint. This job requires the ability and desire to work and communicate well in a dynamic team environment as well as dependability and self-sufficiency.
 What You'll Do - Accountabilities:
 
On-time Product Delivery: Drive to and maintain on-time development and delivery of high-quality features for custom applications and business intelligence tools as defined by monthly sprint plans as measured by: 

% of Critical defects due to SQL code changes and ETL functioning should be limited to a max of 5% 
% of spilled over development tasks should be limited to a max of 5% 
Should acquire the complete ownership of at least 20% of the features developed 
% of reopened defects should be limited to a max of 2% 


Effective Requirements Analysis: New major product work and execution is detailed with full requirements analysis and effective preparation for sprint planning, with detailed task/effort estimates as measured by: 

Difference between estimated effort and actual effort should be limited to a max of 15% and decrease gradually 
Defects due a mis-match in understanding of the requirements should be limited to a max of 10% 


Efficient Object-Oriented Analysis and Design: Thorough object-oriented analysis and design of features with the documentation of necessary design artifacts as measured by: 

Refactoring time to enhance/improve SQL code changes and ETL development should be less than 15% of the original effort. 
At least 20% of the code developed should be reusable 
Reuse of code should be leveraged when possible. Duplication, if any, should be limited to a max of 5%. 


Adherence to Effective Scrum Practices: Adhering to all of the Scrum processes, with active and punctual participation in Scrum meetings as measured by: 

Absenteeism in scrum ceremonies including daily huddles, retrospectives, product reviews, and planning sessions should be limited to a max of 5% with prior intimation 
All planned leaves (both short and long duration) should be intimated in advance and documented so that sprint commitments are not affected. Deviation should be limited to 5%. 
Unplanned leaves (both short and long duration) should be intimated as soon as possible and documented so that sprint commitments are not affected. Deviation should be limited to 5%. 


Continuous Improvement of Domain, Technical, and Behavioral Skills: Continuously enhancing the Product domain knowledge, technical, and behavioral competencies to grow to the next level as measured by: 

Relevant trainings/actions need to be identified, planned and attended 3 times within the year (1 skill from each area - domain, technical, and behavioral) 
Development and demonstration of these skills for the purpose of the facilitation of team training and/or mentoring should be developed for each team member (1 opportunity per year) 


We look for Team and Family who embody the following values and characteristics: 

Believes and is committed to our mission and being an agent of change: that all students are capable of getting to and through college 
Has demonstrated effective outcomes and results, and wants to be held accountable for them 
Has a propensity for action, willing to make mistakes by doing in order to learn and improve quickly 
Works with urgency and purpose to drive student outcomes 
Thrives in an entrepreneurial, high-growth environment; is comfortable with ambiguity and change 
Seeks and responds well to feedback, which is shared often and freely across all levels of the organization 
Works through silos and forges strong cross-departmental relationships in order to achieve outcomes 
We believe in education as a profession and hold ourselves to high level of conduct, professionalism and behaviors as models for our colleagues and students 

Note: At IDEA, the Data Engineer role is a mid to high level role with a focus on building upon the IDEA data warehouse. The IDEA data warehouse is the central data store for all analytics and reporting. As a result, the Engineer is responsible for data extraction, transformation, and loading of data into the data warehouse and is the primary keeper of this system. These processes require strong skills with a variety of specialized tools and techniques for data cleansing, preparation, modeling, and integration. This role is also required to have a strong background in analytics and server-side processing. 

Supervisory Responsibilities:
 This role leads and oversees the work of others in a project capacity as a project technical lead: 

Planning and directing Data Integration/ETL Developer team member activities on projects 
Assigning work 
Overseeing proper maintenance and back-up of source code 
Participation in evaluating performance (for Data Integration/ETL Developers) 
Mediating conflict resolution 

Qualifications:

 Education: Bachelor's degree from four-year college or university in Information Technology, Computer Science, Computer Engineering, and Software Engineering
 Experience: 6+ years related work experience and/or training; or equivalent combination of education and experience. 
Certification/License:
   
 Microsoft Certified Solutions Associate (SQL 2016 BI Development),
 Certified Associate in Project Management (CAPM) preferred


 
What We Offer
 Compensation: 

Salaries for people entering this role typically fall between $66,626 and $80,618, commensurate with relevant experience and qualifications and in alignment with internal equity. This role is also eligible for a performance bonus based on individual and organizational performance and goal attainment.

 
Other Benefits:
 We offer a comprehensive benefits plan, covering the majority of the employee premium for the base medical plan and subsidizing the majority of costs for a spouse/domestic partner and children. Other benefits include dental and vision plans, disability, life insurance, parenting benefits, generous vacation time, com‐muter benefits, referral bonuses, professional development, and a 403(b) plan. We also offer an inclusive environment where staff are encouraged to bring their whole selves to work every day. IDEA may offer a relocation stipend to defray the cost of moving for this role, if applicable.
 

To Apply:
 Please submit your application online through Jobvite. It's in your best interest to apply as soon as possible. It is recommended that you include a cover letter in your application addressing why you are interested in IDEA and how your experience has prepared you for this position."
AWS Data Engineer – R01525162,Brillio,"Remote in Austin, TX",Posted 15 days ago,,https://www.indeed.com/rc/clk?jk=6fc2b9d16d8fc5ae&fccid=a2520645450d003a&vjs=3,"AWS Data Engineer - R01525162


About Brillio: 


  Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022
 


 AWS Data Engineer
 
 Job requirements

 Brillio LLC is a fast growing, pure play Digital Transformation Solutions and Services firm backed by Bain Capital private equity. Founded in 2014 and headquartered in Silicon Valley, Brillio is focused on delivering design led solutions for our customers. We are not an IT Services company trying to pivot and support the “next big thing.” As a digitally native company, everything we have done and will continue to do, will have this laser focus.



 Our culture is driven by Brillio’s Core Values and it is the foundation of who we are as an organization. We equip and empower our employees to push boundaries with an 
‘Entrepreneurial’ 
spirit as they craft 
‘Customer Success’
 stories. With our customer-centric approach, we deliver 
‘Excellence’ 
at every level of our collaboration and this is what fuels Brillio’s growth story. While we are proud of our journey so far, 
‘We Care’
 about our customers, employees and the community where we thrive in. We strive to stay committed to lighting up 100,000 young minds through our CSR efforts.



 Role: AWS Data Engineer
 

   Years of Experience: 6+ years
 

   Travel Required: No
 

   Location: Seattle, WA (Remote)
 


 Job Description: We are looking for an experienced AWS Data Engineer to join our team and help us build and maintain our data infrastructure on AWS. As an AWS Data Engineer, you will work closely with data Architect, Business and Data analysts, BI Developer, Customer Business and engineering team, and other stakeholders to design, implement, and manage data pipelines and systems that support our business needs.
 


 Key Job Responsibilities:


 Design, implement, and maintain data pipelines and data processing systems, ETL Infrastructure on AWS using technologies like Apache Spark, AWS Glue, AWS Lambda, and AWS S3 and strong Python and Pandas hands on experience.
 Collaborate with data Architect and analysts/BI Developer to understand their data requirements and design data models that meet their needs.
 Work with DevOps engineers to ensure that data pipelines are reliable, scalable, and secure.
 Monitor and troubleshoot data pipelines to ensure that data is flowing correctly and on time.
 Develop automation scripts to streamline deployment, testing, and maintenance of data pipelines and systems.
 Document technical designs, standard operating procedures, and best practices for data engineering on AWS.
 Keep up to date with emerging technologies and best practices in data engineering and recommend new tools and technologies as appropriate.
 Excellent communication and collaboration skills.
 Need strong commitment to project, Problem solving, Team Player



 Preferred qualifications:


 Bachelor’s or master’s degree in computer science, Information Technology, or a related field.
 3+ years of experience in data engineering with a focus on AWS technologies.
 Strong knowledge of AWS services such as S3, EC2, Glue, Lambda, and Athena.
 Experience designing and building data pipelines using Apache Spark.
 Experience working with SQL and NoSQL databases.
 Familiarity with ETL processes and tools such as Talend or Informatica.
 Strong scripting skills in Python or Java.



 #LI-KC1
 



   Know what it’s like to work and grow at Brillio: Click here"
Big Data Engineer - PySpark,Logic20/20 Inc.,"Austin, TX",Posted 30+ days ago,"$130,000 - $155,000 a year",https://www.indeed.com/rc/clk?jk=ae6c1a13d1e9155c&fccid=2e9e942085461a66&vjs=3,"Company Description
  We’re a seven-time “Best Company to Work For,” where intelligent, talented people come together to do outstanding work—and have a lot of fun while they’re at it. Because we’re a full-service consulting firm with a diverse client base, you can count on a steady stream of opportunities to work with cutting-edge technologies on projects that make a real difference.
 Logic20/20's Global Delivery Model creates a connected experience for Logicians across geographies. You'll have access to projects in different locations, the technology to support Connected Teams, and in-person and online culture events in our Connected Hub cities.



 Job Description
  As a Data Engineer, you'll be joining a team at one of the nation's largest utilities companies in California to build out an ""All Hazards"" portal, which is a web-based platform designed to provide information and resources related to various types of hazards and emergencies that may impact service areas. The portal is intended to help its customers, stakeholders, and the general public to access information and resources related to safety, preparedness, and response during emergencies or hazardous events, such as wildfires, earthquakes, storms, and other natural disasters.
 Hear more about these efforts as Jeff Lovington shares his experience working in Data Science and Machine Learning for the Energy & Utilities sector.
 About the team
 The Logic20/20 Advanced Analytics team is where skilled professionals in data engineering, data science, and visual analytics join forces to build simple solutions for complex data problems. We make it look like magic, but for us, it’s all in a day’s work. As part of our team, you’ll collaborate on projects that help clients spin their data into a high-performance asset, all while enjoying the company of kindred spirits who are as committed to your success as you are. And when you’re ready to level up in your career, you’ll have access to the training, the project opportunities, and the mentorship to get you where you want to go.
 “We build an environment where we really operate as one team, building up each other’s careers and capabilities.” – Adam Cornille, Director, Advanced Analytics



 Qualifications
  Must have:

 3+ years of implementation experience using PySpark
 5+ years of data engineering experience
 Strong understanding of high-performance ETL development with Python
 Experience with Big Data Technologies (Hadoop, Spark, MongoDB)
 Experience designing and developing cloud ELT and date pipeline with various technologies such as Python, Spark, PySpark, SparkSQL, Airflow, Talend, Matillion, DBT, and/or Fivetran
 Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule

 Preferred:

 Experience with Palantir’s Foundry

 Additional Information
  All your information will be kept confidential according to EEO guidelines.
 Compensation range: $130,000 - $155,000 annually

 About Logic20/20
 To learn more about Logic20/20, please visit: https://www.logic2020.com/careers/life-at-logic
 Core Values
 At Logic20/20, we are guided by three core values: Drive toward Excellence, Act with Integrity & Foster a Culture of We. These values were generated and agreed upon by our employees—and they help us pursue our goal of being one of the best companies to work for and to work with. Learn more at https://www.logic2020.com/company/our-values.
 Logic20/20 Benefits
 Why Logic20/20? It’s our goal to be one of the best companies to work for. One piece of the puzzle is an evolving set of benefits that extend past medical, dental, and 401(k).
 You will have

 Career Development – A built-in program from day 1, providing a mentor and individually-directed training opportunities, plus access to leaders across the company
 PTO, Paid Holidays, & Voluntary Leave – Worry-free time off to recharge and pursue your personal goals
 Community & Committees – As part of our “Culture of We,” Logic20/20 invests in providing many social, interest, and learning opportunities


 Recognition – From peer recognition, swag, and the chance to win a once-in-a-lifetime type of award, we make your Logic20/20 journey stand out
 Referral Programs & Bonuses – Employee, project, and sales referral programs with paid incentives

 Equal Opportunity Statement
 We believe that people should be celebrated: for their talents, ideas, and skills, but most of all, for what makes them unique. We prohibit harassment and/or discrimination based on age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status, or any other basis as protected by federal, state, or local law.
 To learn more about our DE&I initiatives, please visit: https://www.logic2020.com/company/diversity-equity-inclusion
 Privacy Policy
 During the recruitment and hiring process, we gather, process, and store some of your personal data. We consider data privacy a priority. For further information, please view our company privacy policy."
Senior Data Engineer,Procore Technologies,"Austin, TX",Posted 4 days ago,"$126,800 - $174,350 a year",https://www.indeed.com/rc/clk?jk=fca7e1d45123055a&fccid=bc675912b67fcb67&vjs=3,"Job Description
  Senior Data Engineers
 Procore is looking for a Senior Data Platform Engineer to join our data engineering team. Data platform engineers are responsible for designing and implementing Procore's overarching data strategy. Critical projects include the design and operation of Procore's streaming and batching data processing infrastructure, architecture of Procore's datalake and the selection of new infrastructure technologies.
 We're looking for a motivated engineer with at least 5 years of experience. You must be comfortable operating in a high autonomy environment, architecting systems from the ground up and deploying technologies that are new to our organization. drive solutions to wide-ranging data engineering and infrastructure challenges for product and internal operations.
 You will partner with world-class developers, engineers, architects, and data scientists to drive thinking, provide technical leadership, and collaborate in defining best practices around data engineering. You will also work alongside local product management, engineering, and research teams to develop innovative solutions that will influence our product line.

 Examples of our projects:

 An ETL pipeline for our data lake consisting of batch processing, orchestration with Airflow, monitoring with Datadog and alerting with Slack
 A Maven package used by all of Product Dev teams for building Kafka consumers with built in support for configuration, error reporting, monitoring, deserialization, gRPC, Spark, Flink, and Kubernetes
 A multi-stage data lake including landing, process and serving zone


 Some of your responsibilities include:

 Provide technical leadership to efforts around tooling and infrastructure that enable teams to efficiently complete and maintain data science projects
 Partner with teams on modeling and analysis problems – from transforming problem statements into analysis problems, to working through data modeling and engineering, to analysis and communication of results
 Lead code reviews, design, and best practices
 Coach and mentor senior engineers
 Use experience gained in the above and expertise in this space to influence our product roadmap, potentially working with prototype engineering team to add additional capabilities to our products to solve more of these problems


 Who You Are...

 5+ years of experience in a Data Engineer role with a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 Expertise building data pipelines (in either Real-time or batch) on large complex datasets using Spark or Flink frameworks
 Experience with AWS services including EC2, S3, Glue, EMR, RDS, Snowflake, Elastic Search, Cassandra and Data pipeline/streaming tools (Airflow, NiFi, Kafka)
 Experience building and optimizing data pipelines, architectures and data sets. A successful history of manipulating, processing and extracting value from large disconnected datasets.
 Deep knowledge of stream processing using Kafka and highly scalable ‘big data’ data stores.
 Expertise in Java or Python
 Team Player. Experience supporting and working with cross-functional teams in a dynamic environment.
 Strong oral and written communication skills.
 Experience of End-to-end data quality control and automated testing experience

 Bonus Points:

 Experience with unstructured data (PDF, contract, plan, image)
 Data transformation (quality, extraction)
 Experience in working within team handling all the data pipeline from extraction to Data warehouse


 About Us
 Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore. Our headquarters is located on the bluffs above the Pacific Ocean in Carpinteria, CA, with growing offices worldwide. To learn more about our team, click here.
 We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

 Perks & Benefits
 You are a person with dreams, goals, and ambitions—both personally and professionally. That's why we believe in providing benefits that not only match our Procore values (Openness, Optimism, and Ownership) but enhance the lives of our team members. Here are just a few of our benefit offerings: competitive health care plans, unlimited paid vacation, stock options, employee enrichment and development programs, and friends & family events.



 Additional Information
 
 Base Pay Range $126,800-$174,350. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
 
 Perks & Benefits
 At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
 About Us
 Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
 We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
 If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community."
"Customer Engineer, Data and Analytics, Public Sector",Google,"Austin, TX",Posted 19 days ago,,https://www.indeed.com/rc/clk?jk=22e588d287738205&fccid=a5b4499d9e91a5c6&vjs=3,"Minimum qualifications:

Bachelor's degree in Computer Science, a related technical field, or equivalent practical experience. 
6 years of experience in virtualization or cloud native architectures in a customer-facing or support role. 
Experience in cloud computing (e.g., cloud market) and delivering technical presentations. 
Experience in analytic warehouse solutions, Big Data technologies, real-time streaming, performance, and scalability optimizations.


 Preferred qualifications:

Master's degree in Computer Science or a related technical field. 
Experience with Public Sector client groups including state and local government.
Experience in developing data warehousing, data lakes, batch, or real-time event processing and Exact Transform and Load (ETL) workflows solutions (e.g., Informatica, Talend, Alooma, SAP, Data Services). 
Experience in technical sales in the field of cloud computing, data, information lifecycle management, and Big Data. 
Knowledge of Linux. 



     Ability to address domain name systems, transmission control protocol, firewalls, proxy servers, load balancing, virtual private networks, and virtual private cloud.
    




 About the job
 The Google Cloud Platform team helps customers transform and build what's next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. Our teams are dedicated to helping our customers — developers, small and large businesses, educational institutions and government agencies — see the benefits of our technology come to life. As part of an entrepreneurial team in this rapidly growing business, you will play a key role in understanding the needs of our customers and help shape the future of businesses of all sizes use technology to connect with customers, employees and partners.
 As a Customer Engineer, you will work with the Sales team to introduce Google Cloud to our customers. You will help prospective and existing customers and partners understand Google Cloud, develop creative cloud solutions and architectures to solve their business issues and problem-solve any potential roadblocks.
 Google Cloud accelerates organizations’ ability to digitally transform their business with the best infrastructure, platform, industry solutions and expertise. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology – all on the cleanest cloud in the industry. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.
 The US base salary range for this full-time position is $139,000-$213,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
 Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.


 Responsibilities

Work with the team to identify and qualify business opportunities, identify key customer technical objections, and develop a strategy to resolve technical blockers. 
Manage the technical relationship with Google customers (e.g., managing product and solution briefings, proof-of-concept work, and the coordination of additional technical resources). 
Work with customers to demonstrate and prototype Google Cloud product integrations in customer and partner environments. 
Prepare and deliver product messaging in an effort to highlight the Google Cloud Platform value proposition, using techniques that include presentations, product demonstrations, white papers, and request for information response documents.
Travel to customer sites, conferences, and other events as needed.

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form."
Data Science - Software Engineer,Plaxonic Technologies,"Austin, TX",Posted 30+ days ago,$50 - $60 an hour,https://www.indeed.com/company/Plaxonic-Technologies/jobs/Data-Scientist-c8b4f69fcf2405d5?fccid=4fe234f122c10841&vjs=3,"Data Science - Software Engineer
Location: Austin, Texas
 API and UI so that users can fetch image data from Azure
 Lightweight task to check SharePoint to see if new images exist and if so, send them to our
image processing pipeline.
 A lightweight API to update our database when image tests change state
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:

8 hour shift

Experience:

Data science: 8 years (Required)
API: 8 years (Required)
Python: 8 years (Required)

Work Location: On the road"
Senior Data Engineer (remote),Ad Hoc Team,"Remote in Austin, TX",Posted 28 days ago,"$101,570 - $136,994 a year",https://www.indeed.com/rc/clk?jk=b018163a0713f248&fccid=7707621c92754acd&vjs=3,"This is a fully remote position.
 Work on things that matter Ad Hoc is a digital services company that helps the federal government better serve people. Our teams use modern, agile methods to design and engineer government systems that connect Veterans with services, bring affordable health care to millions of people, and support important programs like Head Start. And as we work to make critical government services intuitive, accessible, and human-centered, we're also changing how the government thinks about and uses technology. If you thrive on change, want to help close the gap between consumer expectations and government services, and can see the possibilities in ambiguity, then we want you here with us.
 What matters most Ad Hoc operates according to our commitment to inclusivity, acceptance, accountability, and humility. We aren't heroes. We believe in missions larger than our individual selves and leave our egos at the door, learn from our mistakes, and iterate in order to better serve the people in our country. We prioritize building teams that represent the diversity of the people our government serves. We love the challenge of government-size projects. We want to bring skills to federal agencies, help them better meet the needs of their users, and close the gap between consumer expectations and government.
 Built for a remote life Ad Hoc is remote-first and remote-always. We've designed our culture, communications, and tools to support a nationwide distributed team since the beginning. Being remote by design allows Ad Hoc to be thoughtful and intentional about creating diverse teams and supporting them with a work environment that fits their lives. With a generous PTO policy and Slack channels for every interest (from bird watching to space nerds to parenting) our culture embraces the things happening in your life. Maybe you need to adjust your schedule to care for your family or take a bike ride. At Ad Hoc, that's embraced.
 What you'll do
 Our ideal Back End Software Engineer knows how to build large-scale production systems in modern agile environments. They write well-structured, tested, and secure code with little feedback or guidance. They've designed and implemented reliable and maintainable APIs and built services that integrate with external dependencies. They can articulate how the thing they've built fits into a larger ecosystem. They're not afraid of large, complex problems. They take an active role in planning and delivery efforts, drawing on their experience to suggest better approaches or alternatives. As an Ad Hoc Back End Engineer, you'll be:

Shipping software that impacts the lives of millions of people
Using modern programming languages and frameworks to build scalable services that gracefully integrate with legacy systems
Building and working with APIs to support both the digital services we deliver as well as third-party usage
Using unit and integration testing to ensure systems work as intended
Helping us continuously, iteratively improve

What we hope you'll bring
 

A minimum of four (4) years of professional software development experience
AWS experience
Understanding of ETL/ELT processes and tooling
Understanding of database technologies - setup/ maintenance / data loads / etc. (not data modeling)
Redshift experience preferred
Understand system security
API design and implementation
GIT and DevOps release process
Python or Scala, Python is preferred for ETL
Some experience with older file systems / file based processes such as MOVEit
Some experience with Mulesoft
Experience with agile software development practices emphasizing agility, flexibility, and iterative development


More than that, our ideal candidate wants to contribute to work that is bigger than themselves and wants to make a difference collaborating with their team. They care deeply about building better products, better relationships, and better trust in each interaction people have with their government. They believe in intuitive, easy-to-use government services. They collaborate well with designers, stakeholders, and other teams. They mentor and guide more junior engineers. They're human-centered.
 And if you don't check every box on the list? That doesn't mean you can't help us in our mission to deliver critical government services. Talk to us!
 Some basic requirements

All work must be conducted within the U.S., excluding U.S. territories. Some federal contracts require U.S. citizenship to be eligible for employment.
You must be legally authorized to work in the U.S now and in the future without sponsorship.
As a government contractor, you may be required to obtain a public trust security clearance.
Bachelor's Degree in a technical field is preferred
4 years of professional software development
Our technical screening involves completing a homework assignment that is then graded blind to remove bias. We do not do tricky, unreliable whiteboarding tests. You can read more about our homework here.

Learn more about engineering at Ad Hoc.
 Benefits

Company-subsidized Health, Dental, and Vision Insurance
Use What You Need Vacation Policy
401K with employer match
Paid parental leave after one year of service

Ad Hoc LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, sex, sexual orientation, gender identity or expression, religion, age, pregnancy, disability, work-related injury, covered veteran status, political ideology, marital status, or any other factor that the law protects from employment discrimination.
 In support of the Colorado Equal Pay Transparency Act, and others like it across the country, Ad Hoc job descriptions feature the starting range we reasonably expect to pay to candidates who would join our team with little to no need for training on the responsibilities we've outlined above. Actual compensation is influenced by a wide range of factors including but not limited to skill set, level of experience, and responsibility. The range of starting pay for this role is $101,570 - $136,994 and information on benefits offered is here. Our recruiters will be happy to answer any questions you may have, and we look forward to learning more about your salary requirements.
 job reference: 2011"
Data Engineer - Contract,Huckberry,"Austin, TX 78745",EmployerActive 2 days ago,,https://www.indeed.com/rc/clk?jk=b560a513624705d0&fccid=5ce64caed4b7c135&vjs=3,"Huckberry is seeking a Data Engineer to work closely in analytics to ensure the best data integrity and structuring to support insights. The Data Engineer will primarily work in DBT modeling with our site (Segment) and inventory (NetSuite) data to provide accessibility to our most important data. The Data Engineer will also support the movement and modeling of new data sources in Databricks. The candidate should be well organized, quick at code, and think beyond the task and toward the objective as a thought partner with the analytics team. Huckberry is a rapidly growing company, this data engineer will need to be flexible and step in as a technical support across multiple business facing tools – Toolio, Airtable, Looker, etc.
 Requirements

Collaborate with software engineers and analysts in data collection and data usage to model analytics tables from raw format to useable fact tables
Configure DBT models to keep the cost per query down and query speeds high for the team
Write custom batch load and event streaming Python jobs to transfer data between systems (API, SFTP, etc.) 
Partner with analysts to productionalize machine learning algorithms where helpful (NLP, product recommendations, time-series forecasting, etc.) 
Manage data orchestration and chron scheduling between Python and SQL jobs with tools like DBT Cloud, Dagster, Prefect, etc.
Bring best practices for overall data governance as it relates to data integrity tooling, user-access permission setting, query testing, alerts, customer data protection, etc. 
Mitigate our “bus factor” and “technical debt” with organized folders and consistent field-naming methodology as well as conviction to document-as-you-go




Responsibilities

2-5 years of professional experience as a data engineer in a high-growth medium-sized company that uses tools like DBT, Airflow, GCP, Databricks, etc. 
Experience with ETL modeling and Chron scheduling with tools like DBT, Airflow, Databricks etc.
Strong proficiency in both Python and SQL, Javascript experience a plus
Proven ability to remain organized and flexible – often scoping the 80/20 solution
Experience in GitHub and can work effectively in version controlling best practices
Demonstrated experience with large scale data-warehouse and cloud technologies like GCP as well as relational databases like Postgres
Demonstrated ability to project manage and communicate progress to senior leadership on large-scale projects
Proven ability to collaborate with software engineers and data analysts to create synergy between data collection and data usage
Experience in Machine Learning a plus
Experience with 3rd party pixels and SDKs a plus

Benefits

Flexibile schedule
Competitive pay
Summer Fridays




Company Description
 Huckberry is a leading men’s lifestyle retailer and media company. Millions of guys trust us as their go-to resource for the coolest new gear, lifestyle inspiration, and a lot more. We were recently named one of IAB’s most disruptive consumer brands, and we’ve collaborated with everyone from Matthew McConaughey and Kelly Slater to brands like Danner, Timex, and RRL. We look forward to meeting you.  


Want to get to know us better? Check out our:

Journal: https://huckberry.com/journal 
YouTube: https://www.youtube.com/c/Huckberryco 
Instagram: http://instagram.com/huckberry 



Huckberry encourages candidates of all different backgrounds and identities to apply. We are always eager to further diversify our company, and we are committed to providing an inclusive environment of mutual respect where all can flourish. All of our employment decisions are based solely on merit and business need.
 Notice to California Job Applicants"
Senior Data Engineer,Visa,"Hybrid remote in Austin, TX",Posted 9 days ago,,https://www.indeed.com/rc/clk?jk=e522b926cd1ff5ec&fccid=a3f737e511d9fc8c&vjs=3,"Company Description
  Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.
 When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.
 Join Visa: A Network Working for Everyone.



 Job Description
  To ensure that Visa’s payment technology is truly available to everyone, everywhere requires the success of our key bank or merchant partners and internal business units. The Global Data Science group supports these partners by using our extraordinarily rich data set that spans more than 3 billion cards globally and captures more than 100 billion transactions in a single year. Our focus lies on building creative solutions that have an immediate impact on the business of our highly analytical partners. We work in complementary teams comprising members from Data Science and various groups at Visa. To support our rapidly growing group we are looking for data engineers who are equally passionate about the opportunity to use Visa’s rich data to tackle meaningful business problems.

 This position will be part of VCA (Visa Consulting & Analytics) function in building and maintaining global data assets and engineering solutions. In this role, you will be responsible for helping to develop a global engineering and solutions team focused in standardized development of Consulting Solutions. You will partner closely with Global stake holders in Visa consulting, data Science and data engineering teams. You will get chance to leverage your strategic planning, business analysis and technical knowledge of data engineering, tools and data architecture solutions. In addition to managing our portfolio of data engineering assets and solutions globally, you will play key roles in building relationships with Consulting partners to develop effective market response strategies. You will also be a hands-on expert able to direct & navigate both data engineering and data science teams to build effective data engineering solutions.
 Essential Functions

 Exposure working with Global teams & Fortune 500 companies and possess strong experience working directly in client facing roles.
 Execute and manage large scale ETL processes to support development and publishing of reports, Datamart’s and predictive models.
 Strong Data Analytical and Visualization skills along with experience in self-service reporting tools like Tableau or Power BI with KPIs and facilitate Visa Consulting engagements including data exchange.
 Deliver small to medium complexity dashboard projects either individually or as part of a project team.
 Use design prototyping rigor and consultative best practices with our stakeholders
 Develop solutions leveraging agile principles
 Provide operational production support for the dashboards on Tableau Server.
 Should have strong problem-solving capabilities and ability to quickly propose feasible solutions and effectively communicate strategy and risk mitigation approaches to leadership.
 Build and maintain high performing ETL processes, including data quality and testing aligned across technology, internal reporting and other functional teams
 Create data dictionaries, setup/monitor data validation alerts and execute periodic jobs like performance dashboards, predictive models scoring for client’s deliverables
 Define and build technical/data documentation and experience with code version control systems (e.g. git). Ensure data accuracy, integrity and consistency
 Find opportunities to create, automate and scale repeatable financial and statistical analysis for Visa Consulting and Analytics.
 Collaborate with Data Engineering teams in North America and other Global regions to production and maintenance of key data assets.
 Strong written, verbal, and interpersonal skills needed to effectively communicate technical insights and recommendations with business customers and leadership team.

 This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.



 Qualifications
 
 Basic Qualifications:
 


5 or more years of relevant work experience with a Bachelors Degree or at 
least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, 
  JD, MD) or 0 years of work experience with a PhD
 
 Preferred Qualifications:
 


6 or more years of work experience with a Bachelors Degree or 4 or more 
years of relevant experience with an Advanced Degree (e.g. Masters, MBA, JD, 
  MD) or up to 3 years of relevant experience with a PhD
 

Minimum of 6+ years of work experience with a Bachelor's Degree or 4+ years 
of experience with a Master’s or Advanced Degree in a Quantitative field such 
  as Statistics, Mathematics, Operational Research, Computer Science, 
  Economics, Engineering, or equivalent with analytics expertise in applying 
  statistical solutions to business problems 
 

Technical expertise in leading business intelligence and data visualization 
projects
 

Working knowledge of Hadoop ecosystem and associated technologies, (For 
e.g. Apache Spark, MLlib, GraphX, iPython, sci-kit, Pandas etc.)
  Business skills and general qualities:
 

Good business acumen to orient data analysis to business needs of clients.
Ability to translate data and technical concepts into requirements documents, 
business cases and user stories.
 

Good understanding of agile working practices and related program 
management skills.
 

Good communication and presentation skills with ability to interact with 
different cross-functional team members at varying levels
 

Ability to learn new tools and paradigms as data science continues to evolve 
at Visa and elsewhere.
 

Demonstrated intellectual and analytical rigor, team oriented, energetic, 
collaborative, diplomatic, and flexible style.
 

Intellectually curious and continuously striving to learn.

 Technical skills:
 


A strong BI/data visualization portfolio and hands-on experience 
Expert proficiency with visualization software, preferably Tableau, is required 
Knowledge of interactive data visualization best practices 
Experience leading design discovery meetings with end-users is preferred 
Excellent grasp of the design thinking and the prototyping process 
Have a good understanding of data management systems and processes 
within the ecosystem (source systems, data feeds, reference data 
  management, ETL, Data Warehouses and Marts, data cube design, etc.) and 
  has experience working with SQL or Tableau Prep. Experience with any script 
  language (R/Python) is a plus. 
 

Experience working in an Agile or fast-paced and dynamic environment is a 
plus 
 

Experience working with any prototyping tool 
Visa experience or knowledge of payments industry is a plus, but not 
mandatory
 
 Additional Information
  Work Hours: Varies upon the needs of the department.
 Travel Requirements: This position requires travel 5-10% of the time.
 Mental/Physical Requirements: This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.
 Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.
 Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of Article 49 of the San Francisco Police Code.
 U.S. APPLICANTS ONLY: The estimated salary range for a new hire into this position is 126,000.00 to 163,800.00 USD, which may include potential sales incentive payments (if applicable). Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. In addition, this position may be eligible for bonus and equity. Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program."
Sr. Software Development Engineer in Test (Data Focused),LPL Financial,"Austin, TX 78701 (Downtown area)",Posted 14 days ago,"$90,080 - $135,120 a year",https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DKsgO2x_f7Hkxt3efMxTlFg7j6PgdtPg50JwtVahjkzRNSPlr-rYnseRopiFvdqFpli8lNV05U5beFXEjS_6n3c6PStb3YymQAtvZfZ5aSr5eQQKh7HevLV7K6o3p-FMAzSKSMTDOB2YBKZHPmVPijjfqzd3dqsgLs-B_wQX68cX3itzNNZWjgnOvjEzIjfPwoTtiwXMNc_galNHOttLG54lLB6ITa32qwJhr28mtmolnCI_d4829NMjNq6si6JYKurIPsrcEiiFPIWagZ0pXxto3mqNhpBO1RKyW9igGTjEXrz_170yRGfB4FNeesMR9Qm7v8nnVB1BB_DpWcq0LcvdXfGdxG9e5yfVghUNG9fNt92QTR8RerawqEU_L3SRkdFT1o4ELyI3i-Vv7S_Xyl_Tcsvnn3fjv57_yb9fcC5v1enGyxHMmUqcvxDQ-U9awbvWIC7pp_VqbLu9s0NFNdRkUf1EMsng0jrgfugPbTofFXOjnmIrVYupfMxO8Qd34=&xkcb=SoBD-_M3ML32MwgMjL0GbzkdCdPP&p=13&fvj=0&vjs=3,"Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!

 LPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.

 Job Overview:

 We are seeking a Senior Quality Assurance Engineer. Our ideal candidate will be responsible for designing and executing tests manually (15%) and automatically (85%) to identify and resolve all data related issues to meet quality standards. Our tight-knit team combines a Scrum process with a strong testing culture.

 Responsibilities:

 Build and perform End to End QA effort on Data Warehouse/ETL/BI Platforms
 Review requirements, specifications and technical design documents to provide timely and meaningful feedback
 Understand data flow and test strategy for ETL , Data warehouse and Business Intelligence testing
 ETL testing of mapping, transformations and data pipelines.
 Data warehouse testing involving the testing of stored procedures, SSIS packages, slowly changing dimension tables, Change Data capture etc.
 Business Intelligence testing involving the validation of DataMart, ODS, Data models and SSRS reports.
 Create detailed, comprehensive and well-structured test plans and test cases
 Identify, document and track software defects
 Work closely with development teams to perform root cause analysis of issues
 Manage multiple tasks and adjust to shifting priorities, as necessary
 Write advanced SQL queries for ETL/data warehousing/Business Intelligence testing
 Identify test cases which has to be automated. Prepare test automation strategy.
 Design and develop test automation for backend.
 Work with the team to continually improve test processes and practices based on inspection/adaption of previous iterations and to ensure adherence to process, tools and metrics standards within the project team
 Show initiatives and accountability with strong time management skills with project teams.
 Exhibit strong collaboration/interpersonal skills
 Be a self-starter and possess the ability to research issues and improve processes


 What are we looking for?
 We want strong collaborators who can deliver a world-class client experience. We are looking for people who thrive in a fast-paced environment, are client-focused, team oriented, and are able to execute in a way that encourages creativity and continuous improvement.

 Requirements:

 Bachelor’s Degree, Computer Science or similar, plus 6+ years of experience OR Master’s Degree, Computer Science or similar, plus 5+ years of experience OR no degree, plus 10+ years of experience working as a Developer or QA Engineer or SDET working in Agile scrum teams
 5 –10 years of experience with Software Quality Assurance- QA project life cycle, test plan, test strategies, test scenarios, test cases, traceability matrix.
 3 - 5+ years of experience in Database/Data Warehouse/ETL Testing.
 3 - 5+ years of Strong experience in advanced SQL scripting.
 Experience in MS SQL server, SSIS and SSRS.
 3+ years’ experience of Data Lake/Hadoop platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Hadoop/Spark implementations.


 Core Competencies:

 Experience in ETL testing techniques
 Good understanding of Data Warehousing and business intelligence concepts and testing techniques
 Experience in Working with Star Schema, ODS, multi-dimensional models, slowly changing dimensions
 Experience in working with ETL framework, Change Data capture, DataMart, Data models etc.
 Working knowledge of test automation for backend using Java and associated frameworks 
Experience in continuous testing practices in a CI/CD development pipeline, and deploying test automation
 Solid experience in strategizing and planning all testing activities including automation.
 Ability to review and analyze business requirements in order to produce test strategy and test cases.
 Expertise working in Cloud data environment
 Familiarity with one or more SQL-on-Hadoop technology (Hive, Impala, Spark SQL, Presto)
 Experience in Agile projects (Scrum, Kanban etc.).
 Working Knowledge in Test Management software (JIRA,qTest).
 Solid experience with Defect Management Process.
 Quick learner and self-starter who requires minimal supervision to excel in a dynamic environment.
 Excellent analytical and problem solving skills.
 Strong verbal and written communications skills.
 Experience in Banking or Financial services domain is preferred.
 Experience in working with financial platforms
 Experience in Cluster, Containers/VMs, On Premise, On Cloud
 Strong knowledge on Data Lake, Azure, ELT & ETL, Data Warehouse, BI, Data Factory Tools
 Strong in SQL preparation in Oracle/SQL Server/NoSQL/RDMS/Big Data


 Preferences:

 Awareness of Agile QA Software development life cycle –backlog, sprints, standups, burndowns.


 Pay Range: $90,080-$135,120/year
 
 Actual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!
 
 Why LPL? 

At LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.

 We are one team on one mission. We take care of our advisors, so they can take care of their clients.

 Because our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.

 Want to hear from our employees on what it’s like to work at LPL? Watch this!

 We take social responsibility seriously. Learn more here

 Want to see info on our benefits? Learn more here

 Join the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.

 Information on Interviews:

 LPL will only communicate with a job applicant directly from an @lplfinancial.com email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210."
iPhone Data Analysis/Software Engineer,Apple,"Austin, TX",Posted 14 days ago,,https://www.indeed.com/rc/clk?jk=847f497c16054673&fccid=c1099851e9794854&vjs=3,"Summary


      Posted: Feb 1, 2022
     


       Role Number:
       200339450



      Apple’s iPhone Hardware Engineering organization is looking for a highly motivated engineer with a real passion in both hardware and software. As a member of the iPhone modeling and algorithm team, you will work on system-level modeling of iOS devices and build ground breaking simulation platform to guide the critical design decisions including quantifying the tradeoffs between HW, SW or a hybrid approach. You will have the opportunity to influence the architectural roadmaps and high-level system specifications for the future iOS devices. This is an extraordinary cross-disciplinary opportunity and it requires intensive team collaboration effort coupled with strong system-engineering background.
     









Key Qualifications




Must have proven understanding on embedded system architecture. Deep knowledge in system-engineering architectural and implementation tradeoffs.
Strong programming skill (Matlab, C/C++) and proficient scripting experience (Perl/Python).
Significant experience in data analysis, visualization and data mining.
Have significant experience in crafting auto data validation/analysis tools or regression platform
Web-development experience is a plus
Familiarity with battery technology, logic design or circuit design is a plus
Power measurement and sensing experience is a plus
Excellent communication, persuasion and negotiation skills










Description



       We are looking for an engineer capable of handling challenges from “cradle to grave”. We love self-motivated teammates who are committed to continually innovate. Bring your passion and dedication and you will discover excitement and endless possibilities in the process of building our next-gen products in Apple. In this role you will be responsible for: • Support automated power data collection infrastructure for latest iPhone. • Build visualization tool for x-functional team to digest the power performance data • Build high-level simulation framework for future iOS devices and new power-saving technologies • Build internal web based tools • Perform analytics and data analysis across gigantic amount of data generated by multiple generation of iPhones • Analyze the field data and provide the guidance to the x-function teams for feature improvements • Use case power analysis • Drive the future power-saving algorithms/techniques • Some international travel expected
      








Education & Experience



       • Bachelor degree or higher in CS/EE/CE/Mathematics
      








Additional Requirements"
"Engineer Leadership, Data | General Interest",OJO Labs,"Austin, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=ec43d55f962b0c92&fccid=9bfcece151789df9&vjs=3,"OJO is committed to fostering an environment where all employees and candidates can thrive, grow, and lead. We strongly encourage members of marginalized communities, including people of color, lesbian, gay, bisexual, transgender, queer and non-binary people, veterans, parents, and individuals with disabilities, to apply with OJO. If you feel like you don’t meet all of the requirements for this role, we encourage you to apply anyway. We know the confidence gap and imposter syndrome gets in the way of meeting incredible candidates and we don’t want anything to get in the way of meeting you. If you need any assistance completing any forms or to otherwise participate in any portion of our interview process, please reach out to recruiting@ojolabs.com.
 About OJO

OJO is an Austin-based real estate technology company guiding more people to successful homeownership. The company’s platform for buying, selling, and homeownership meets people wherever they are on their journey, offering personalized guidance every step of the way. Through a bespoke combination of people and technology, OJO cultivates a deep understanding of individual needs and preferences, matching people with the right tools and trusted providers to equip anyone to unlock the abundant benefits of home ownership.
 The OJO residential search site, Movoto by OJO, provides personalized recommendations and highlights listings best suited to each consumers’ needs and preferences, so they never miss a home that might be the right fit. OJO Homeowner, the home management and finance tool, helps consumers uncover hidden savings, monitor equity, and plan for the future, whether managing their home as an asset or identifying the right time to sell. OJO also has a network of more than 30,000 top-rated real estate professionals, the OJO Select Network, and matches each consumer with the expert who can best support their individual journey.
 As one of the fastest-growing companies in the U.S., OJO Labs placed 29th on the Deloitte Tech Fast 500 and 49th on the Inc. 5000. To date, the company has raised more than $140 million to fuel its rapid growth. CEO and Founder, John Berkowitz, has been named EY Entrepreneur of the Year for Central Texas and a top CEO by the Austin Business Journal and the company’s executives have won more than a dozen industry awards in the past year. OJO Labs is headquartered in Austin, Texas and has more than 700 employees globally with a presence in Chicago, Minneapolis, and San Mateo; and a large operations center in St. Lucia.
 We consistently incorporate social and environmental impact into decision-making at OJO because we consider it important to the success of our business and our overarching purpose to equip anyone to unlock the abundant benefits of homeownership. From saving to buying, owning to selling, we guide people through all the twists and turns along the way. So they can unlock more than just the front door. 



About OJO Engineering
 At OJO Engineering, we highly value a thirst for learning, the ability to collaborate, and passion for our customers. We are excited to tackle hard technical problems. We take our responsibility seriously as we use technology to help our customers make important, sometimes life-changing, buying decisions. If you enjoy working in a fast-paced environment alongside some of the best engineers in Austin, come join us!
 We’re looking for a mindful, empathetic, and self-aware Senior Engineering Manager to join our team to help us move our product and engineering teams forward.
 About the OJO Aegis + Odin Data Intelligence Team:
 Our Aegis and Odin Data Intelligence teams are comprised of Product and Engineering talent. The team's mission is the unification and democratization of data and enablement of OJO to make informed decisions based on that data. As our Engineering Manager of these data teams, you will be influential in helping to scale our solutions and play a vital role in data democratization, data strategy, and data governance. You will lead our data engineers, develop the vision around our data warehouse and products, and transform our data into business and customer value.


Responsibilities:

Plan and support career development for the members of your team.
Inspire and grow engineers through mentorship, empowerment, and by challenging them to do their best work.
Ship features that will delight our users.
Partner with product management and design to help set priorities and plan the roadmap for your team.
Facilitate technical design discussions, remove blockers and align stakeholders
Drive programs in engineering reliability, productivity, process, and quality
Recruit and retain best-in-class engineering talent to build a diverse high performing team
Inspire an atmosphere 



What we need from you:

You have a passion for mentoring and coaching engineers
Lead our Data engineering across OJO Lab's data platforms, pipelines, and products to help shape the future of what we build using data.
Champion a data-driven culture by spearheading the company through building a modern data architecture stack.
Have a broad background in big data, data warehouse modernization, and analytics in a cloud environment.
Have built and led geographically distributed data engineering teams developing scalable data solutions.
Identify opportunities and find the win-win-win solution for your team and OJO
Drive data solutions and features that will impact business decisions and our product roadmap.
5 (+) years of experience in data warehousing, data architecture, and/or data engineering environments, leading small to medium-sized teams.
You are flexible, dedicated, and curious
You believe in the craft of software and data engineering, but are pragmatic when necessary
Proven interests in OJO and attention to detail. Please put “Ada Lovelace” in your resume or your cover letter.
You have a track record of delivering features and functionality on time with high quality
You have excellent communication skills



What do we have to offer?

Equitable Pay Practices
Equity
Flexible PTO Policy
20+ Paid Company Holidays
Options for 100% Coverage of Employee and Dependent Medical Health Premiums
Robust Ancillary Benefit Offerings Including Dental, Vision, Life Insurance and Disability Coverage, and Pre Tax Savings Accounts
Talkspace Access for Employees and Their Dependents
Home Office Stipend
Monthly Wellness Stipend
Generous Paid Parental Leave
Investment in Continued Learning
Promote from Within Philosophy
Volunteer Program
Dog-friendly Workplace

Ready to join us? Here’s what to expect next:
 Step 1: Complete the application below
 Step 2: Recruiter Phone Screen
 Step 3: Hiring Manager Video Interview
 Step 4: Subject Matter Expert/Technical Video Interview
 Step 5: Role-specific Project Assignment + Final Round Interview
 Diversity and Inclusion at OJO:
 OJO is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. We provide equal employment opportunity to all employees and applicants for employment and do not discriminate on any basis prohibited by law, including race, color, sex, gender, sexual orientation, gender identity or expression, pregnancy, age, religion, national origin, disability, marital status, and veteran status. We provide equal employment opportunities at all stages in the employment process, including hiring, recruitment, selection, compensation, benefits, promotion, demotion, layoff, termination, and all other terms and conditions of employment. Further, all of our employees have a responsibility to treat others with dignity and respect at all times.
 Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are typically done which will ensure an equal employment opportunity without imposing undue hardship on OJO. Please reach out to recruiting@ojolabs.com if you need assistance completing any forms or to participate in any portion of the application or interview process."
Senior Data Engineer,Wunderman Thompson,"Remote in Austin, TX",Posted 12 days ago,Full-time,https://www.indeed.com/rc/clk?jk=31e9ab4e9fcf4770&fccid=fb09a4b702935704&vjs=3,"Who we are:
 At Wunderman Thompson we exist to inspire growth for ambitious brands. Part creative agency, part consultancy and part technology company, our experts provide end-to-end capabilities at a global scale to deliver inspiration across the entire brand and customer experience.
 We are 20,000 strong in 90 markets around the world; our people bring together creative storytelling, diverse perspectives, inclusive thinking, and highly specialized vertical capabilities to drive growth for our clients. We offer deep expertise across the entire customer journey, including communications, commerce, consultancy, CRM, CX, data, production, and technology.

 Who is Wunderman Thompson MAP
 Wunderman Thompson MAP is the world-leading center of excellence for Marketing Automation, Personalization, Loyalty and CRM at scale. Our mission is to deliver value for our clients by humanizing the relationship between the brand and the consumer. To do this, we help clients make data-driven and personalized experiences that can be scaled with efficiency and operationalized intelligently. We believe that serving consumers personal, mindful content in an omnichannel world is key to transforming experiences. With the brain of a consultancy, the heart of an agency and the power of technology and data, we work with some of the world's most admired brands to help them on their transformation journey to becoming truly customer-centric.
 At Wunderman Thompson MAP, we are always making room for more. We are 800+ technology specialists, data scientists, strategic thinkers, consultants, operations experts, and creative minds from 40+ nationalities who collaborate closely to help our clients inspire and engage consumers on five continents.
 Who we are looking for:
 Are you a Senior Data Engineer with a passion to build, support & maintain digital marketing solutions that offers real value? Are you interested in working with the largest global brands and complex challenges in the media analytics space? Then you might be the Senior Data Engineer we're looking for!
 What will your day look like?
 As our new Senior Data Engineer, you will become part of our growing Data Insights and Science team. Here, you will employ new technologies across multiple cloud platforms to help successful brands reach their next level in 1:1 retargeting, communication and CRM. More specifically, your tasks include:

Identify, collect, and integrate data from various sources by building high quality data pipelines and data models for analytics and business intelligence (BI) purpose.
Develop and optimize code to enable pipelines at minimum cost and ease of maintenance.
Build monitoring procedures & tools to ensure solid ETL flows and data quality.
Design processes and tools to correct ETL incidents.
Collaborate with CRM developers, data scientists and data analysts and product owners to ensure the supplied data supports the business initiatives.
Consult our data analytics teams to ensure best practices on the technical use of data are followed.
Design data architectures and collaborate in data migrations in cloud environments.Who are you going to work with?

You will join a team of highly skilled Architects, Data Scientists and Consultants who are passionate about unlocking insights from data through analytics. You will also get to work closely with experts from other Technology, Creative and Client Teams.
 What do you bring to the table?
 As a person, you have a team player mindset and an open-minded attitude. You can communicate ideas and technical topics honestly and clearly – also to non-experts – while respecting the views of others on the team. You are eager to understand and find solutions, allowing you to quickly adapt to changing situations and come up with new ideas.
 At the same time, you solve problems in an analytical and pragmatic manner. Moreover, you have:

4+ years of experience in data engineering, big data, business intelligence or data science
4+ years of experience in Spark, Python, Scala or similar.
Excellent SQL skills enabling large scale data transformation and analysis.A comprehensive understanding of cloud data warehousing, data pipelines and data transformation (extract, transform and load) processes and supporting technologies such as Google Dataflow, Looker, DBT, EMR, CI/CD Pipelines, Airflow DAGs, and other analytics tool.
Experience with cloud based data infrastructures (Ideally GCP, but AWS or Azure would also suffice)
Programming experience in Javascript, Python or similar.
Expertise in managing databases, including performance tuning, backup and recovery.
Solid knowledge of data quality management best practices, including data profiling, data cleansing, and data validation
Knowledge of strengths and limitations of visualization tool (e.g. PowerBI, Tableau, Looker etc) in terms of data modelling in the visualization tools.
Understanding of versioning control tools like GitHub to changes related to dbt models and transformations, ensuring that changes are tracked, documented, and reviewed by the appropriate stakeholders.
Knowledge of applying data governance principles, policies, and practices that ensure data accuracy, consistency, and security
Knowledge of Kubernetes would be an advantage

Personal skills:

Strong desire to contribute towards keeping data tidy and well organized
Ability to think critically, identifying issues autonomously, and proposing corrective actions.
Build great relationships with your team and stakeholders
A leader in personalised customer experiences

What we offer:

Passionate, driven people | We champion a culture of people that do extraordinary work.
Consciously cultivated culture | We aim to embody the behaviors to build an inclusive community that is in it together, bringing both positivity and active listening into the workplace as we simultaneously strive to empower creative bravery.
Competitive benefits | What we offer full time hires ranges from the full spectrum of group health coverage options (medical, dental, vision) to a generous 401k match (100% dollar-for-dollar match, up to 5% of salary contribution), and a variety of paid time off offerings that reflect our investment in all aspects of your overall life balance and wellness.
Growth-minded opportunities | We aim to nurture a culture of real-time feedback, growth-oriented mindset, and plenty of training opportunities through Wunderman Thompson and WPP, so you can continue to grow personally and professionally.

Wunderman Thompson is an equal opportunity employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability, or other protected group status. We believe in creating a dynamic work environment that values diversity and inclusion and strives to recruit a diverse slate of candidates to help us achieve that goal.
 #LI-JK1


    The base salary range for this position at the time of this posting is indicated below. Individual compensation varies based on job-related factors, including location, business needs, level of responsibility, experience, and qualifications. We offer a competitive benefits package, click WPP Benefits for more details.
  
 _

    $80,000—$140,000 USD
  


 At Wunderman Thompson, we are committed to actively building a diverse, equitable and inclusive workplace where everyone feels welcomed, valued and heard, and is treated with dignity and respect. As leaders and creative partners across industries, it is our responsibility to cultivate an environment reflective of our greatest asset; our people. We believe that this commitment inspires growth and delivers equitable outcomes for everyone as well as the clients and communities we serve.
 Wunderman Thompson is a WPP agency. For more information, please visit our website and follow Wunderman Thompson on our social channels via Twitter, Facebook, LinkedIn, and Instagram.
 Note: We rely on legitimate interest as a legal basis for processing personal information under the GDPR for purposes of recruitment and applications for employment.
 When you click the ""Submit Application"" button below, this will send any information you add below to Wunderman Thompson. Before you do this, we think it's a good idea to read through our Recruitment Privacy Policy. California residents should read our California Recruitment Privacy Notice. This explains what we do with your personal data when you apply for a role with us, and, how you can update the information you have provided us with or how to remove it."
Data Engineer,PILYTIX,"Remote in Austin, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=a6bb0948b54cbabf&fccid=dcd0a0f2e35971d1&vjs=3,"PILYTIX brings Explainable Artificial Intelligence (XAI) to sales and fundraising teams with software-as-a-service products that enable them to be more effective throughout their entire sales funnel. Data Engineers will assist in the development of PILYTIX's cutting-edge zeroG™ platform by creating and maintaining the data pipelines that ingest, transform and archive client data. They will also build internal tools to support our team of Data Scientists and Software Engineers, powering the PILYTIX products to help our clients win more revenue, faster.

 Responsibilities:

Author and monitor directed acyclic graphs (DAGs) in Apache Airflow to ingest and transform data 
Build and maintain internal Python packages to streamline ingest processes and add connections for new types of data
Manage Kubernetes infrastructure as well as PostgreSQL and BigQuery databases on Google Cloud Platform
Work collaboratively with the app development team to add new data-driven features to our software-as-a-service product
Work collaboratively with the data science team to enhance our AI and machine learning capabilities
Participate in Agile / Kanban processes on a daily basis
Comply with change management policies and code reviews to ensure data integrity and system stability

Requirements

BS/MS in a STEM field and 2+ years of software industry experience programming and working with data
Exceptional understanding of data architecture and software engineering best practices
2+ years experience with Python (Python 3 preferred)
2+ years experience with SQL (PostgreSQL & BigQuery preferred)
2+ years of experience with Docker 
1+ years experience with cloud infrastructure (GCP preferred)
1+ years of experience with server orchestration (Kubernetes preferred)
Experience using Apache Airflow or similar data pipeline systems
Experience using Git or other DVCS
Knowledge of Agile / Kanban processes
Entrepreneurial spirit and highly self-motivated

Job is based in Austin TX, but extraordinarily qualified remote candidates (willing to travel to Austin semi-regularly) may apply.
 Benefits

Competitive base salary with ability to earn bonuses
Professional development and entrepreneurial opportunities
Paid time off
401(k)
Medical and dental plans"
Principal Data Engineer,Digital Turbine,"Hybrid remote in Austin, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=f6503071ec81eb6b&fccid=b117c357fa57e851&vjs=3,"At 
  Digital Turbine, we make mobile advertising experiences more meaningful and rewarding for users, app publishers, and advertisers — intelligently connecting people in more ways, across more devices. We provide app publishers and advertisers with powerful ads and experiences that captivate consumers, fuel performance, and help telecoms and OEMs supercharge awareness, acquisition, and monetization. In a rapidly evolving industry, we are constantly innovating and creating better paths of discovery to connect consumers, publishers, and advertisers across the mobile ecosystem.
  




Key Responsibilities of the Principal Data Engineer: 

Guide the team towards successful project delivery
 Provide technical leadership and key decision making
 Work with and mentor individual team members to meet professional goals
 Continuous effort to automate and increase team efficiency
 Maintain high standards of software quality via code review, testing, automation standardization, and tooling
 Collaborate directly with both developers and business stakeholders
 Provide estimates and risk-reducing spike stories
 Assist in collection and documentation of requirements from business
 Assist in planning deployments and migrations
 Prepare status updates and run the daily standup
 Analyze data problems and provide solutions
 Assess opportunities for improvements and optimization

 Required Qualifications of the Principal Data Engineer: 

Master’s Degree or equivalent work experience
 Minimum 12 years of experience working with open source databases
 Minimum 10 years of experience working with ETL and related data pipeline technologies.
 Highly proficient in open source SQL systems, particularly MySQL and PostgreSQL
 Proficiency in multiple scripting languages, including Python and Ruby
 Demonstrable experience with cloud-based ETL tools, including EMR,
 Expertise with distributed data stores, with demonstrable experience using Redshift and with optimizing query performance
 Deep understanding of data structures and schema design
 Prior work with AWS ecosystem, particularly RDS, SQS and SNS, StepFunctions, CDK)
 Exceptional analytical, organizational, interpersonal, and communication (both oral and written) skills
 Self-motivated, driven, resourceful, and able to get things done
 Enjoy working in fast-paced, collaborative, Agile environments
 Ability to adapt and learn quickly is a fundamental necessity


   #LI-RJ1#LI-Hybrid
 


 About Digital Turbine:



 Digital Turbine (NASDAQ: APPS) powers superior mobile consumer experiences and results for the world’s leading telcos, advertisers and publishers. Our end-to-end platform uniquely simplifies the ability to supercharge awareness, acquisition and monetization — connecting our partners to more consumers, in more ways, across more devices.
  




  The company is headquartered in Austin, Texas, with global offices in New York, Los Angeles, San Francisco, London, Berlin, Singapore, Tel Aviv, and other cities serving top agency, app developer, and advertising markets. Listed on Deloitte Technology Fast 500 for six consecutive years since 2015 and winner of Austin Chamber of Commerce’s Company Culture in 2020.
 


 Digital Turbine is an equal opportunity employer committed to building a diverse and inclusive team. We welcome people of different backgrounds, experiences, abilities, and perspectives. We embed diversity in our mindset, products, and teams to empower an inclusive, equitable, and culturally fluent environment. Building this culture within our teams makes us better collaborators and partners, driving better outcomes.
 


 To view our 
  Global Recruitment Privacy Notice, please click here."
"Manufacturing Engineer, Data Scientist",Tesla,"Austin, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=826a3783a73d2c1c&fccid=86e9be6ce380173e&vjs=3,"What to Expect
 

    Under Tesla’s Manufacturing engineering organization, manufacturing engineers lead installation, commissioning, and ramping up some of the most advanced manufacturing equipment that produce Tesla’s cutting-edge vehicle powertrains and energy storage. The Power Electronics and Energy Products manufacturing engineering team is responsible for equipment installation, alignment and calibration of equipment, quality validation (PFMEA/Process Control Plan), root-cause analysis to improve OEE (Overall Equipment Effectiveness) as well as managing the integration of the manufacturing operating systems.
  
 The data scientist in the Power Electronics and Energy Products manufacturing engineering team collaborates with members of design, production, quality and manufacturing engineering teams to improve our most-difficult manufacturing processes and equipment through data science, experimentation, and modeling to drive tangible optimizations to the production environment. The data scientist also trains others to improve the sophistication and effectiveness of their scientific investigations within the organization.
 What You’ll Do
 

 Collaborate with manufacturing and design subject matter experts to develop analytics methods to qualify and ramp assembly and test processes, and improve process capability and stability, cycle time, first pass yield, performance, availability, scrap, cost, and product quality and reliability
 Build manufacturing data sources/schemas and create ETL (Extract Transform Load) pipelines
 Own the development and deployment of critical manufacturing metrics, analyses, and dashboards
 Build advanced analytics including using ML/AI methods to improve manufacturing process, equipment, and line performance, and product quality and reliability
 Create the data and analytics roadmap for the power electronics and energy storage manufacturing engineering program

 What You’ll Bring
 

 3+ years of work experience applying data analytics in a high-volume manufacturing environment
 Proficient with the following tools or equivalent: databases and acquisition (SQL), statistical analysis (JMP), visualization (Tableau), and general-purpose programing (Python, R)
 Experience building dashboards, analysis reports, and solving real manufacturing problems with data
 Skillful in time series analysis, multivariate models, hypothesis testing, A/B testing, inferential statistics, and causal analysis
 Experience in some of the machine learning areas: Anomaly detection and signal processing, time-series data analysis and modeling, Conventional Machine Learning methods, Computer Vision, Deep learning, CNNs
 BS in Computer Science, Statistics, Mechanical engineering, or other STEM areas"
Data Engineers,ANALYTOS,"Austin, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=6c7fde820395515f&fccid=9a4f13cfc6042a6a&vjs=3,"Qualifications: BSc IT , MSC IT , MCA
 
Experience : 0 to 2 years 
 A good data engineer is has extensive knowledge on databases and best engineering practices. These include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding what is necessary to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning, and ensuring a deterministic pipeline. 
 
Job Description
 The Core Data Engineer - Analyst will: Lead the discussion around Hadoop Clusters and our Big Data technology platform.
  Lead the discussion around monitoring and control of our data warehouse systems
  Work as a part of a cross-functional team to ensure data is flowing from transaction systems, logs, etc. into our processing pipelines
  Work with Data Scientists to ensure demands are met for data analysis
  Ensure task queues are processing jobs efficiently at scale
  Work with Architecture to diagnose complex software systems and discuss security and performance concerns early in the process
  Lead research on emerging technologies
  Lead troubleshooting efforts, but not be solely responsible for them
  Train and mentor other engineering team members around service discovery, configuration management, and provisioning technology
  Managing Infrastructure like Laptops / Desktops / Servers in case of any issues.
 
Required Skills
 You thrive in an informal work environment with flexible hours but inflexible deadlines.
  You have displayed an aptitude for picking up new skills and approaches and are eager to broaden your knowledge of the use and analysis of data to solve real-world problems.
  You are willing to engage in continuing education through both formal and informal studies to further develop your skills.
  Analytical thinker and problem solver. 
 
Desired Skills Relevant work experience in managing Windows / Linux distributions, patching, updating in a large scale environment
  Relevant work experience setting up Tableau or other visualization tools
  Experience working with Hadoop, MapReduce, HBase, ZooKeeper, and other relevant technology
  Coding/Scripting ability in Bash + Ruby, Python, Scala, or Java
  Experience scaling API environments such as Apache, Nginx, Node/Express, Ruby/Sinatra
  Experience working with distributed systems and partitioning of data
  Good Communication Skills.




 For applying for these positions email your resume on 
 careers@analytos.com"
Data Engineer,Apgar Consulting,"Austin, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=990257e6edfa4b02&fccid=6781a4a26ea3b732&vjs=3,"Apgar North America is looking to recruit Data engineer – Master Data Management to join the growing team in the US and be part of a group of globally diverse teammates passionate about data.
 If you are looking for personal development, entrepreneurship experience, and career growth;
 If you are looking for a company where you can influence
 If you are looking for a true people-centric environment;
 Keep reading.







     We’ll tell you everything. 
    

Solution implementation 


       As a Senior Data Engineer ( MDM Consultant / MDM Engineer), you will take a leading role in implementing data platform solutions for our clients. You will work within a team on a customer project and will benefit from a manager or a more senior peer’s supervision. You will participate in international projects and communicate with teammates and clients.
      







      From analysis to implementation 
     


       You will analyze internal processes and data usage, identify improvements, and implement sustainable and scalable change.
      

        Your primary role will be to participate in all stages of a data platform implementation project based on various leading technologies.
      






      Take part of Apgar's development 
     


       You will also participate in Apgar’s development, in our innovation process, and you will be given the opportunity to be part of:
      

The operational management of a consulting firm
The daily life of a growing organization







      Skills required 
     


       Your experience, skills, and competencies set you apart:
      

Bachelor’s degree (preferably in Computer Science, IT, Engineering, Mathematics, or similar)
Excellent communication and interpersonal skills
Problem solver with a keen eye for details and an innate drive to deliver results
Able to engage yourself in a project and enjoy the challenges and processes involved
Legally authorized to work in the US


       Your thirst for entrepreneurship and your willingness to participate in our North America development are decisive factors.
      






      Required profile 
     


       Dynamic and self-motivated, you have lots of enthusiasm, and endless energy to learn. You enjoy working in a team, and you can communicate proficiently to both business and technical audiences.
      

        You have a minimum of 5 years of experience in Master Data Management, conducting requirements analysis and delivering technology solutions through consulting project activities.
      






      Practical considerations 
     


Start date: as soon as possible
Salary: attractive package (fixed + variable) depending on profile
Location: US







      Perks 
     


Flexible working hours and ability to work remotely
Competitive salary
Bonuses
Flexible PTO
Medical, dental, and vision insurance
Life insurance and short/long term disability insurance
Retirement plan with company contribution
Individual coaching from one of our partners and trainings
Great working environment and new equipment
US HQ office located in the most vibrant tech hub in Austin (The Domain)"
Senior Data Engineer,S&P Global,"Remote in Austin, TX 73301",Posted 26 days ago,"$70,300 - $139,800 a year",https://www.indeed.com/rc/clk?jk=d28f1e3355691cc4&fccid=b716e44d2c6283e7&vjs=3,"The Role: Senior Data Engineer 


The Team: As a member of the Data Operations team, you will help modernize the extensive data domain of the Issuer Solutions business. 


The Impact: As we redesign and reimagine our client-facing and internal tools, data quality and consistency across multiple tools and platforms will be a key factor in our success, and the Data Engineering team is tasked with ensuring that all data consumers, from developers to business analysts to clients, have tools to access the data they need in the format they need it. 


What’s in it for you:
 Designing and implementing data-ingestion and data-publishing tools for diverse datasets. 
Implementing and maintaining data-monitoring and data-cataloguing tools. 
Implementing and maintaining process-management and data-movement tools for ETL’s. 
Offering guidance and best practices to scrum teams who work with data. 
Evaluating and implementing new data management technology with the goal of continually improving team efficiency and data quality. 
Providing support and guidance for business analysts using data analysis tools such as Alteryx and PowerBI. 


Responsibilities:
 Minimum of 3+ years experience as a software engineer 
Experience as a Data Engineer in a production environment 
Focus on ETL’s, data monitoring, data engineering, etc. 
Python, C#, Docker, MS SQL Server, PostgreSQL 
Experience with AWS and/or Azure DevOps 

AWS tech: Aurora DB, Glue, Lambda 
Alteryx Server 
Infrastructure as code using Terraform/CloudFormation 
Solid understanding of containers and orchestration tools (Docker, CI/CD, etc.) 
AirFlow, DataHub, NiFi and other data-management and process-management tools a plus 
Experience with BI and analytical tools such as PowerBI and Alteryx 

 Flexible Working (optional) 
We pride ourselves on our agility and diversity, and we welcome requests to work flexibly. For most roles, flexible hours and/or an element of remote working are usually possible. Please talk to us at interview about the type of arrangement that is best for you. We will always try to be adaptable wherever we can. 

 Return to Work 
Have you taken time out for caring responsibilities and are now looking to return to work? As part of our Return to Work initiative (link to career site page when available), we are encouraging enthusiastic and talented returners to apply, and will actively support your return to the workplace. 


Grade/Level ( relevant for internal applicants only ): 10 


The Location: Florida, Georgia, New York, Virginia, Connecticut, Ohio, Delaware 


Compensation and Benefits Information:
 S&P Global states that the anticipated base salary range for this position is $70,300 - $139,800. Base salary ranges may vary by geographic location. 
In addition to base compensation, this role is eligible for an annual incentive bonus plan. 

 This role is eligible to receive additional S&P Global benefits. For more information on the benefits we provide to our employees, visit Our Benefits (spgbenefits.com) . 


About Company Statement:
 S&P Global delivers essential intelligence that powers decision making. We provide the world’s leading organizations with the right data, connected technologies and expertise they need to move ahead. As part of our team, you’ll help solve complex challenges that equip businesses, governments and individuals with the knowledge to adapt to a changing economic landscape. 

 S&P Global Market Intelligence partners with customers to broaden their perspective and operate with confidence by bringing them leading data sources and technologies that embed insight in their daily work. 

 ----------------------------------------------------------- 

 Equal Opportunity Employer 
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment. 

 If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. 


US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law. 

 ----------------------------------------------------------- 20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group) 


Job ID: 288863 

Posted On: 2023-06-22 

Location: Virtual, North Carolina, United States"
Senior Data Operations Engineer,BetterUp,"Remote in Austin, TX",Posted 30+ days ago,"$136,850 - $193,000 a year",https://www.indeed.com/rc/clk?jk=765f4ee1b25a3400&fccid=2345e2002bd590b5&vjs=3,"Let's face it, a company whose mission is human transformation better have some fresh thinking about the employer/employee relationship.   We do. We can't cram it all in here, but you'll start noticing it from the first interview.   Even our candidate experience is different. And when you get an offer from us (and accept it), you get way more than a paycheck. You get a personal BetterUp Coach, a development plan, a trained and coached manager, the most amazing team you've ever met (yes, each with their own personal BetterUp Coach), and most importantly, work that matters.   This makes for a remarkably focused and fulfilling work experience. Frankly, it's not for everyone. But for people with fire in their belly, it's a game-changing, career-defining, soul-lifting move.  Join us and we promise you the most intense and fulfilling years of your career, doing life-changing work in a fun, inventive, soulful culture.   If that sounds exciting—and the job description below feels like a fit—we really should start talking.
 We're looking for a Senior Data / DataOps engineer who cares deeply about their craft, and who wants to use their skills to bring about positive change in the world while working in a high-performing organization. On the Data Operations team, our mission is to architect, build and operate a world class data platform enabling teams to apply analytics and ML for social good. We are product engineers that strive to enable all BetterUppers to build data driven products that further our mission.
 We're looking for someone who is comfortable in the rapidly changing nature of a startup environment but also adept at moving relentlessly forward: doing what needs to be done to unblock projects that truly deliver value to our users. At BetterUp we delight in supporting and pushing each other to bring out the best in our colleagues, and would love someone to join the team who shares our passions for empathy, excellence, and continuous improvement. We also deeply understand that a key to peak performance is balance, and our culture is focused on providing the support our people need to be able to bring their whole selves to bear in service of our mission. 
What you'll do:

Data evangelist: Bring, build, and drive data culture and best practices, enabling the product and engineering org to build better, more reliable, and secure data pipeline and data-driven products and powering use cases spanning internal and customer-facing analytics, data science / ML needs, and in-app experiences
DevEx delighter: Use tooling and automation to deliver a developer experience that enables teams to quickly and easily build out data products following mature SDLC principles
System designer: Passion for building systems, platforms, and tools that people use. You'll use your expertise in the broader data ecosystem and the modern architectures, approaches, and emerging technologies in this space, on top of a strong foundation on the fundamentals of building distributed systems in the cloud.
Act as an owner: It may start with a proof of concept but it's not done until it's in production. Adept at moving projects forward and able to unblock projects regardless of where we are in the development lifecycle.
Do less, deliver more: Familiar with the terms YAGNI and yak shaving? Focus your efforts on high-impact initiatives that really move the needle.
Impress yourself: We hold ourselves to quality above and beyond something that ""just gets it done."" Each system or line of code is an opportunity to demonstrate craftspersonship.
Collaborate without ego: Work together with teams to drive cross-team and cross-functional technical roadmaps, and willing to take on roles small or large in order to further the mission at hand.

If you have some or all of the following, please apply:

4+ years of relevant data engineering, data infrastructure, DataOps / MLOps, DevOps, SRE, or general systems engineering experience (high growth startup experience is a plus)
A leader for your teammates and driver of large cross functional projects within your organization
Familiarity or expertise using and maintaining modern data platform technologies and services like Kafka, Airflow, Snowflake, Segment, Stitch, Fivetran, dbt, Looker, etc.
Familiarity or expertise using and maintaining ML tooling and platforms like AWS Sagemaker, GCP Vertex AI, BentoML, MLFlow, Kubeflow, etc.
Experience doing infrastructure-as-code using tools like Terraform, Ansible, Chef, etc., and a pathological inclination towards automation and CI/CD
Full lifecycle ownership up through production and experience with observability and monitoring tools like DataDog, Honeycomb, Sentry, etc.
Experience architecting and implementing data governance processes and tooling (such as data catalogs, lineage tools, role-based access control, PII handling)
Strong coding ability in Python (preferred) or other languages like Java, C#, Golang, etc., and a solid grasp of SQL fundamentals 

Benefits:
 At BetterUp, we are committed to living out our mission every day and that starts with providing benefits that allow our employees to care for themselves, support their families, and give back to their community.

Access to BetterUp coaching; one for you and one for a friend or family member
A competitive compensation plan with opportunity for advancement
Medical, dental and vision insurance
Flexible paid time off
Per year:

All federal/statutory holidays observed
4 BetterUp Inner Work days (https://www.betterup.co/inner-work)
5 Volunteer Days to give back
Learning and Development stipend
Company wide Summer & Winter breaks

Year-round charitable contribution of your choice on behalf of BetterUp
401(k) self contribution

We are dedicated to building diverse teams that fuel an authentic workplace and sense of belonging for each and every employee. We know applying for a job can be intimidating, please don't hesitate to reach out — we encourage everyone interested in joining us to apply.  BetterUp Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, disability, genetics, gender, sexual orientation, age, marital status, veteran status. In addition to federal law requirements, BetterUp Inc. complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 At BetterUp, we compensate our employees fairly for their work. Base salary is determined by job-related experience, education/training, residence location, as well as market indicators. The range below is representative of base salary only and does not include equity, sales bonus plans (when applicable) and benefits. This range may be modified in the future.
 The base salary range for this role is $136,850 – $193,000.
 We value your privacy. Your personal data will be processed in accordance with our Privacy Policy. If you have any questions about the privacy of your personal data or your rights with regards to your personal data, please reach out to support@betterup.co
 #LI-Remote"
Principal Data Engineer (R-14645),Dun & Bradstreet,"Hybrid remote in Austin, TX 78752",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=573bb7609cfe51c4&fccid=fa0ca3b638673d62&vjs=3,"Why We Work at Dun & Bradstreet


   Dun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!
 


 As a Principal Data Engineer you will be responsible for designing, managing, and optimizing an organization's data architecture. You are an expert in data modeling, database design, data integration, and data governance. You will play a crucial role in ensuring that an organization's data is organized, accessible, secure, and aligned with business goals. Are you someone that can thrive in a high energy, high growth, fast paced environment and curious? Then you might be just who we are looking for!
 
What You'll Do:

 Collaborate with stakeholders, business analysts, and IT teams to understand data requirements and develop an overarching data strategy aligned with business objectives.
 Define data architecture principles, standards, and guidelines to ensure consistency and interoperability across different systems and data sources.
 Develop conceptual, logical, and physical data models that accurately represent the organization's data requirements and business processes. 
Designing, implementing, and supporting a platform consumed by millions of users where performance is the key, collaborate with solution architects, business, QA and project managers.
 Create documentation, reports, and dashboards to support the effective management and operation of the platform.
 Design and oversee the implementation of data integration processes, including Extract, Transform, Load (ETL) workflows, data pipelines, and data integration patterns.
 Act as a subject matter expert on data architecture and provide guidance and mentorship to other team members.
 Application support/ bug fixes. 

Requirements (Must Have):

 Must have at 10-15 years proven experience as a data architect or in a similar role, designing and implementing data architectures.
 Bachelor’s degree in computer science, information systems, or other related field or equivalent work experience.
 Experience with data bases, including relational and non-relational, design and create tables, views, triggers, partitions, complex stored procedures, functions, indexes and other database objects.
 Experience with data modelling & mapping knowledge along with experience in XML & JSON document formats, strong in T-SQL, ETL & data processing using SSIS.
 Strong knowledge of Microsoft SQL Server and SSIS also software design, data structures and algorithms. 
Must be familiar with database design, scalability, and performance.
 Good understanding of AWS S3, EC2, Windows operating systems and networking.
 Good understanding web-based applications and REST APIs.
 Passionate and curious about data with the ability to query and analyze datasets in order to drive fact-based decisions.
 Able to work independently with minimal supervision.
 Excellent communication skills – written, verbal, presentation and interpersonal.
 Willingness to learn new skills and implement new technologies.


 Benefits We Offer


 Generous paid time off in your first year, increasing with tenure.
 Up to 16 weeks 100% paid parental leave after one year of employment.
 Paid sick time to care for yourself or family members.
 Education assistance and extensive training resources.
 Do Good Program: Paid volunteer days & donation matching.
 Competitive 401k & Employee Stock Purchase Plan with company matching.
 Health & wellness benefits, including discounted Gympass membership rates.
 Medical, dental & vision insurance for you, spouse/partner & dependents.
 Learn more about our benefits: http://bit.ly/41Yyc3d.



 All Dun & Bradstreet job postings can be found at https://www.dnb.com/about-us/careers-and-people/joblistings.html. Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.
 


 Equal Employment Opportunity (EEO):
 Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law. View the EEO is the Law poster 
here
 and its supplement 
here.
 View the pay transparency policy 
here
. 




  Global Recruitment Privacy Notice"
Senior Data Engineer (Databricks),Apixio,"Hybrid remote in Austin, TX",Posted 30+ days ago,"$130,000 - $160,000 a year",https://www.indeed.com/rc/clk?jk=cba5ce41eee17550&fccid=d709010bf50f2482&vjs=3,"Who We Are:
 Apixio is advancing healthcare with data-driven intelligence and analytics. Our Artificial Intelligence platform gives organizations across the healthcare spectrum the power to mine clinical information at scale, creating novel insights that will change the way healthcare is measured, care is delivered, and discoveries are made.



Who you are:

 Experience designing and developing data engineering solutions.


 Support analytics, data science and data platform teams and understand their unique needs and challenges.


 Design, implement and manage data flows that integrate information from various sources into a common pool implementing data pipelines based on ELT and ETL models.


 Build dashboards and query tools for cross-functional teams in the organization. 


 Develop datasets and models to meet the requirements of analysts and key stakeholders; optimizing query compute and data storage patterns for cost and performance


 Work closely with the engineering teams to troubleshoot issues with API's and data exchanges between multiple systems.

 What Knowledge, Skills & Abilities you bring to the table::

 5+ years Data Engineering experience including 


 3+ years hands-on experience working with large, structured/unstructured datasets using partitioned cloud storage architecture using query engines such as Spark, Delta Lake.


 3+ yearsExperience designing, developing, deploying and testing in Databricks. 


 3+ years of hands-on experience in Python/Pyspark/SparkSQL.


 2+ years experience on Big data pipelines/DAG tools like Airflow, dbt is required.


 2+ years of SQL experience, specifically to write complex, highly optimized queries across large volumes of data.


 Experience in the AWS computing environment and storage services such as s3/glacier is required.


 Experience with conceptual, logical and/or physical database designs is required.


 Good knowledge in Linux and shell scripting is highly desired.


 Past experience in healthcare data extraction, transformation and normalization is highly desired.


 Hands-on experience with Kafka or other live streaming technology is highly desired.


 Experience with Data Visualization tools like Looker, Tableau is desired.


 Strong communication skills to relay complex data integration requirements to team members.

 After 3 months you will have accomplished: 

 Develop a very good understanding of Apixio's products and existing DI tools and processes.


 Work closely with the R&D team to implement the building blocks of the next generation data integration platform.

 After 1 year you will have accomplished: 

 Instill excellence into the processes, methodologies, standards, and technology choices embraced by the team.


 Serve as a SME in migrating the legacy data Integration pipelines to the next generation Integration platform.



    The salary range below is for Base Salary. Total compensation also includes benefits and variable compensation. Compensation will be determined based on several factors including, but not limited to, skill set, years of experience, and the employee’s geographic location.
  
 Base Compensation

    $130,000—$160,000 USD
  


 We recognize that people come with experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Your skills and background may be more translatable to this role than you initially thought. Allow us the opportunity to get to know you. Please let us know if you require accommodations during the interview process.
 What Apixio can offer you:

Meaningful work to improve healthcare
Competitive compensation
Exceptional benefits, including medical, dental and vision, FSA
401k with company matching
Generous vacation policy
A hybrid work schedule (2 days in office & 3 days work from home) (Note: If the position is designated as REMOTE it will stay REMOTE)
Modern open office in beautiful San Mateo, CA; Los Angeles, CA; San Diego, CA; Austin, TX and Dallas, TX
Subsidized gym membership
Catered, free lunches
Parties, picnics, and wine-downs
Free parking

We take your privacy very seriously. Please review our privacy policy to see exactly how we protect your information here
 We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.
 Apixio will consider for employment qualified applicants with criminal histories pursuant to the San Francisco Fair Chance Ordinance subject to the requirements of all state and federal laws and regulations.
 If you are a recruiter or placement agency, please do not submit resumes to any person or email address at Apixio prior to having a signed agreement from Talent Acquisition. Apixio is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Furthermore, any resumes sent to us without an agreement in place will be considered your company's gift to Apixio and may be forwarded to our recruiters for their attention and no fee will be paid.


 LI-RB1"
"Senior Data Scientist / Data Engineer, Drive Systems",Tesla,"Austin, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=26ae858104c39641&fccid=86e9be6ce380173e&vjs=3,"What to Expect
 

    The Drive Systems team is looking for an exceptionally talented and self-directed data scientist/data engineer to manage a large scope of data improvement projects across our production processes, validation tests, and field reliability failures. This role requires a well-rounded individual who can build, scale, and maintain data pipelines and warehousing systems and also analyze data to evaluate process health, sleuth problem areas, and drive and quantify improvements. The role will include creating and improving robust global databases and data quality standards, as well as developing scalable evaluation models and reporting systems. A strong candidate will be able to work with existing factory data and process engineering teams to build data solutions as well as drive focused investigations and improvement projects based on analysis of the data.
  
 What You’ll Do
 

Define data standards and storage requirements and collaborate with IT to design efficiently structured storage solutions across a range of applications and users
Work with factory operations and field reliability teams to identify useful data sets and quantitative metrics across a range of processes and use cases and build visualizations and investigation tools
Develop and improve automated reporting and monitoring systems for key performance metrics and statistical process control
Perform exploratory analysis, correlation studies, design experiments, and analyze results to drive investigations and improvement projects
Define data formatting and fidelity requirements with suppliers and internal users, build data pipelines for structured and unstructured quality and process data, and move/transform data into structured database formats in automated pipelines to enable real-time analysis

 What You’ll Bring
 

Bachelor’s degree or higher in quantitative discipline (Statistics, Data Analytics, Computer Science, Applied Mathematics, Physics, Engineering) or the equivalent in experience and evidence of exceptional ability
Minimum 4 years relevant working experience in analytical or quantitative roles
Proficiency in programming languages such as Python, R, SQL, C#, JavaScript, Golang
Proficiency in data visualization methods and tools such as Tableau, R Shiny, Dash, Plotly, etc.
Proficiency in software deployment/management tools such as Docker, Kubernetes, Kafka, Jenkins, GitHub
Strong working knowledge of relational and/or non-relational databases
Significant experience with real-world automation, high volume manufacturing, and process control
Strong working knowledge of physics and engineering principles, mathematics, and statistics
Exceptional organization and self-direction, collaboration skills, and ability to drive efficient execution across multiple projects and priorities"
Principal Data Software Engineer,Atlassian,"Austin, TX 78701 (Downtown area)",Posted 9 days ago,Full-time,https://www.indeed.com/rc/clk?jk=42244461fab7ca18&fccid=e6d4ba9e2cfe7902&vjs=3,"Working at Atlassian



 Atlassians have flexibility in where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.



 Apply for the 
   Principal Data Software Engineer role at Atlassian and give yourself a chance to join one of our great US Based Product Development Teams working on our Enterprise Agility tools such as Jira Align. Have you heard of the Fortune 500 list? Yup, these are our Enterprise Agility Customers! They are BIG, with development teams reaching 50k users. Our tools allow data analysis of the largest software portfolios in the world. You will end up with your code helping millions of people all around the Globe!
  



What you'll do


 As a Data Architect for the Enterprise Agility Engineering team, you'll work closely with multiple software engineering teams and product squads to improve Jira Align.








You will refine and govern our logical data models which track team data for the largest software organizations in the world.
 You will guide designs and decisions for our data architecture through statistical analysis of production data.
 You will meet and work with our enterprise scale customers to put the personal touch on our engineering plans.
 We are also considering multiple database technologies to optimize the storage and retrieval for our end-users, including new solutions for data ingestion and translation. This exciting opportunity will engage you with our Site Reliability Engineering and Security teams as well the application development technical leads.
 You will work with design and product management team to translate our customer reporting requirements into improvements into our current mechanisms.
 You will work through our support and field organization to understand how our customers use our data today, to lead future changes as efficiently as possible.
 This role would be a great fit for an inclusive leader and coach, someone with a willingness to oversee the data architecture from every angle. You will develop and implement solutions that operate at scale - seeing your own technology efforts directly improve the product.








Report directly to the Senior Engineering Manager






Your background


 15+ years of experience in Software Development
 Public cloud experience in AWS and/or Azure
 Technical leader, able to pitch ideas and lead programs to trigger across multiple teams and departments
 Proficiency in SQL, python, Java, C# or another JVM-based language
 Success with designing and maintenance of systems with multiple dependencies
 Relational and NoSQL database design and best practices
 Previous experience in data ingestion and translation tools
 Experience managing customer data for Security and Privacy
 Data modeling and API design
 Deep understanding of and experience in cross-team collaboration in large-scale projects






About Teams


 Jira Align Data Team Owns multiple production services that transform the team level data into enterprise decision making tools.






Compensation


    At Atlassian, we tie our base pay ranges to role and level. In the United States, that means your base pay ranges will fall into one of three geographic pay zones depending on your location. Our current base pay ranges for new hires in each zone are:
  

    (Zone A: $187,500 - $287,500)
  

    (Zone B: $168,700 - $258,800)
  

    (Zone C: $155,600 - $238,700)
  

    Within each range, base pay is ultimately determined based on your skills, expertise, and experience. This role may also be eligible for benefits, bonuses, commissions, and/or equity.
  

    Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
  


 Our perks & benefits



 Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit 
   go.atlassian.com/perksandbenefits
 to learn more.
  


 About Atlassian



 At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
  


 We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
  


 To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
  


 To learn more about our culture and hiring process, visit 
   go.atlassian.com/crh
."
Data Engineer II,Belcan,"Austin, TX 78758 (The Domain area)",Posted 25 days ago,Contract,https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DXzDzZ1Oulz9LSjzVbF8otUHEujJfFPwzVdyJWZPnyGP21i8g1idx-A-BThzGW7o8YTukT4JHiY6Tb-lK8SJhlYqPjixWgfQKrtmFZibfdOCswg9osVsXAMMVVT1zNBLfZUVarJHeCAkbty3ThUYSSuigF52Srt1BFKDaMJbofbhpfnLEZ_h3w-3E598KS3C353luk6T2Lc7rJy6QLRKN_PDQk8pwy2uhZUiHsaQkb_xgVZhNh_2B3_PiB-1vUwZ3gy7dE8zQ48yFlQXX1VPBMwcDfceyO56RKhsqWwO8qqjxx4EeQTyo4LvFYN8q8hH9_F_ZGnlQmB9-xxHb5Oe270h1ybVpSErWAsJ2xfQDiKaZhCfThAwNbx5Z3uByRgIFSaNOvqJ1zaVrTq2uKYgWHJ47wQ5CAKsKLfCZq3V4vMHgfmamIKrE25ebA4_k8HTjf2aIEWZ4KMVpW7mBRIohs35nxH7101VI7xrwtRY41Uow_M8rSPDW9PvjZVpSkgVnnFIsPbOVFo062I4B0JL6JO8ss0JjLuqVNgBENvF8ARYqrH5Ym7h8qlA4q7F8dx19CG3tyxZNFcHPPeUgoJoOzLhAwZo8Zvof5VcUV3SyQGqzHs0tumCHAF9ujz1KiGhcTE18pJYuFFrLG6UbYFbro48AtOKPTajm41qZv5KYNCXPXI8TMKhWXDe34V3-rXNk3N3zPWSNN__xK3jyViW9QPba2ae9y1IBBe1YrinT8TcuCjzfK9moEjg6RofLFCxeIMnLfUHC5p4IxtYU2sqcft9Z43fBc2hxVqFOzCD_XwV-HOYs7CF5bGMQJ-Ba5XIRO1plGhqZsFQ==&xkcb=SoDr-_M3ML30-2QY150FbzkdCdPP&p=13&fvj=0&vjs=3,N/A
Lead Data Engineer,University of Texas at Austin,"Austin, TX 78701 (Downtown area)",Posted 30+ days ago,"$130,000 a year",https://www.indeed.com/rc/clk?jk=970fca0e494e89b7&fccid=f7282ad3490137c7&vjs=3,N/A
"Software Engineer, Data",Acrisure Technology Group,"Hybrid remote in Austin, TX",Posted 28 days ago,,https://www.indeed.com/rc/clk?jk=6791f9f7256ede81&fccid=5756f855d0a2475d&vjs=3,"Software Engineer, Data
 Hybrid Position (3 days per week average in Downtown Austin, TX office)
 About us:
 Acrisure Technology Group (ATG) is a fast paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
 At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
 We are a small team of extremely high caliber engineers with a diverse background across industries and technologies. Our engineers have worked at large companies like Google and Amazon, high frequency trading companies like Two Sigma and Jump Trading, and a variety of smaller startups, including successful startup founders.
 The Role:
 As a Software Engineer focused on Data Engineering, you'll be an essential part of the team building world-class software to transform the insurance industry. Working closely with engineers, researchers, product and design talent, and domain experts, you will design and implement new data processing systems that enable both cutting edge research, and high quality user experiences. As a successful candidate, you will take full advantage of state-of-the-art tools, conceiving of new ones when the right solution does not yet exist. You are driven by a passion for improving the world through technology and delighting users. Help us to turn vision into reality.
 Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
 Here are some of the ways in which you'll achieve impact:

Build efficient and reliable technology with a customer-first mindset.
Assist in designing and maintaining our tech stack, building when it makes sense, inventing when necessary, and upgrading as tools evolve.
Identify, adopt, and evangelize best practices.
Advocate for and identify creative implementations to optimize business impact.
Measure the effectiveness of new features, find and address performance issues, and drive continuous improvement.
Work collegially and effectively as we grow a world-class, diverse engineering team.

You may be a fit for this role if you have:

4+ years of experience in a Data Engineering or similar role using modern data processing techniques.
Exceptional knowledge of computer science fundamentals and software engineering principles, as well as expertise with distributed data processing.
Experience writing recurring data ingestion and validation pipelines across multiple data sources.
Strong knowledge of data architecture and modeling best practices.
Expert level SQL experience.
A product- and customer-focus, with a passion for delighting the end user using data.
An excitement about the opportunity to use data and AI to transform the insurance industry.
An entrepreneurial spirit and are action-biased; self-directed and excited to build something from scratch in a fast-paced, experimentation-driven environment.
Strong communication skills that allow you to be effective in a cross-functional team, as well as deliver data driven insights.
Ability to stamp out unnecessary complexity, harness necessary complexity, and make complex topics clear and accessible to others.
Empathy, kindness towards others, a positive attitude, and self-awareness.
A unique, non-traditional perspective to enhance our team's problem solving abilities.

Academics: Bachelor's degree in Computer Science or a related field, or equivalent experience.
 Location: Must be willing and able to work in the Austin,TX office an average of 3 days a week or as business needs permit.
 #LI-Hybrid #BI-Hybrid"
GCP Data Engineer,Sintesys,"Austin, TX",EmployerActive 7 days ago,"$70,000 - $120,000 a year",https://www.indeed.com/company/SINTESYS/jobs/Data-Engineer-33dfde94d71ff8c6?fccid=8521deabffa13eec&vjs=3,N/A
Data Engineer II (Data Analytics Specialist),Match Inc,"Austin, TX 78727",EmployerActive 2 days ago,$51 - $56 an hour,https://www.indeed.com/company/Match-Inc/jobs/Data-Engineer-2d156525189b38ee?fccid=9dbaeb5de04679f7&vjs=3,N/A
Senior Data Engineer,MAP,"Austin, TX",Posted 9 days ago,Full-time,https://www.indeed.com/rc/clk?jk=cc2e1df15a74a484&fccid=3223156f9ea04632&vjs=3,N/A
Senior Data Engineer,"Double Line, Inc.","Austin, TX 78758 (North Austin area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=47ba162fca6eddc9&fccid=a3af736f7a482061&vjs=3,N/A
Big Data Engineer - PySpark,Logic20/20 Inc.,"Austin, TX",Posted 30+ days ago,"$130,000 - $155,000 a year",https://www.indeed.com/rc/clk?jk=ae6c1a13d1e9155c&fccid=2e9e942085461a66&vjs=3,N/A
Senior Data Engineer,Procore Technologies,"Austin, TX",Posted 4 days ago,"$126,800 - $174,350 a year",https://www.indeed.com/rc/clk?jk=fca7e1d45123055a&fccid=bc675912b67fcb67&vjs=3,N/A
Data Science - Software Engineer,Plaxonic Technologies,"Austin, TX",Posted 30+ days ago,$50 - $60 an hour,https://www.indeed.com/company/Plaxonic-Technologies/jobs/Data-Scientist-c8b4f69fcf2405d5?fccid=4fe234f122c10841&vjs=3,N/A
Senior Data Engineer (remote),Ad Hoc Team,"Remote in Austin, TX",Posted 28 days ago,"$101,570 - $136,994 a year",https://www.indeed.com/rc/clk?jk=b018163a0713f248&fccid=7707621c92754acd&vjs=3,N/A
Data Engineer - Contract,Huckberry,"Austin, TX 78745",EmployerActive 2 days ago,Contract,https://www.indeed.com/rc/clk?jk=b560a513624705d0&fccid=5ce64caed4b7c135&vjs=3,N/A
Senior Back End Architect & Python (Data Pipeline) Engineer,"Really Communications, Inc.","Austin, TX 78702 (Rosewood area)",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=cf19b67af7d799c0&fccid=dd616958bd9ddc12&vjs=3,N/A
Data Engineer,PILYTIX,"Remote in Austin, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=a6bb0948b54cbabf&fccid=dcd0a0f2e35971d1&vjs=3,N/A
Senior Data Operations Engineer,BetterUp,"Remote in Austin, TX",Posted 30+ days ago,"$136,850 - $193,000 a year",https://www.indeed.com/rc/clk?jk=765f4ee1b25a3400&fccid=2345e2002bd590b5&vjs=3,N/A
Principal Data Engineer (R-14645),Dun & Bradstreet,"Hybrid remote in Austin, TX 78752",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=573bb7609cfe51c4&fccid=fa0ca3b638673d62&vjs=3,N/A
Principal Data Engineer,Digital Turbine,"Hybrid remote in Austin, TX",Posted 30+ days ago,Full-time,https://www.indeed.com/rc/clk?jk=f6503071ec81eb6b&fccid=b117c357fa57e851&vjs=3,N/A
Senior Data Engineer (Databricks),Apixio,"Hybrid remote in Austin, TX",Posted 30+ days ago,"$130,000 - $160,000 a year",https://www.indeed.com/rc/clk?jk=cba5ce41eee17550&fccid=d709010bf50f2482&vjs=3,N/A
Data Engineer with Machine Learning-Full Time,Purple Drive Technologies,"Remote in Austin, TX",Posted 30+ days ago,"$120,000 a year",https://www.indeed.com/company/Purple-Drive-Technologies/jobs/Machine-Learning-Engineer-213413daef5802e6?fccid=b50885016c495d25&vjs=3,N/A
Senior Data Science Platform Engineer,"Iodine Software, LLC","Austin, TX 78759 (Arboretum area)",Posted 12 days ago,Full-time,https://www.indeed.com/rc/clk?jk=de352df6469c54c8&fccid=060474ef1576996b&vjs=3,"Sr. Data Science Platform Engineer




Company Overview


  Iodine is an enterprise AI company that is championing a radical rethink of how to create value for healthcare professionals, leaders, and their organizations: automating complex clinical tasks, generating insights and empowering intelligent care. Powered by the largest set of clinical data and use cases available, our groundbreaking clinical machine-learning engine, Cognitive ML, constantly ingests the patient record to generate real-time, highly focused, predictive insights that clinicians and hospital administrators can leverage to dramatically augment the management of care delivery.
 



What You’ll Do


  Iodine thrives on innovation. We are looking for a Data Science Platform Engineer that will help us build out the platform that supports the training and application of our predictive models and performs deep analysis to extract machine consumable meaning from unstructured clinical documentation. You will be joining a small team that has become one of the top players in our field in just two years. Because we work on the cutting edge of a lot of technologies, we need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively. You will be accessing our enormous amount of data to help drive our future innovation.
 

Work with the rest of the data platform team to design, implement, and test our high-throughput, event-based data pipeline for real-time data processing
Design and build tools and frameworks to develop and enhance our predictive model suite and scalably support real-time predictions in production
Design and implement highly performing and highly scalable applications
Work as a core team member to develop a complete and integrated solution
Deliver quality software and documentation on time
Comply with development standards to ensure consistency across the larger development team
Conduct in-depth technical and performance analyses in support of production issue troubleshooting
Monitor and maintain production systems





What You'll Need




Minimum Requirements (Education, certifications and experience):


Bachelor’s degree in Computer Science or related area, Masters preferred
7+ years of professional experience writing Java or Python code, with at least 3 years building data platforms
Expert proficiency with SQL
Seasoned practitioner of engineering best practices such as CI/CD and automated testing
Comfort working in a Linux environment
Passion for exploring, applying and following the evolution of cutting edge technologies related to AI, machine learning, NLP and large scale data processing
Professional experience with MLOps, Docker, Kubernetes, relational databases (PostgreSQL preferred), Kafka, REST API design, and microservices application architectures
Experience with public cloud solutions, such as AWS or Azure
Proven track record of successful delivery of progressively complex technical projects
Coaching and mentoring junior engineers in the team
Team player DNA with a positive, self-starter attitude
Attention to detail, highly organized, with an absolute focus on quality of work





Preferred Requirements:


Familiarity with ClearML, Triton, PyTorch, and TensorFlow
Familiarity with statistics and healthcare domain
Proven expertise in successful large project/build management and execution
Demonstrated ability to manage multiple work streams simultaneously and efficiently





Why should you join Iodine?


  This is a unique opportunity to join a close-knit, rapidly growing team and help us improve a key piece of the organization. You will have the opportunity to drive smarter healthcare processes through technology, so hospitals can stay focused on patient care. You will join a passionate and ambitious team, with a proven record of success building multiple companies. Learn more about our company culture on Built In Austin and on our website at www.iodinesoftware.com.
 



  +++++
 



At Iodine Software, we don’t just accept 
DIFFERENCE
 — we celebrate it, we support it, and we flourish on it for the benefit of our employees, our products, and our community. We pride ourselves to be an equal opportunity workplace and are an affirmative action employer.




  **You must be currently authorized to work full-time in the United States**"
Sr. Data Engineer,Experfy Inc,"Austin, TX",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=4f451388d096307c&fccid=19327ae379f6e1a2&vjs=3,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
 Responsibilities
 Position Responsibilities

Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions

 Requirements

8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise – Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services – Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)

 Apply for this job"
Senior Data Engineer,ConsumerAffairs,"Remote in Austin, TX",Posted 30+ days ago,"$113,000 - $160,000 a year",https://www.indeed.com/rc/clk?jk=492a778c17180cc0&fccid=ec1d37eb69df3dba&vjs=3,"***THIS IS A REMOTE POSITION***
 ConsumerAffairs helps consumers make smart buying decisions in moments of need. Every month millions of consumers turn to our site and tools for help with their considered (often emotional) purchases. 
We educate them about their options, learn about their specific needs, and connect hundreds of thousands of them directly to brands. These brands use our SaaS tools to manage their reviews and communicate directly with consumers to serve them better. Our business thrives when the consumers who trust us get matched with the right brands for them.
 We’re fast-paced and our core values are the bedrock of who we are and who we want to be. 
Our employees believe in raising the bar through data-driven innovation, intellectual curiosity, and grit. We have a team first mentality, and manifest wins by putting the team first. Collaboration and teamwork is in our hearts and we believe winning together is the most fun. But, above all else, we care. We have servant hearts for our consumers, customers, and colleagues. If you want to be part of a globally diverse team that’s focussed on helping people, in an environment where we raise the bar, win as a team, and care above all else—then ConsumerAffairs may be just the place for you!  ABOUT THE JOB:
 The Senior Data Engineer is responsible for monitoring, expanding, and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems as well as building them from the ground up.
 The Senior Data Engineer supports our solution architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
 Candidates should have education and/or experience in data modeling, requirements analysis, and best practices in analytics. Experience with BI applications is a definite plus.

 RESPONSIBILITIES & EXPECTATIONS:
 These responsibilities are not to be construed as a complete statement of all duties performed. Employees will be required to perform other job-related duties as required

Create and maintain optimal data pipeline architecture
Collect and refine business requirements to create technical specifications
Collaborate with other D&A teams to integrate disparate data sources and systems
Collaborate with other CA teams to identify and refine requirements
Develop and implement comprehensive data ingestion processes
Develop and implement comprehensive data integration processes
Develop and implement data models to support operations and analytics
Keep data separated and secure at rest and in transit
Maintain and support cloud data architecture
Maintain job processes managed in Airflow and DBT
Maintain data integration processes via Airbyte, Stitch, Fivetran, and Talend
Create, maintain, and update technical documentation
Define reporting and alerting requirements for data teams
Develop audit and monitoring processes for quality assurance
Develop and manage relationships with partners to optimize integration
Help design, troubleshoot, document, and maintain system processes
Assemble large, complex data sets that meet business requirements
Create data tools for analytics and data scientist team members
Constantly look for ways to improve monitoring, uncover issues, and deliver value
Meet professional obligations through efficient work habits such as meeting deadlines, honoring agreed schedules, coordinating resources in an effective and timely manner, and demonstrating respect for others

Requirements
 Education/Licensure/Certification:

 BSc/BA in Computer Science, Engineering or relevant field

 Experience:

5 years’ experience in Data Integration and Integration
5 years’ experience in Data Warehousing, notably Cloud Data
Experience in SQL across relational and dimensional databases
Experience in AWS and Snowflake implementation and administration
Industry experience is beneficial

Knowledge, skills and abilities:

Fluent in English
Must be highly curious, driven, and disciplined
Understanding of data models, data warehouses, and data mining
Familiarity with BI technologies
Knowledge of SQL is required
Familiarity with cloud analytics is beneficial (AWS, Snowflake)
Proven abilities to take initiative and be innovative
Analytical mind with a problem-solving aptitude
Stands up for decisions and takes responsibility for results
Shares both good and bad outcomes transparently
Obsessed with ensuring an exceptional customer experience- for both internal and external customers. 
Stands up for decisions, takes responsibility for results, and shares both good and bad outcomes transparently.
Demonstrates a relentless focus on results with a commitment to deliver; 
Takes decisive action, and confidently changes course if unsuccessful.
Displays a growth mindset to continually improve; encourages everyone around them to be tenacious and never settle.
Constantly seeks feedback to improve; Focuses on solving issues through teamwork, and collaboration
Acts with urgency; delivers top results in hours and days instead of weeks and months.
Relentless in their pursuit of success and possessing the willpower to embrace challenges as opportunities


   Specific Measures of Success - Expected Outcomes  Start Date to Start Date +1 Year

Ongoing support and leadership towards migration from Redshift to Snowflake, Q1 target
Contribute thought leadership and best practices to data warehouse architecture
Significantly contribute to tech stack optimization towards process and cost efficiency
Recommend/deploy improvements to overall team processes and educational opportunities





CORE VALUES: 
Raise The Bar 

We raise the bar through innovation, intellectual curiosity, and grit. We are not satisfied with yesterday and our hearts thirst to be better tomorrow. 

Win As A Team

 We manifest wins by putting the team first. We have collaboration and teamwork in our hearts and believe winning together is the most fun.

 Care Above All Else

 We care above all else. We have servant hearts for our consumers, customers, and colleagues. We show our care with our passion for results.


 Physical Requirements & Environmental Conditions
 Location: US/ Remote
 Frequency of travel: Occasional travel may be required for meetings, training and/or conferences. 
Light physical activities and efforts required in working within an office environment. 
(Reasonable accommodations will be made in accordance with existing ADA requirements for otherwise qualified individuals with disabilities.)
 ConsumerAffairs provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
 This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. 
Benefits

Competitive salary commensurate with skills, experience, and location. Salary range is between $113,000-160k 
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Family Leave (Maternity, Paternity)
Short Term & Long Term Disability
Training & Development
Work From Home
Free Food & Snacks
Stock Option Plan"
Senior Data Engineer (US Remote),Swyfft,"Remote in Austin, TX 73301",Posted 6 days ago,Full-time,https://www.indeed.com/rc/clk?jk=02b035b60b4eadc7&fccid=e5904a85da12679e&vjs=3,"Swyfft Holdings, LLC, consists of Swyfft, LLC and Core Programs, LLC. Both are fast-growing, tech-enabled MGA’s that are disrupting the traditional insurance industry by re-imagining how you price and bind home insurance and commercial package products. From lightning-fast quotes to hassle-free claims servicing, Swyfft Holdings, LLC leverages big data to provide the very best customer service experience in the industry. We're growing, we’re expanding and we're looking for “tech-savvy” folks like you to join our team!


 About the Position:


 As a Senior Data Engineer, you will be instrumental in supporting the development and use of our data systems. Your goal is to ensure that information flows timely and accurately to and from the organization as well as within. As a successful Senior Data Engineer will bring forth a strong understanding of databases and data analysis procedures. You are a highly technical SQL expert with strong problem-solving skills and excellent troubleshooting capabilities. In a perfect world, you might even have some previous experience working for an insurtech or an analytics measurement platform.


 This position is a 100% remote U.S. based opportunity. Some travel for day-to-day work, team meetings, and training will be required.



 Key Responsibilities: (What you'll be asked to do)



Build efficient ways to organize, store and analyze data while maintaining availability and consistency.
Create processes and enforce policies for effective data management.
Formulate techniques for quality data collection to ensure adequacy, accuracy and legitimacy of data.
Devise and implement efficient and secure procedures for data handling and analysis with attention to all technical aspects.
Establish rules and procedures for data sharing with upper management and external stakeholders.
Support others in the daily use of data systems and ensure compliance to legal and company standards.
Provide assistance with reports and data extraction when needed.
Monitor and analyze information and data systems and evaluate their performance to discover ways of enhancing them (such as new technologies and upgrades). 
Ensure digital databases and archives are protected from security breaches and data losses.
Troubleshoot data-related problems and authorize maintenance or modifications



 The Successful Candidate: (what we're looking for)



You have a strong understanding of databases and data analysis procedures.
You have an analytical mindset and strong problem-solving skills.
You have excellent communication and collaboration skills.
You have intense attention to detail and quality assurance.



 Some Requirements:



Expertise in SQL (MS and PostgreSQL).
Familiarity with modern database and information system technologies. 
Expertise in both Tableau Desktop and Server. 
7+ years of experience as a data manager.
Excellent understanding of data administration and management functions such as collection, analysis, and distribution.
Understanding of data warehousing and star schemas.
Basic familiarity with predictive analysis and data visualization techniques.
Solid understanding of R and Python environment configuration and basic programming.
Expert level in Microsoft Excel.


Understanding of spatial database functionality is a plus.



Education:



Bachelors’ degree or equivalent experience required in a related field.
Advanced degrees or Certifications are a plus.



Computer Skills:



You are familiar with predictive analysis and data visualization techniques using relevant tools such as: Tableau, Dataiku, R, Python.
Must be proficient with MS Office and other internal insurance related programs, systems or applications. 


Ability to communicate effectively using programs such as Slack & MS Teams. You are comfortable sharing screens and video chatting.



 Other:



Reliable high-speed internet connectivity required.
Designated quiet work from home space.



 We Have a Great Benefits Package!



20 days of PTO annually
Medical, Dental, Vision
Short- and Long-Term Disability (Company Paid)
Life & AD&D (Company Paid)
Healthcare, Dependent Care and Transit FSA
401K with a generous matching contribution and no vesting schedule



 It is the policy of Swyfft to provide equal employment opportunities to all employees and applicants for employment without regard to race, religion, color, ethnic origin, gender, gender identity, age, marital status, veteran status, sexual orientation, disability, or any other basis prohibited by applicable federal, state, or local law. EOE/AA/M/D/V/F.


 Please Note: Swyfft is not accepting 3rd party agency resumes for this position, please do not forward resumes to our careers email address or Swyfft employees. Swyfft will not be responsible for any fees related to unsolicited resumes."
"Senior Data Engineer, Data Solutions",Invitae,"Remote in Austin, TX",Posted 30+ days ago,"$138,400 - $173,000 a year",https://www.indeed.com/rc/clk?jk=b4ac09f6f2ac1d31&fccid=03925754e1730576&vjs=3,"Invitae (NYSE: NVTA) is a leading medical genetics company trusted by millions of patients and their providers to deliver timely genetic information using digital technology. We aim to provide accurate and actionable answers to strengthen medical decision-making for individuals and their families. Invitae's genetics experts apply a rigorous approach to data and research, serving as the foundation of their mission to bring comprehensive genetic information into mainstream medicine to improve healthcare for billions of people.

 By joining Invitae, you'll work alongside some of the world's experts in genetics and healthcare at the forefront of genetic medicine. We've crafted a culture that empowers our teams and our teammates to have the biggest impact and to explore their interests and capabilities. We prize freedom with accountability and offer significant flexibility, along with excellent benefits and competitive compensation in a fast-growing organization!
 We are looking for a reliable and motivated Senior Data Engineer to join our Patient Data Network team who can support our Data Solutions Team in developing the data ingestion pipelines and data platform architecture that supports the analytical and reporting needs of internal stakeholders, data scientists, and our machine learning team, as well as externally facing products.
 What you'll do:

Understand our complex data ecosystem
Be hands-on with the technical design and implementation of reliable, scalable and efficient data processing framework (batch and streaming), data driven products and software solutions for external and internal customers
Identifies, prioritizes, and solves for ambiguous, open-ended problems
Collaborate with multiple teams; Owns and delivers data solutions from end-to-end with high quality

What you bring:

Typically requires a minimum of 8 years of related experience with a Bachelor's degree; or 6 years and a Master's degree; or a PhD with 3 years experience. Any equivalent combination of training, education, and experience that provides the required skills, knowledge and abilities.
Extensive hands-on experience working with large datasets, pipelines, and modern warehouse technologies
Self-starter attitude and ability to work towards a larger goal with minimal guidance
Advanced experience in SQL queries and performance tuning
Understanding of functional programing paradigms
Proficiency in Scala, Java, Python and a demonstrable ability to quickly learn
Focus on high quality code, including automated testing and coding best practices
Experience with messaging/queuing systems or stream processing systems
Experience in building distributed systems with infrastructure automation, monitoring and alerting
Track record of working with cross functional teams and stakeholders,

Additional Preferred but not Required Skills:

Experience with Snowflake as a warehouse technology
Experience using Kafka for implementing streaming application
Experience with CI/CD pipelines (e.g. GitHub Actions)
Experience with maintaining and administering Kubernetes clusters
Interest in working on related but separate projects in parallel
Experience with DBT as data transformation tool
Experience with data lineage/data governance tools like Atlan
Experience with data modeling/dimensional modeling

#LI-Remote


    This salary range is an estimate, and the actual salary may vary based on a wide range of factors, including your skills, qualifications, experience and location. This position is eligible for benefits including but not limited to medical, dental, vision, life insurance, disability coverage, flexible paid time off, Spring Health, Carrot Fertility, participation in a 401k with company match, ESPP, and many other additional voluntary benefits. Invitae also offers generous paid leave programs so you can spend time with your new child, recover from your own illness or care for a sick family member.
  
 USA National Pay Range

    $138,400—$173,000 USD
  


 Please apply even if you don't meet all of the ""What you bring"" requirements noted. It's rare that someone checks every single item, it's ok, we encourage you to apply anyways.
 Join us!
 At Invitae, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.  We truly believe a diverse workplace is crucial to our company's success and to better serve our diverse patients. Your input is especially valuable. We'd greatly appreciate it if you can take a quick moment to make your selection(s) below. Submissions will be anonymous.
 You can find a detailed explanation of our privacy practices here."
Lead Customer Engineer – Instinct Data Center GPU Software,"Advanced Micro Devices, Inc","Austin, TX 78735 (East Oak Hill area)",Posted 30+ days ago,,https://www.indeed.com/rc/clk?jk=381271fcd666bd39&fccid=b45c4b5a9c9a7448&vjs=3,"Overview: 
 
 WHAT YOU DO AT AMD CHANGES EVERYTHING



 We care deeply about transforming lives with AMD technology to enrich our industry, our communities, and the world. Our mission is to build great products that accelerate next-generation computing experiences – the building blocks for the data center, artificial intelligence, PCs, gaming and embedded. Underpinning our mission is the AMD culture. We push the limits of innovation to solve the world’s most important challenges. We strive for execution excellence while being direct, humble, collaborative, and inclusive of diverse perspectives. This is who we are at our best. One Company. One Team.
 


 AMD together we advance_
  Responsibilities: 
 
 Lead Customer Engineer – Instinct Data Center GPU Software



 THE ROLE:


   AMD is unwavering in our dedication to delivering an exceptional customer experience. We understand that ease of use and responsive customer support are essential ingredients in achieving this objective. As a valued member of the Data Center GPU & Accelerated Processing applications engineering organization, your role as Lead Customer Engineer is pivotal in driving and sustaining best-in-class customer support for a major cloud service provider in the US.
 


 THE PERSON:


   As a leader in the customer engineering organization, you have a critical role in ensuring overall customer satisfaction at one of our most important cloud data center customers through resolution & closure of complex technical support issues and managing needed collaboration between the customer, their partner, software development, product management, and business leaders. You should possess excellent software debugging skills (C/C++ and Python required), have a fundamental understanding of how to run machine learning (ML) workloads on server systems with accelerated processing, and have successfully supported at least one major hyperscale cloud provider in your career.
 


 KEY RESPONSIBILITIES:


 Lead the efforts of the DC GPU customer engineering (CE) team to ensure customer success in the deployment and use of AMD Instinct™ Accelerators at a major CSP in the US. These activities include: 

The replication and debug of customer issues (software stack, firmware, driver), working with cross-functional teams, and driving root cause investigation.
 Understand customer requirements and schedule, identify gaps in AMD offering and work with key stakeholders to close them.

 Act as the primary point of contact for the customer and ensure all issues for the account are prioritized, tracked, and resolved in a timely manner. 
Facilitate technical discussions with the customer, their designated OEM/ODM partners, and relevant AMD subject matter experts, as needed.
 Help recruit best-in-class technical engineering talent and provide guidance to management on how to improve performance and efficiency of the existing team.
 Manage technical escalations from customers as well as your staff's internal escalations to AMD's various software development teams.
 Represent technical support management during customer calls/escalations and act as an escalation point for customer issues, driving towards a resolution.
 Jump in to help other members of the team in resolving customer issues as needed. 
Proactively monitor customer tickets while ensuring all issues are handled properly. 
Communicate the team's effectiveness in resolving major customer issues and overall support trends to management.
 Serve as a mentor to other CE team regarding technical issues and soft skills.
 Roll up weekly support highlights and red flags to management.
 Provide recommendations to improve internal processes and tools to enhance customer experience with our products and software.



 PREFERRED EXPERIENCE:


 Familiarity with at least one Machine Learning (ML) Framework, such as TensorFlow or PyTorch, and benchmarking ML workloads on GPUs and/or other AI accelerators.
 Python & C/C++ programming, employing best software design practices.
 GPU software development or validation involving HIP, CUDA, ROCm, or OpenCL.
 Software performance evaluations, optimizations and debugging.
 Customer first mindset. Driven to meet or exceed customer expectations, resolve issues expeditiously and, when possible, proactively.
 Able to size technical issues and articulate their impact to AMD and our customers.
 Effective communication & presentation skills and comfortable in a customer-facing environment. Experience with executive rollups.



 ACADEMIC CREDENTIALS:


   BS in Computer Science, Computer Engineering, or Electrical Engineering. MS is preferred.
 


 LOCATION:


   Austin, TX, Santa Clara, Markham, Ontario
 

   #LI-RL1
  Qualifications: 
 
   At AMD, your base pay is one part of your total rewards package. Your base pay will depend on where your skills, qualifications, experience, and location fit into the hiring range for the position. You may be eligible for incentives based upon your role such as either an annual bonus or sales incentive. Many AMD employees have the opportunity to own shares of AMD stock, as well as a discount when purchasing AMD stock if voluntarily participating in AMD’s Employee Stock Purchase Plan. You’ll also be eligible for competitive benefits described in more detail here.
 


 AMD does not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services. AMD and its subsidiaries are equal opportunity, inclusive employers and will consider all applicants without regard to age, ancestry, color, marital status, medical condition, mental or physical disability, national origin, race, religion, political and/or third-party affiliation, sex, pregnancy, sexual orientation, gender identity, military or veteran status, or any other characteristic protected by law. We encourage applications from all qualified candidates and will accommodate applicants’ needs under the respective laws throughout all stages of the recruitment and selection process."
Senior Data Center Network Engineer,Samsung SDS America,"Austin, TX",EmployerActive 2 days ago,"$140,000 - $160,000 a year",https://www.indeed.com/rc/clk?jk=891bea7dc91b9e2d&fccid=da3c7fed78dd1607&vjs=3,"Samsung SDS America is looking for a Sr. Data Center Network Engineer to support, maintain, manage, troubleshoot, and execute network projects for a brand new state-of-the-art Samsung Semiconductor Manufacturing facility in Taylor, TX. The ideal candidate would bring a strong background as a SME supporting and designing Data Center network solutions, with strong experience in Spine-Leaf, and VxLAN with Cisco and Arista. Candidate should have over 10 years of experience as a network engineer supporting large enterprise environments, and a minimum of 5 years of hands on design/support experience on Data Center networking infrastructure (SME).  
This role will provide ongoing network support for all enterprise network aspects, provide 2nd to last level of support to resolve complex issues, and is expected to be a technical leader for network guidance for other team members and management. Participation in on-call rotation is required. Candidates should be team oriented, have a strong sense of responsibility, adhere to SOPs, and effectively communicate with all levels of the organization. Candidates must also have strong verbal and written communication skills, and must know the importance of time management and prioritization, and understand criticality of uptime in a manufacturing environment.  
This position will temporarily join the network team at the Samsung Austin Semiconductor facility (in North-East Austin) to received training, and learn about Samsung standards and procedures. After or during training, the work location will move to Taylor, TX to support the construction site network, and later to support the brand new manufacturing facility during implementation and ongoing operation.  
This is an onsite role and employees must live within a 1-hour drive to Taylor, TX.  
Samsung SDS is the digital arm of the Samsung group and a global provider of cloud and digital transformation innovations. Samsung SDS delivers enterprise-grade solutions and services in cloud, secure mobility, analytics / AI, digital marketing and digital workspace. We enable our customers in government, financial services, healthcare, and other industries to drive business in a hyper-connected economy helping them to increase productivity, safeguard assets, and make smarter decisions.
 Responsibilities:

Provide senior level operational support for all company networks, including Data Center networks and legacy networks 
Provide network support for issues escalated by other network team members and internal customers 
Provide immediate response to restore service due to outage/impact events 
Support the design and implementation of complex network architectures, including routing, switching, network security, load balancing and virtualization technology 
Make recommendations for the Spine-Leaf topology designs, VxLAN EVPN, legacy networks. 
Collaboration with other teams, such as system administrators, developers, and security teams to ensure that the data center network infrastructure is aligned with their requirement 
Resolve assigned tickets in a timely manner to provide solutions to internal customers 
Perform POC testing in a lab environment (HW/SW Validations, workarounds, etc.) 
Review, provide feedback, and approve other team members’ network procedures (Release Plans) to validate technical accuracy for network changes, and help answer technical questions in Change Management meetings 
Candidate must be available and willing to provide support 24/7 (overnight support as needed - due to the nature of our manufacturing environment) – Must respond to escalation tickets within SLA, and participate in an on-call rotation 
Must adhere to standard operating procedures, standard configuration, check lists, and conform to corporate change-control and security policies 
Participate in maintenance window planning, coordination, preparation, execution and monitoring 
Perform network lifecycle management – Design, implement, manage, and support hundreds of critical network devices (Cisco and Arista switches, routers, PAN firewalls, F5 load balancers, Cisco wireless controller and access points, etc.) 
Perform HW/SW upgrades, refresh/migration projects, expansion projects, plan and execute network changes, process RMAs, escalate and manage SRs with vendors, perform internal network audits and remediation 
Excellent analytical, troubleshooting and communications skills at all organizations levels 
Create reports per management request, and on a weekly basis 
Setup and analyze packet captures to support network team and internal customers 
Maintain and update documentation for all network systems, create completion reports, and conduct knowledge transfer to other team members 
Must be a team player with passion to mentor other team members 

Requirements

Bachelor’s Degree in Computer Science or related field
10+ years of continuous large scale, enterprise class planning, design, implementation, and network support experience in network engineer positions
Expert Level: 5+ years of continuous hands on experience with Cisco/Arista EVPN VXLAN design and Spine-Leaf topologies, implementation and support
CCNP (Preferred CCNP Data Center or CCIE)
Strong experience with VPC configurations, MLAG, and best practices for server’s connectivity
Extensive routing protocol experience - EIGRP, OSPF, BGP, MPBGP, PIM and EVPN
Strong experience of VPC, STP, MLag, LACP, VRF, IPv4 subnetting, 802.1x, TACACS+, SYSLOG, Cisco IOS, Cisco NX-OS
Strong ability to troubleshoot complex issues related to hardware, L3/L2 deployment, applications
Experience with PIM, MSDP, Anycast-RP, Multicast
Strong experience with design, configuration, operations and support of Cisco Catalyst Switches, Nexus, Arista switches, Palo Alto Firewalls, and F5 Load Balancers’
Must live within 1 driving hour maximum from Taylor, TX
Strong customer focus, ownership, urgency and drive
Strong responsibility for network documentation, and passion to mentor other team members

Nice to have:

Experience with Packet Capture Analysis, SSL certificates, Cisco DCNM or ACI 
Experience supporting connectivity for servers in a Data Center environment and Data Center design experience
Familiarity with Unix/Linux and the ability to script in Python, Ansible or other automation tools is a plus

Benefits
 Samsung SDSA offers a comprehensive suite of programs to support our employees:

Top-notch medical, dental, vision and prescription coverage
Wellness program
Parental leave
401K match and savings plan
Flexible spending accounts
Life insurance
Paid Holidays
Paid Time off
Additional benefits 

Samsung SDS America, Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity or expression, national origin, disability, status as a protected veteran, marital status, genetic information, medical condition, or any other characteristic protected by law."
Senior Data Platform Engineer (Hybrid),Apixio,"Hybrid remote in Austin, TX",Posted 14 days ago,"$120,000 - $185,000 a year",https://www.indeed.com/rc/clk?jk=6c68355c68901fac&fccid=d709010bf50f2482&vjs=3,"Who We Are:
 Apixio is advancing healthcare with data-driven intelligence and analytics. Our Artificial Intelligence platform gives organizations across the healthcare spectrum the power to mine clinical information at scale, creating novel insights that will change the way healthcare is measured, care is delivered, and discoveries are made.



The Opportunity at Apixio:
 Apixio is a healthcare analytics company that leverages artificial intelligence and big data to improve healthcare outcomes. We are seeking a talented Senior Data Platform Engineer to join our team and help build and maintain our data platform. As a Senior Data Platform Engineer at Apixio, you will work on a fast-paced team of talented individuals who are dedicated to improving healthcare. You will have the opportunity to lead and mentor other data platform engineers and work closely with data analysts and data scientists to enable both analytics and AI models.
 Who You Are:
 You are an experienced data platform engineer with multiple years of ETL experience and coding expertise in Scala, Java, and Python. You have experience working with one or more of Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks. You are familiar with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture. You are a problem solver who enjoys collaborating with others to build innovative solutions. You have excellent communication and collaboration skills.
 What You Will Own:
 As a Senior Data Platform Engineer at Apixio, you will own the maintenance, design, and implementation of our data platform. This will include data pipelines, ETL workflows, OCR, developing and optimizing our internal data lakehouse, and ensuring data security and compliance with regulatory requirements. You will work closely with the application teams, data analysts, and data scientists to provide data access and enable analytics. You will automate processes and develop tools to improve the efficiency of the data platform. You will lead and mentor other data platform engineers, stay current with emerging trends and technologies in data engineering, and work to incorporate them into the Apixio data platform.
 In addition to the above, you will also be responsible for:

Maintaining, designing, and implementing scalable, reliable, and high-performance data architectures
Developing and implementing best practices for data integration, data quality, and data governance
Ensuring the scalability, reliability, and security of the data platform
Collaborating with other teams at Apixio to understand their data needs and develop solutions to meet those needs
Managing and optimizing cloud-based infrastructure for the data platform
Keeping up-to-date with emerging technologies and trends in data engineering and healthcare tech

What You Bring to the Table:

Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
5+ years of experience in data platform engineering with coding expertise in Scala, Java, and Python
Experience with technologies such as Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks
Experience with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture
Experience leading and mentoring other engineers
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Nice to have: healthcare experience and familiarity with healthcare tech standards like x12 EDI, HL7 FHIR, CCDA, and V2 messaging



   The salary range below is for Base Salary. Total compensation also includes benefits and variable compensation. Compensation will be determined based on several factors including, but not limited to, skill set, years of experience, and the employee’s geographic location.
  
 Base Compensation

    $120,000—$185,000 USD
  


 We recognize that people come with experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Your skills and background may be more translatable to this role than you initially thought. Allow us the opportunity to get to know you. Please let us know if you require accommodations during the interview process.
 What Apixio can offer you:

Meaningful work to improve healthcare
Competitive compensation
Exceptional benefits, including medical, dental and vision, FSA
401k with company matching
Generous vacation policy
A hybrid work schedule (2 days in office & 3 days work from home) (Note: If the position is designated as REMOTE it will stay REMOTE)
Modern open office in beautiful San Mateo, CA; Los Angeles, CA; San Diego, CA; Austin, TX and Dallas, TX
Subsidized gym membership
Catered, free lunches
Parties, picnics, and wine-downs
Free parking

We take your privacy very seriously. Please review our privacy policy to see exactly how we protect your information here
 We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.
 Apixio will consider for employment qualified applicants with criminal histories pursuant to the San Francisco Fair Chance Ordinance subject to the requirements of all state and federal laws and regulations.
 If you are a recruiter or placement agency, please do not submit resumes to any person or email address at Apixio prior to having a signed agreement from Talent Acquisition. Apixio is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Furthermore, any resumes sent to us without an agreement in place will be considered your company's gift to Apixio and may be forwarded to our recruiters for their attention and no fee will be paid.


 LI-RB1"
